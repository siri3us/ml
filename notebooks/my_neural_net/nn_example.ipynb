{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "* [1. Modules](#modules)\n",
    "    * [1.1 Base objects](#base)\n",
    "        * [1.1.1 Layer](#layer)\n",
    "        * [1.1.2 Sequential](#seq)\n",
    "    * [1.2 Helpers](#helpers)\n",
    "        * [1.2.1 Initializers](#initializers)\n",
    "        * [1.2.2 Regularizers](#regularizers)\n",
    "    * [1.3 Layers](#layers)\n",
    "        * [1.3.1 Dense](#dense)\n",
    "        * [1.3.2 Softmax](#softmax)\n",
    "        * [1.3.3 Dropout](#dropout)\n",
    "        * [1.3.4 BatchNormalization](#batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layer'></a>\n",
    "## Layer [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/sequential/layer.py\n",
    "\n",
    "#import numpy as np\n",
    "#from collections import OrderedDict\n",
    "\n",
    "class Layer:\n",
    "    def assert_nans(self, arr):\n",
    "        assert not np.any(np.isnan(arr))\n",
    "    def assert_inf(self, arr):\n",
    "        assert not np.any(np.isinf(self.grad_input))\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.training = True\n",
    "        self.initialized = False\n",
    "           \n",
    "    # Initialization\n",
    "    def initialize(self, params):\n",
    "        params = self._initialize(params)\n",
    "        self.initialized = True\n",
    "        return params\n",
    "    \n",
    "    def _initialize(self, params):\n",
    "        self._check_initialization_params(params)\n",
    "        params = self._initialize_name(params)\n",
    "        return params\n",
    "\n",
    "    def _check_initialization_params(self, params):\n",
    "        assert 'input_shape' in params\n",
    "        assert 'seed'  in params\n",
    "        assert 'dtype' in params\n",
    "        assert 'names' in params\n",
    "    \n",
    "    def _initialize_name(self, params):\n",
    "        names = params['names']\n",
    "        layer_type_name = type(self).__name__\n",
    "        n_layers = names.setdefault(layer_type_name, 0)\n",
    "        self.name = layer_type_name + str(n_layers)\n",
    "        names[layer_type_name] += 1\n",
    "        return params\n",
    "    \n",
    "    # Propagation\n",
    "    def forward(self, input):\n",
    "        return self.update_output(input)\n",
    "    def update_output(self, input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        self.update_grad_input(input, grad_output) # This updates self.grad_input\n",
    "        self.update_grad_param(input, grad_output)\n",
    "        return self.grad_input\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        pass\n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        pass\n",
    "        \n",
    "    def get_params(self):\n",
    "        return OrderedDict()\n",
    "        \n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict()\n",
    "        \n",
    "    def zero_grad_params(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/sequential/sequential.py\n",
    "\n",
    "#import numpy as np\n",
    "#from .layer import Layer\n",
    "#import copy\n",
    "#from collections import OrderedDict\n",
    "\n",
    "class Sequential(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def _initialize(self, params):\n",
    "        params.setdefault('seed', 0)\n",
    "        params.setdefault('dtype', np.float32)\n",
    "        params.setdefault('names', {})\n",
    "        self._check_initialization_params()\n",
    "        for n_layer, layer in enumerate(self.layers):\n",
    "            params = layer.initialize(params)\n",
    "        return params\n",
    "\n",
    "    def update_output(self, input):\n",
    "        \"\"\"This function passes input through all layers and saves output\"\"\"\n",
    "        for n_layer, layer in enumerate(self.layers):\n",
    "            output = layer.forward(input)\n",
    "            input = output\n",
    "        self.output = output\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, input, grad_output):\n",
    "        n_layers = len(self.layers)\n",
    "        for n_layer in reversed(list(range(1, n_layers))):\n",
    "            grad_output = self.layers[n_layer].backward(self.layers[n_layer - 1].output, grad_output)\n",
    "        self.grad_input = self.layers[0].backward(input, grad_output)\n",
    "        return self.grad_input\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = OrderedDict()\n",
    "        for layer in self.layers.get_params():\n",
    "            for param_name, param_value in layer.items():\n",
    "                params[param_name] = param_value\n",
    "        return params\n",
    "\n",
    "    def get_grad_params(self):\n",
    "        grad_params = OrderedDict()\n",
    "        for layer in self.layers:\n",
    "            for grad_name, grad_value in layer.get_grad_params.items():\n",
    "                grad_params[grad_name] = grad_value\n",
    "        return grad_params\n",
    "        \n",
    "    def zero_grad_params(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad_params()\n",
    "            \n",
    "    def __getitem__(self, n):\n",
    "        return self.layers[n]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '->'.join([str(layer) for layer in self.layers])\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Sets all layers to training mode\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "            \n",
    "    def evaluate(self):\n",
    "        \"\"\"Sets all layers to evaluation mode\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='helpers'></a>\n",
    "## Helper objects [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initializers'></a>\n",
    "### Initializers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/initializers/initializers.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class DeterministicInitializer(Initializer):\n",
    "    def __init__(self, init_value):\n",
    "        self.init_value = init_value\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        return self.init_value.astype(dtype)\n",
    "    \n",
    "class RandomInitializer(Initializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__()\n",
    "        self.gen = np.random.RandomState(seed)\n",
    "\n",
    "        \n",
    "class ZerosInitializer(Initializer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        if shape is None:\n",
    "            return 0.0\n",
    "        return np.zeros(shape, dtype=dtype)\n",
    "\n",
    "\n",
    "class NormalInitializer(RandomInitializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        stddev = 1.0\n",
    "        if len(shape) == 2:\n",
    "            stddev = 1. / np.sqrt(shape[0])\n",
    "        if len(shape) == 4:\n",
    "            stddev = 1.0 / np.sqrt(np.prod(shape[1:]))\n",
    "        return self.gen.uniform(-stddev, stddev, size=shape).astype(dtype)\n",
    "\n",
    "\n",
    "class NormalInitializer(RandomInitializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        \n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        stddev = 1.0\n",
    "        if len(shape) == 2:\n",
    "            stddev = 1. / np.sqrt(shape[0])\n",
    "        if len(shape) == 4:\n",
    "            stddev = 1.0 / np.sqrt(np.prod(shape[1:]))\n",
    "        return self.gen.normal(loc=0, scale=stddev, size=shape).astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularizers'></a>\n",
    "### Regularizers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/regularizers/regularizers.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Regularizer:\n",
    "    pass\n",
    "\n",
    "class EmptyRegularizer(Regularizer):\n",
    "    def __bool__(self):\n",
    "        return False\n",
    "    \n",
    "class L2regularizer(Regularizer):\n",
    "    def __init__(self, l2=0.0):\n",
    "        self.l2 = l2\n",
    "    def __bool__(self):\n",
    "        return True\n",
    "    def loss(self, arr):\n",
    "        return 0.5 * np.sum(arr ** 2)\n",
    "    def grad(self, arr):\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dense'></a>\n",
    "### 1.3.1 Dense [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/dense.py\n",
    "\n",
    "#import numpy as np\n",
    "#from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, units, use_bias=True, \n",
    "                 W_initializer=None, b_initializer=None, \n",
    "                 W_regularizer=None, b_regularizer=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - units - Integer or Long, dimensionality of the output space.\n",
    "        - W_initializer\n",
    "        - b_initializer\n",
    "        - seed - used for initializers!!!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W_initializer = W_initializer\n",
    "        self.b_initializer = b_initializer\n",
    "        self.W_regularizer = W_regularizer\n",
    "        self.b_regularizer = b_regularizer\n",
    "\n",
    "    # initialization\n",
    "    def _initialize(self, params):\n",
    "        # Params check and name initialization\n",
    "        params = super()._initialize(params)\n",
    "\n",
    "        # Initializing params and grads\n",
    "        params = self._initialize_W(params)\n",
    "        params = self._initialize_b(params)\n",
    "        \n",
    "        # Regularization\n",
    "        if self.W_regularizer is None: self.W_regularizer = EmptyRegularizer()\n",
    "        if self.b_regularizer is None: self.b_regularizer = EmptyRegularizer()\n",
    "        return params\n",
    "\n",
    "    def _initialize_W(self, params):\n",
    "        input_shape = params['input_shape']\n",
    "        seed = params['seed']\n",
    "        dtype = params['dtype']\n",
    "        if self.W_initializer is None:\n",
    "            self.W_initializer = NormalInitializer(seed=seed)\n",
    "        elif isinstance(self.W_initializer, np.ndarray):\n",
    "            assert self.W_initializer.shape == (input_shape[1], self.units)\n",
    "            self.W_initializer = DeterministicInitializer(self.W_initializer)\n",
    "        else:\n",
    "            assert False\n",
    "        self.W = self.W_initializer(shape=(input_shape[1], self.units), dtype=dtype)\n",
    "        self.grad_W = np.zeros_like(self.W, dtype=dtype)\n",
    "        params['seed'] = seed + 1\n",
    "        params['input_shape'] = (input_shape[0], self.units) # Input shape for the next layer\n",
    "        return params\n",
    "        \n",
    "    def _initialize_b(self, params):\n",
    "        dtype = params['dtype']\n",
    "        if self.b_initializer is None:\n",
    "            self.b_initializer = ZerosInitializer()\n",
    "        elif isinstance(self.b_initializer, np.ndarray):\n",
    "            assert self.b_initializer.shape == (self.units,)\n",
    "            self.b_initializer = DeterministicInitializer(self.b_initializer)\n",
    "        else:\n",
    "            assert False\n",
    "        self.b = self.b_initializer(shape=(self.units,), dtype=dtype)\n",
    "        self.grad_b = np.zeros_like(self.b, dtype=dtype)\n",
    "        return params\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        self.assert_nans(input)\n",
    "        self.output = np.dot(input, self.W)  # [B x I] x [I x O] = [B x O]\n",
    "        if self.use_bias:\n",
    "            self.output += self.b[None, :]\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        self.assert_nans(grad_output)\n",
    "        self.grad_input = np.dot(grad_output, self.W.T)         # [B x O] x [O x I] = [B x I]\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        self.assert_nans(grad_output)\n",
    "        assert input.shape[0] == grad_output.shape[0]\n",
    "        batch_size = input.shape[0]\n",
    "        self.grad_W = np.dot(input.T, grad_output)               # ([I x B] x [B x O]).T = [I, O]\n",
    "        if self.W_regularizer:\n",
    "            self.grad_W += self.W_regularizer.grad(self.W)\n",
    "        if self.use_bias:\n",
    "            self.grad_b = np.mean(grad_output, axis=0)\n",
    "            if self.b_regularizer:\n",
    "                self.grad_b += self.b_regularizer.grad(self.b)\n",
    "        print(self.grad_W)\n",
    "        print(self.grad_b)\n",
    "        \n",
    "    def get_regularization_loss(self):\n",
    "        loss = 0\n",
    "        if self.W_regularizer:\n",
    "            loss += self.W_regularizer.loss(self.W)\n",
    "        if self.use_bias:\n",
    "            if self.b_regularizer:\n",
    "                loss += self.b_regularizer.loss(self.b)  \n",
    "        return loss\n",
    " \n",
    "    def get_params(self):\n",
    "        return OrderedDict([(self.name + '/W', self.W), (self.name + '/b', self.b)])\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict([(self.name + '/W', self.grad_W), (self.name + '/b', self.grad_b)])\n",
    "\n",
    "    def zero_grad_params(self):\n",
    "        self.grad_W.fill(0)\n",
    "        self.grad_b.fill(0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Dense({}->{})'.format(self.input_size, self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': numpy.float32,\n",
       " 'input_shape': (10, 100),\n",
       " 'names': {'Dense': 1},\n",
       " 'seed': 12}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = Dense(units=100)\n",
    "dense.initialize({'input_shape': (10, 20), 'seed': 11, 'dtype': np.float32, 'names': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  5.084615128635586e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "batch_size = 2\n",
    "input_size = 120\n",
    "output_size = 3\n",
    "\n",
    "X = np.linspace(-0.1, 0.5, num=batch_size * input_size).reshape(batch_size, input_size) # [2, 120]\n",
    "W = np.linspace(-0.2, 0.3, num=input_size * output_size).reshape(input_size, output_size) # [360] - > [120, 3]\n",
    "b = np.linspace(-0.3, 0.1, num=output_size)\n",
    "\n",
    "dense = Dense(output_size, W_initializer=W, b_initializer=b)\n",
    "dense.initialize({'input_shape': (-1, 120), 'dtype': np.float32, 'seed': 1, 'names': {}})\n",
    "\n",
    "output = dense.forward(X)\n",
    "correct_output = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(output, correct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(func, input, grad_output, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(input)\n",
    "    it = np.nditer(input, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        index = it.multi_index\n",
    "        oldval = input[index]\n",
    "        input[index] = oldval + h\n",
    "        pos = func().copy()\n",
    "        input[index] = oldval - h\n",
    "        neg = func().copy()\n",
    "        input[index] = oldval\n",
    "        grad[index] = np.sum((pos - neg) * grad_output) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def eval_numerical_gradient_layer_param(layer, param_name, input, grad_output, h=1e-5):\n",
    "    assert layer.initialized\n",
    "    param_value = getattr(layer, param_name)\n",
    "    def func():\n",
    "        return layer.forward(input)\n",
    "    return eval_numerical_gradient_array(func, param_value, grad_output, h=h)\n",
    "\n",
    "def eval_numerical_gradient_layer_input(layer, input, grad_output, h=1e-5):\n",
    "    assert layer.initialized\n",
    "    def func():\n",
    "        return layer.forward(input)\n",
    "    return eval_numerical_gradient_array(func, input, grad_output, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape = (10, 6)\n",
      "W.shape     = (6, 5)\n",
      "b.shape     = (5,)\n",
      "num_grad_W.shape     = (6, 5)\n",
      "grad_b.shape         = (5,)\n",
      "num_grad_input.shape = (10, 6)\n",
      "[[-2.37342917 -0.51198268  0.31810037  2.90403428  1.06935402]\n",
      " [-3.51268592 -2.11141032 -3.63676781 -1.97058092 -2.23413933]\n",
      " [ 5.08663365  1.49488732  3.34009108  5.93662486 -2.15844283]\n",
      " [-0.69809993 -0.20996862 -2.34841896  2.76050051 -3.16921717]\n",
      " [ 2.22793491  2.34320739 -4.92577398  2.06883897 -3.34916043]\n",
      " [-0.77798671 -1.29867108 -3.14496814  0.15478615 -3.09581511]]\n",
      "[-0.57858866 -0.21428895 -0.39364814 -0.41066459 -0.00925332]\n",
      "grad_W.shape         = (6, 5)\n",
      "grad_b.shape         = (5,)\n",
      "grad_input.shape     = (10, 6)\n",
      "Testing affine_backward function:\n",
      "grad_input error:  1.7513525864769402e-10\n",
      "grad_W error:      0.0006785753951179602\n",
      "grad_b error:      0.818406012402901\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "\n",
    "input = np.random.randn(10, 6)\n",
    "W = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "grad_output = np.random.randn(10, 5)\n",
    "\n",
    "print('input.shape =', input.shape)\n",
    "print('W.shape     =', W.shape)\n",
    "print('b.shape     =', b.shape)\n",
    "\n",
    "dense = Dense(5, W_initializer=W, b_initializer=b)\n",
    "dense.initialize({'input_shape': (-1, 6), 'dtype': np.float32, 'seed': 1, 'names': {}})\n",
    "\n",
    "num_grad_W     = eval_numerical_gradient_layer_param(dense, 'W', input, grad_output)\n",
    "num_grad_b     = eval_numerical_gradient_layer_param(dense, 'b', input, grad_output)    \n",
    "num_grad_input = eval_numerical_gradient_layer_input(dense, input, grad_output)\n",
    "print('num_grad_W.shape     =', num_grad_W.shape)\n",
    "print('grad_b.shape         =', num_grad_b.shape)\n",
    "print('num_grad_input.shape =', num_grad_input.shape)\n",
    "\n",
    "\n",
    "grad_input = dense.backward(input, grad_output)\n",
    "grad_W = dense.grad_W\n",
    "grad_b = dense.grad_b\n",
    "print('grad_W.shape         =', grad_W.shape)\n",
    "print('grad_b.shape         =', grad_b.shape)\n",
    "print('grad_input.shape     =', grad_input.shape)\n",
    "\n",
    "#The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('grad_input error: ', rel_error(num_grad_input, grad_input))\n",
    "print('grad_W error:     ', rel_error(num_grad_W, grad_W))\n",
    "print('grad_b error:     ', rel_error(num_grad_b, grad_b))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.793744  , -2.1426065 , -3.9418273 , -4.1122227 , -0.09265885],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax'></a>\n",
    "### Softmax [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/softmax.py\n",
    "\n",
    "#import numpy as np\n",
    "#from ..sequential import Layer\n",
    "\n",
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        self.assert_nans(input)\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        np.exp(self.output, self.output)\n",
    "        self.output /= np.sum(self.output, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        self.assert_nans(grad_output)\n",
    "        G = np.multiply(self.output, grad_output)\n",
    "        self.grad_input = G - self.output * np.sum(G, axis=1, keepdims=True)\n",
    "        #assert self.grad_input.shape == grad_output.shape\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': numpy.float32,\n",
       " 'input_shape': (10, 25),\n",
       " 'names': {'SoftMax': 1},\n",
       " 'seed': 1}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = SoftMax()\n",
    "softmax.initialize({'input_shape': (10, 25), 'dtype': np.float32, 'seed': 1, 'names': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax'></a>\n",
    "### Dropout [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/dropout.py\n",
    "\n",
    "#import numpy as np\n",
    "#from ..sequential import Layer\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def _initialize(self, params):\n",
    "        # Check params and initialize name\n",
    "        params = super()._initialize(params)\n",
    "        seed = params['seed']\n",
    "        self.gen = np.random.RandomState(seed)\n",
    "        params['seed'] += 1\n",
    "        return params\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        if self.training:\n",
    "            self.mask = self.gen.choice([0, 1], p=[self.p, 1 - self.p], size=input.shape)\n",
    "            self.output = np.multiply(self.mask, input)\n",
    "        else:\n",
    "            self.output = (1 - self.p) * input\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        if self.training:\n",
    "            self.grad_input = np.multiply(self.mask, grad_output)\n",
    "        else:\n",
    "            self.grad_input = (1 - self.p) * grad_output\n",
    "        return self.grad_input\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        self.mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'seed': 2, 'dtype': <class 'numpy.float32'>, 'names': {'Dropout': 1}, 'input_shape': (10, 25)}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dropout = Dropout(0.5)\n",
    "print(dropout.initialized)\n",
    "print(dropout.initialize({'input_shape': (10, 25), 'dtype': np.float32, 'seed': 1, 'names': {}}))\n",
    "print(dropout.initialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax'></a>\n",
    "### BatchNormalization [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/batch_normalization.py\n",
    "#import numpy as np\n",
    "#from collections import OrderedDict\n",
    "#from ..sequential import Layer\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var  = momentum * running_var  + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _initialize(self, params):\n",
    "        # Check params and initialize name\n",
    "        params = super()._initialize(params)\n",
    "        input_shape = params['input_shape']\n",
    "        dtype = params['dtype']\n",
    "        n_features = input_shape[1]\n",
    "        self.running_mean = np.zeros(n_features, dtype=dtype)\n",
    "        self.running_var = np.zeros(n_features, dtype=dtype)\n",
    "        self.gamma = np.ones(n_features, dtype=dtype)\n",
    "        self.beta = np.zeros(n_features, dtype=dtype)\n",
    "        return params\n",
    "        \n",
    "    def update_output(self, input):\n",
    "        if self.training:\n",
    "            self.sample_mean = np.mean(input, axis=0)\n",
    "            self.sample_var = np.var(input, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.sample_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.sample_var\n",
    "            self.normed_input = (input - self.sample_mean[None, :]) / np.sqrt(self.sample_var + eps)[None, :]\n",
    "            self.output = self.gamma[None, :] * self.normed_input + self.beta[None, :]\n",
    "        else:\n",
    "            input_norm = (X - self.running_mean[None, :]) / (np.sqrt(running_var[None, :] + eps))\n",
    "            self.output = self.gamma[None, :] * input_norm + self.beta[None, :]\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        if self.traning:\n",
    "            var = self.sample_var\n",
    "        else:\n",
    "            var = self.running_var\n",
    "        self.grad_input = self.gamma / np.sqrt(var + self.eps)[None, :] *\\\n",
    "                ((grad_output - np.mean(grad_output, axis=0)[None, :]) -\\\n",
    "                 self.normed_input * np.mean(np.multiply(self.normed_input, grad_output), axis=0)[None, :]) \n",
    "        \n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        self.grad_gamma = np.sum(np.multiply(self.normed_input, grad_output), axis=0)\n",
    "        self.grad_beta = np.sum(grad_output, axis=0)\n",
    "          \n",
    "    def get_params(self):\n",
    "        return OrderedDict([(self.name + '/gamma', self.gamma), (self.name + '/beta', self.beta)])\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict([(self.name + '/gamma', self.grad_gamma), (self.name + '/beta', self.grad_beta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'seed': 1, 'dtype': <class 'numpy.float32'>, 'names': {'BatchNormalization': 1}, 'input_shape': (10, 25)}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "bn = BatchNormalization()\n",
    "print(bn.initialized)\n",
    "print(bn.initialize({'input_shape': (10, 25), 'dtype': np.float32, 'seed': 1, 'names': {}}))\n",
    "print(bn.initialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
