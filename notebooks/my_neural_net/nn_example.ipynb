{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "* [1. Modules](#modules)\n",
    "    * [1.1 Base objects](#base)\n",
    "        * [1.1.1 Layer](#layer)\n",
    "        * [1.1.2 Sequential](#seq)\n",
    "        * [1.1.3 Model](#model)\n",
    "    * [1.2 Helpers](#helpers)\n",
    "        * [1.2.1 Initializers](#initializers)\n",
    "        * [1.2.2 Regularizers](#regularizers)\n",
    "        * [1.2.3 Gradients checker](#grad_checker)\n",
    "    * [1.3 Layers](#layers)\n",
    "        * [1.3.1 Dense](#dense)\n",
    "            * [1.3.1.1 Dense: forward](#dense_forward)\n",
    "            * [1.3.1.2 Dense: backward](#dense_backward)\n",
    "        * [1.3.2 Softmax](#softmax)\n",
    "        * [1.3.3 Dropout](#dropout)\n",
    "            * [1.3.3.1 Dropout: forward](#dropout_forward)\n",
    "            * [1.3.3.2 Dropout: backward](#dropout_backward)\n",
    "        * [1.3.4 BatchNormalization](#bn)\n",
    "            * [1.3.4.1 BatchNormalization: forward](#bn_forward)\n",
    "            * [1.3.4.2 BatchNormalization: backward](#bn_backward)\n",
    "    * [1.4 Criterions](#criterions)        \n",
    "    * [1.5 Optimizers](#optimizers)\n",
    "    * [1.6 Solver](#solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layer'></a>\n",
    "### 1.1.1 Layer [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Object <__main__.A object at 0x7fb25516dac8> must be initialized to call its methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5f0f9020edf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-5f0f9020edf9>\u001b[0m in \u001b[0;36mfn_\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Object {} must be initialized to call its methods.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Object <__main__.A object at 0x7fb25516dac8> must be initialized to call its methods."
     ]
    }
   ],
   "source": [
    "def dtype_conversion(fn):\n",
    "    def fn_(self=None, *args, **kwargs):\n",
    "        dtype = self.dtype\n",
    "        return fn(self, \n",
    "                  *[arg.astype(dtype, copy=False) for arg in args], \n",
    "                  **{k: v.astype(dtype, copy=False) for k, v in kwargs.items()})\n",
    "    return fn_\n",
    "\n",
    "def check_initialized(fn):\n",
    "    def fn_(self=None, *args, **kwargs):\n",
    "        assert self.initialized, 'Object {} must be initialized to call its methods.'.format(self)\n",
    "        return fn(self, *args, **kwargs)\n",
    "    return fn_\n",
    "\n",
    "class A:\n",
    "    def __init__(self, dtype=np.float32, initialized=True):\n",
    "        self.dtype = dtype\n",
    "        self.initialized = initialized\n",
    "        \n",
    "    @check_initialized\n",
    "    @dtype_conversion\n",
    "    def sum(self, a, b):\n",
    "        print(a.dtype, b.dtype)\n",
    "        return a + b\n",
    "\n",
    "np.random.seed(45)\n",
    "a = np.random.randn(5)\n",
    "b = np.random.randn(5)\n",
    "s = A(np.float32, False)\n",
    "s.sum(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/sequential/layer.py\n",
    "\n",
    "#import numpy as np\n",
    "#from collections import OrderedDict\n",
    "\n",
    "class Layer:\n",
    "    # Checks\n",
    "    def _assert_nans(self, arr):\n",
    "        assert not np.any(np.isnan(arr)), 'NaNs etected!'\n",
    "    def _assert_infs(self, arr):\n",
    "        assert not np.any(np.isinf(arr)), 'Infs detected!'\n",
    "    def _check_arrays(self, *arrays):\n",
    "        if self.debug:\n",
    "            for arr in arrays:\n",
    "                self._assert_nans(arr)\n",
    "                self._assert_infs(arr)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = None       # Output of the layer is always kept for backpropagatoin\n",
    "        self.grad_input = None   # Input gradient is saved just in case\n",
    "        self.training = True     # \n",
    "        \n",
    "        self.forward_enter_call  = lambda: None\n",
    "        self.forward_exit_call   = lambda: None\n",
    "        self.backward_enter_call = lambda: None\n",
    "        self.backward_exit_call  = lambda: None\n",
    "        \n",
    "        self.dtype = None         # Must be set during initialization\n",
    "        self.debug = False        # Must be set during initialization\n",
    "        self.initialized = False  # Must be set to True after initialization\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__\n",
    "    \n",
    "    # Setting callbacks\n",
    "    def set_forward_enter_call(self, callback=lambda: None):\n",
    "        self.forward_enter_call = callback\n",
    "    def set_forward_exit_call(self, callback=lambda: None):\n",
    "        self.forward_exti_call = callback\n",
    "    def set_backward_enter_call(self, callback=lambda: None):\n",
    "        self.backward_enter_call = callback\n",
    "    def set_backward_exit_call(self, callback=lambda: None):\n",
    "        self.backward_exit_call = callback\n",
    "    \n",
    "    # Initialization\n",
    "    def initialize(self, params):\n",
    "        \"\"\"Thist function is called during compilation process to initialize layer\"\"\"\n",
    "        params = self._initialize(params)\n",
    "        self.initialized = True\n",
    "        return params\n",
    "    def _initialize(self, params):\n",
    "        \"\"\"Must be called at each layer via super()._initialize(params) at the beginning of layer.initialize() call\"\"\"\n",
    "        params = self._check_initialization_params(params)\n",
    "        params = self._initialize_name(params)\n",
    "        self.debug = params['debug']\n",
    "        self.dtype = params['dtype']\n",
    "        return params\n",
    "    def _check_initialization_params(self, params):\n",
    "        assert 'input_shape' in params, 'Input shape must be provided.' # This is probably not critical\n",
    "        params.setdefault('seed', 0)\n",
    "        params.setdefault('names', {})\n",
    "        params.setdefault('debug', False)\n",
    "        params.setdefautl('dtype', np.float64)\n",
    "        return params\n",
    "    def _initialize_name(self, params):\n",
    "        names = params['names']\n",
    "        layer_type_name = type(self).__name__\n",
    "        n_layers = names.setdefault(layer_type_name, 0)\n",
    "        self.name = layer_type_name + str(n_layers)\n",
    "        names[layer_type_name] += 1\n",
    "        return params\n",
    "\n",
    "    # Forward propagation\n",
    "    @check_initialized\n",
    "    @dtype_conversion\n",
    "    def forward(self, input):\n",
    "        assert self.initialized\n",
    "        self._check_arrays(input) # Check\n",
    "        self.update_output(input) # Finding output tensor\n",
    "        if self.fcall is not None: self.fcall()    # Callback during forward propagation\n",
    "        return self.output\n",
    "    def update_output(self, input):\n",
    "        \"\"\"Must update self.output\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Backward propagation\n",
    "    @check_initialized\n",
    "    @dtype_conversion\n",
    "    def backward(self, input, grad_output):\n",
    "        assert self.initialized # Otherwise some params are not initialized and self.dtype is not available\n",
    "        # Checks and transformations\n",
    "        self.assert_nans(grad_output)\n",
    "        self.assert_infs(grad_output)\n",
    "        grad_output = grad_output.astype(self.dtype, copy=False)\n",
    "        input = input.astype(self.dtype, copy=False)\n",
    "        # Backprop\n",
    "        self.update_grad_input(input, grad_output) # This updates self.grad_input\n",
    "        self.update_grad_param(input, grad_output)\n",
    "        if self.bcall is not None: self.bcall()    # Callback during backward propagation\n",
    "        return self.grad_input\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        \"\"\"Must update self.grad_input\"\"\"\n",
    "        pass\n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        pass\n",
    "\n",
    "    # Regulariation\n",
    "    @check_initialized\n",
    "    def get_regularization_loss(self):\n",
    "        return 0.0\n",
    "    \n",
    "    # Getting params and gradients\n",
    "    @check_initialized\n",
    "    def get_params(self):\n",
    "        return OrderedDict()\n",
    "    @check_initialized\n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict()\n",
    "    @check_initialized\n",
    "    def set_params(self, new_params):\n",
    "        params = self.get_params()\n",
    "        for param_name in new_params:\n",
    "            np.copyto(params[param_name], new_params[param_name]) \n",
    "\n",
    "    # Changing network mode\n",
    "    @check_initialized\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    @check_initialized\n",
    "    def evaluate(self):\n",
    "        self.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='seq'></a>\n",
    "### 1.1.2 Sequential [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/sequential/sequential.py\n",
    "\n",
    "#import numpy as np\n",
    "#import copy\n",
    "#from collections import OrderedDict\n",
    "#from .layer import Layer\n",
    "\n",
    "class Sequential(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "    def __repr__(self):\n",
    "        return '->'.join([str(layer) for layer in self.layers])\n",
    "    def __getitem__(self, n):\n",
    "        return self.layers[n]\n",
    "    \n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # Initialization\n",
    "    def compile(self, config):\n",
    "        \"\"\"\n",
    "        Compilation stage for all layers in the network:\n",
    "            sets random seeds\n",
    "            sets dtypes\n",
    "            sets names\n",
    "            runs parameters initialization\n",
    "        \"\"\"\n",
    "    \n",
    "    def initialize(self, params):\n",
    "        self._check_initialization_params(params)\n",
    "        for n_layer, layer in enumerate(self.layers):\n",
    "            params = layer.initialize(params)\n",
    "        return params\n",
    "\n",
    "    # Forward propagation\n",
    "    def update_output(self, input):\n",
    "        \"\"\"This function passes input through all layers and saves output\"\"\"\n",
    "        for n_layer, layer in enumerate(self.layers):\n",
    "            output = layer.forward(input)\n",
    "            input = output\n",
    "        self.output = output\n",
    "        return self.output\n",
    "        \n",
    "    # Backward propagation\n",
    "    def backward(self, input, grad_output):\n",
    "        n_layers = len(self.layers)\n",
    "        for n_layer in reversed(list(range(1, n_layers))):\n",
    "            grad_output = self.layers[n_layer].backward(self.layers[n_layer - 1].output, grad_output)\n",
    "        self.grad_input = self.layers[0].backward(input, grad_output)\n",
    "        return self.grad_input\n",
    "        \n",
    "    # Get params and their gradients\n",
    "    def get_params(self):\n",
    "        assert self.isinitialized\n",
    "        params = OrderedDict()\n",
    "        for layer in self.layers.get_params():\n",
    "            for param_name, param_value in layer.items():\n",
    "                params[param_name] = param_value\n",
    "        return params\n",
    "    def get_grad_params(self):\n",
    "        assert self.isinitialized\n",
    "        grad_params = OrderedDict()\n",
    "        for layer in self.layers:\n",
    "            for grad_name, grad_value in layer.get_grad_params().items():\n",
    "                grad_params[grad_name] = grad_value\n",
    "        return grad_params\n",
    "    def zero_grad_params(self):\n",
    "        assert self.isinitialized\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad_params()\n",
    "            \n",
    "    # Regularization\n",
    "    def get_regularization_loss(self):\n",
    "        loss = 0.0\n",
    "        for layer in self.layers:\n",
    "            loss += layer.get_regularization_loss()\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets all layers to training mode\"\"\"\n",
    "        assert self.initialized\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "    def evaluate(self):\n",
    "        \"\"\"Sets all layers to evaluation mode\"\"\"\n",
    "        assert self.initialized\n",
    "        for layer in self.layers:\n",
    "            layer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "### 1.1.3 Model [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sequential, criterion):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='helpers'></a>\n",
    "## 1.2 Helper objects [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initializers'></a>\n",
    "### 1.2.1 Initializers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/initializers/initializers.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class DeterministicInitializer(Initializer):\n",
    "    def __init__(self, init_value):\n",
    "        self.init_value = init_value\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        return self.init_value.astype(dtype)\n",
    "\n",
    "\n",
    "class RandomInitializer(Initializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__()\n",
    "        self.gen = np.random.RandomState(seed)\n",
    "\n",
    "        \n",
    "class ZerosInitializer(Initializer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        if shape is None:\n",
    "            return 0.0\n",
    "        return np.zeros(shape, dtype=dtype)\n",
    "\n",
    "\n",
    "class NormalInitializer(RandomInitializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        stddev = 1.0\n",
    "        if len(shape) == 2:\n",
    "            stddev = 1. / np.sqrt(shape[0])\n",
    "        if len(shape) == 4:\n",
    "            stddev = 1.0 / np.sqrt(np.prod(shape[1:]))\n",
    "        return self.gen.uniform(-stddev, stddev, size=shape).astype(dtype)\n",
    "\n",
    "\n",
    "class NormalInitializer(RandomInitializer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        \n",
    "    def __call__(self, shape=None, dtype=np.float32):\n",
    "        stddev = 1.0\n",
    "        if len(shape) == 2:\n",
    "            stddev = 1. / np.sqrt(shape[0])\n",
    "        if len(shape) == 4:\n",
    "            stddev = 1.0 / np.sqrt(np.prod(shape[1:]))\n",
    "        return self.gen.normal(loc=0, scale=stddev, size=shape).astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularizers'></a>\n",
    "### 1.2.2 Regularizers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/regularizers/regularizers.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Regularizer:\n",
    "    pass\n",
    "\n",
    "\n",
    "class EmptyRegularizer(Regularizer):\n",
    "    def __bool__(self):\n",
    "        return False\n",
    "\n",
    "\n",
    "class L2regularizer(Regularizer):\n",
    "    def __init__(self, l2=0.0):\n",
    "        self.l2 = l2\n",
    "    def __bool__(self):\n",
    "        return True\n",
    "    def loss(self, arr):\n",
    "        return 0.5 * self.l2 * np.sum(arr ** 2)\n",
    "    def grad(self, arr):\n",
    "        return self.l2 * arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grad_checker'></a>\n",
    "### 1.2.3 Gradients Checker [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "class GradientsChecker:\n",
    "    def __init__(self, step=1e-5):\n",
    "        self.step = step\n",
    "        \n",
    "    def eval_gradients(self, layer, input, grad_output):\n",
    "        assert isinstance(input, np.ndarray) # It must be an array\n",
    "        self.layer = layer\n",
    "        self.input = input\n",
    "        self.num_grad_input = eval_numerical_gradient_array(self.forward, input, grad_output, self.step)\n",
    "        self.num_grad_params = OrderedDict()\n",
    "        for param_name, param_value in layer.get_params().items():\n",
    "            self.num_grad_params[param_name] = eval_numerical_gradient_array(\n",
    "                self.forward, param_value, grad_output, self.step)\n",
    "        self.grad_input  = layer.backward(input, grad_output)\n",
    "        self.grad_params = layer.get_grad_params()\n",
    "        \n",
    "    def __call__(self, layer, input, grad_output):\n",
    "        self.eval_gradients(layer, input, grad_output)\n",
    "        for param_name in layer.get_params():\n",
    "            print('grad_{} error: {}'.format(param_name, \n",
    "                  rel_error(self.num_grad_params[param_name], self.grad_params[param_name])))\n",
    "        print('grad_X error: {}'.format(rel_error(self.num_grad_input, self.grad_input)))\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.layer.forward(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='layers'></a>\n",
    "## 1.3 Layers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dense'></a>\n",
    "### 1.3.1 Dense [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/dense.py\n",
    "\n",
    "#import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, units, use_bias=True, \n",
    "                 W_initializer=None, b_initializer=None, \n",
    "                 W_regularizer=None, b_regularizer=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - units - Integer or Long, dimensionality of the output space.\n",
    "        - W_initializer\n",
    "        - b_initializer\n",
    "        - seed - used for initializers!!!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W_initializer = W_initializer\n",
    "        self.b_initializer = b_initializer\n",
    "        self.W_regularizer = W_regularizer\n",
    "        self.b_regularizer = b_regularizer\n",
    "        self.params = OrderedDict()\n",
    "        self.grad_params = OrderedDict()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Dense({}->{})'.format(self.input_size, self.output_size)   \n",
    "    \n",
    "    # Initialization\n",
    "    def _initialize(self, params):\n",
    "        # Params check and name initialization\n",
    "        params = super()._initialize(params)\n",
    "\n",
    "        # Initializing params and grads\n",
    "        params = self._initialize_W(params)\n",
    "        params = self._initialize_b(params)\n",
    "        \n",
    "        # Regularization\n",
    "        if self.W_regularizer is None: self.W_regularizer = EmptyRegularizer()\n",
    "        if self.b_regularizer is None: self.b_regularizer = EmptyRegularizer()\n",
    "        return params\n",
    "\n",
    "    def _initialize_W(self, params):\n",
    "        input_shape = params['input_shape']\n",
    "        seed = params['seed']\n",
    "        dtype = params['dtype']\n",
    "        if self.W_initializer is None:\n",
    "            self.W_initializer = NormalInitializer(seed=seed)\n",
    "        elif isinstance(self.W_initializer, np.ndarray):\n",
    "            assert self.W_initializer.shape == (input_shape[1], self.units)\n",
    "            self.W_initializer = DeterministicInitializer(self.W_initializer)\n",
    "        else:\n",
    "            assert False\n",
    "        self.W = self.W_initializer(shape=(input_shape[1], self.units), dtype=dtype)\n",
    "        self.grad_W = np.zeros_like(self.W, dtype=dtype)\n",
    "        params['seed'] = seed + 1\n",
    "        params['input_shape'] = (input_shape[0], self.units) # Input shape for the next layer\n",
    "        return params\n",
    "        \n",
    "    def _initialize_b(self, params):\n",
    "        dtype = params['dtype']\n",
    "        if self.b_initializer is None:\n",
    "            self.b_initializer = ZerosInitializer()\n",
    "        elif isinstance(self.b_initializer, np.ndarray):\n",
    "            assert self.b_initializer.shape == (self.units,)\n",
    "            self.b_initializer = DeterministicInitializer(self.b_initializer)\n",
    "        else:\n",
    "            assert False\n",
    "        self.b = self.b_initializer(shape=(self.units,), dtype=dtype)\n",
    "        self.grad_b = np.zeros_like(self.b, dtype=dtype)\n",
    "        return params\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        self.assert_nans(input)\n",
    "        self.output = np.dot(input, self.W)  # [B x I] x [I x O] = [B x O]\n",
    "        if self.use_bias:\n",
    "            self.output += self.b[None, :]\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        self.grad_input = np.dot(grad_output, self.W.T)         # [N x H] x [H x D] = [N x D]\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        self.grad_W = np.dot(input.T, grad_output)               # ([D x N] x [N x H]).T = [D, H]\n",
    "        if self.W_regularizer:\n",
    "            self.grad_W += self.W_regularizer.grad(self.W)\n",
    "        if self.use_bias:\n",
    "            self.grad_b = np.sum(grad_output, axis=0)\n",
    "            if self.b_regularizer:\n",
    "                self.grad_b += self.b_regularizer.grad(self.b)\n",
    "\n",
    "    def get_regularization_loss(self):\n",
    "        loss = 0.0\n",
    "        if self.W_regularizer:\n",
    "            loss += self.W_regularizer.loss(self.W)\n",
    "        if self.use_bias:\n",
    "            if self.b_regularizer:\n",
    "                loss += self.b_regularizer.loss(self.b)  \n",
    "        return loss\n",
    " \n",
    "    def get_params(self):\n",
    "        return OrderedDict([(self.name + ':W', self.W), (self.name + ':b', self.b)])\n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict([(self.name + ':W', self.grad_W), (self.name + ':b', self.grad_b)])\n",
    "    def zero_grad_params(self):\n",
    "        self.grad_W.fill(0)\n",
    "        self.grad_b.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': numpy.float64,\n",
       " 'input_shape': (10, 100),\n",
       " 'names': {'Dense': 1},\n",
       " 'seed': 1}"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = Dense(units=100)\n",
    "dense.initialize({'input_shape': (10, 20)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dense_forward'></a>\n",
    "#### 1.3.1.1 Dense: forward [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.7698488884e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "batch_size = 2\n",
    "input_size = 120\n",
    "output_size = 3\n",
    "\n",
    "dtype = np.float64\n",
    "X = np.linspace(-0.1, 0.5, num=batch_size * input_size).reshape(batch_size, input_size) # [2, 120]\n",
    "W = np.linspace(-0.2, 0.3, num=input_size * output_size).reshape(input_size, output_size) # [360] - > [120, 3]\n",
    "b = np.linspace(-0.3, 0.1, num=output_size)\n",
    "X = X.astype(dtype)\n",
    "W = W.astype(dtype)\n",
    "b = b.astype(dtype)\n",
    "dense = Dense(output_size, W_initializer=W, b_initializer=b)\n",
    "dense.initialize({'input_shape': (-1, 120)})\n",
    "\n",
    "output = dense.forward(X)\n",
    "correct_output = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                           [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(output, correct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dense_backward'></a>\n",
    "#### 1.3.1.2 Dense: backward [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (10, 6)\n",
      "W.shape = (6, 5)\n",
      "b.shape = (5,)\n",
      "grad_Dense0:W error: 2.1752635504596857e-10\n",
      "grad_Dense0:b error: 7.736978834487815e-12\n",
      "grad_X error: 1.0908199508708189e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "dtype = np.float64\n",
    "X = np.random.randn(10, 6).astype(dtype)\n",
    "W = np.random.randn(6, 5).astype(dtype)\n",
    "b = np.random.randn(5).astype(dtype)\n",
    "grad_Y = np.random.randn(10, 5).astype(dtype)\n",
    "print('X.shape =', X.shape)\n",
    "print('W.shape =', W.shape)\n",
    "print('b.shape =', b.shape)\n",
    "\n",
    "dense = Dense(5, W_initializer=W, b_initializer=b)\n",
    "dense.initialize({'input_shape': X.shape})\n",
    "grad_checker = GradientsChecker()\n",
    "grad_checker(dense, X, grad_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax'></a>\n",
    "### 1.3.2 Softmax [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/softmax.py\n",
    "\n",
    "#import numpy as np\n",
    "#from ..sequential import Layer\n",
    "\n",
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        np.exp(self.output, self.output)\n",
    "        self.output /= np.sum(self.output, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        G = np.multiply(self.output, grad_output)\n",
    "        self.grad_input = G - self.output * np.sum(G, axis=1, keepdims=True)\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_shape': (10, 25), 'names': {'SoftMax': 1}, 'dtype': <class 'numpy.float64'>, 'seed': 0}\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftMax()\n",
    "print(softmax.initialize({'input_shape': (10, 25)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "### 1.3.3 Dropout [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/dropout.py\n",
    "\n",
    "#import numpy as np\n",
    "#from ..sequential import Layer\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    # initialization\n",
    "    def _initialize(self, params):\n",
    "        # Check params and initialize name\n",
    "        params = super()._initialize(params)\n",
    "        seed = params['seed']\n",
    "        self.gen = np.random.RandomState(seed)\n",
    "        params['seed'] += 1\n",
    "        return params\n",
    "    \n",
    "    # Setters\n",
    "    def set_p(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    # Forward propagation\n",
    "    def update_output(self, input):\n",
    "        if self.training:\n",
    "            self.mask = self.gen.choice([0, 1], p=[self.p, 1 - self.p], size=input.shape)\n",
    "            self.output = np.multiply(self.mask, input)\n",
    "        else:\n",
    "            self.output = (1 - self.p) * input\n",
    "        return self.output\n",
    "    \n",
    "    # Backward propagation\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        if self.training:\n",
    "            self.grad_input = np.multiply(self.mask, grad_output)\n",
    "        else:\n",
    "            self.grad_input = (1 - self.p) * grad_output\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': numpy.float64,\n",
       " 'input_shape': (10, 25),\n",
       " 'names': {'Dropout': 1},\n",
       " 'seed': 1}"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = Dropout(0.5)\n",
    "dropout.initialize({'input_shape': (10, 25)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout_forward'></a>\n",
    "#### Dropout: forward [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests with p =  0.3\n",
      "Mean of input:  10.0002078785\n",
      "Mean of train-time output:  7.00311276126\n",
      "Mean of test-time output:  7.00014551493\n",
      "Fraction of train-time output set to zero:  0.299804\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.6\n",
      "Mean of input:  10.0002078785\n",
      "Mean of train-time output:  3.98521948532\n",
      "Mean of test-time output:  4.00008315139\n",
      "Fraction of train-time output set to zero:  0.60156\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.75\n",
      "Mean of input:  10.0002078785\n",
      "Mean of train-time output:  2.52026295957\n",
      "Mean of test-time output:  2.50005196962\n",
      "Fraction of train-time output set to zero:  0.748108\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "X = np.random.randn(500, 500) + 10\n",
    "dropout = Dropout(0)\n",
    "dropout.initialize({'input_shape': (10, 25)})\n",
    "\n",
    "for p in [0.3, 0.6, 0.75]:\n",
    "    dropout.p = p\n",
    "    dropout.train()\n",
    "    out      = dropout.forward(X)\n",
    "    dropout.evaluate()\n",
    "    out_test = dropout.forward(X)\n",
    "    \n",
    "    print('Running tests with p = ', p)\n",
    "    print('Mean of input: ', X.mean())\n",
    "    print('Mean of train-time output: ', out.mean())\n",
    "    print('Mean of test-time output: ', out_test.mean())\n",
    "    print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "    print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout_backward'></a>\n",
    "#### Dropout: backward [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_X relative error:  1.89289321191e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "X = np.random.randn(5, 5) + 10\n",
    "grad_Y = np.random.randn(*X.shape)\n",
    "\n",
    "dropout = Dropout(0.8)\n",
    "# The code below resets state of the dropout layer after each forward propagation\n",
    "forward_callback = lambda: dropout.initialize({'input_shape': (10, 25)})\n",
    "dropout.set_fcall(forward_callback)\n",
    "forward_callback()\n",
    "\n",
    "grad_checker = GradientsChecker()\n",
    "grad_checker.eval_gradients(dropout, X, grad_Y)\n",
    "grad_X     = grad_checker.grad_input\n",
    "num_grad_X = grad_checker.num_grad_input\n",
    "\n",
    "print('grad_X relative error: ', rel_error(grad_X, num_grad_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bn'></a>\n",
    "### 1.3.4 BatchNormalization [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/layers/batch_normalization.py\n",
    "#import numpy as np\n",
    "#from collections import OrderedDict\n",
    "#from ..sequential import Layer\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var  = momentum * running_var  + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _initialize(self, params):\n",
    "        # Check params and initialize name\n",
    "        params = super()._initialize(params)\n",
    "        input_shape = params['input_shape']\n",
    "        dtype = params['dtype']\n",
    "        n_features = input_shape[1]\n",
    "        self.running_mean = np.zeros(n_features, dtype=dtype)\n",
    "        self.running_var = np.zeros(n_features, dtype=dtype)\n",
    "        self.gamma = np.ones(n_features, dtype=dtype)\n",
    "        self.beta = np.zeros(n_features, dtype=dtype)\n",
    "        return params\n",
    "\n",
    "    # Forward propagation\n",
    "    def update_output(self, input):\n",
    "        if self.training:\n",
    "            self.sample_mean = np.mean(input, axis=0)\n",
    "            self.sample_var = np.var(input, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.sample_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.sample_var\n",
    "            self.normed_input = (input - self.sample_mean[None, :]) / np.sqrt(self.sample_var + self.eps)[None, :]\n",
    "            self.output = self.gamma[None, :] * self.normed_input + self.beta[None, :]\n",
    "        else:\n",
    "            normed_input = (input - self.running_mean[None, :]) / (np.sqrt(self.running_var[None, :] + self.eps))\n",
    "            self.output = self.gamma[None, :] * normed_input + self.beta[None, :]\n",
    "        return self.output\n",
    "\n",
    "    # Backward propagation\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        if self.training:\n",
    "            var = self.sample_var\n",
    "        else:\n",
    "            var = self.running_var\n",
    "        self.grad_input = self.gamma / np.sqrt(var + self.eps)[None, :] *\\\n",
    "                ((grad_output - np.mean(grad_output, axis=0)[None, :]) -\\\n",
    "                 self.normed_input * np.mean(np.multiply(self.normed_input, grad_output), axis=0)[None, :]) \n",
    "    def update_grad_param(self, input, grad_output):\n",
    "        self.grad_gamma = np.sum(np.multiply(self.normed_input, grad_output), axis=0)\n",
    "        self.grad_beta = np.sum(grad_output, axis=0)\n",
    "          \n",
    "    # Get params and grad_params\n",
    "    def get_params(self):\n",
    "        return OrderedDict([(self.name + '/gamma', self.gamma), (self.name + '/beta', self.beta)])\n",
    "    def get_grad_params(self):\n",
    "        return OrderedDict([(self.name + '/gamma', self.grad_gamma), (self.name + '/beta', self.grad_beta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': numpy.float64,\n",
       " 'input_shape': (10, 25),\n",
       " 'names': {'BatchNormalization': 1},\n",
       " 'seed': 0}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BatchNormalization()\n",
    "bn.initialize({'input_shape': (10, 25)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bn_forward'></a>\n",
    "#### 1.3.4.1 Batch Normalization: forward [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_shape': (200, 3), 'names': {'BatchNormalization': 1}, 'dtype': <class 'numpy.float64'>, 'seed': 0}\n",
      "Before batch normalization:\n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:  [ 27.18502186  34.21455511  37.68611762]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [  1.77635684e-17  -5.32907052e-17   2.13717932e-17]\n",
      "  std:  [ 0.99999999  1.          1.        ]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 0.99999999  1.99999999  2.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "bn = BatchNormalization()\n",
    "print(bn.initialize({'input_shape': a.shape}))\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=0))\n",
    "print('  stds: ', a.std(axis=0))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm = bn.forward(a)\n",
    "print('  mean: ', a_norm.mean(axis=0))\n",
    "print('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "bn.gamma = gamma\n",
    "bn.beta = beta\n",
    "a_norm  = bn.forward(a)\n",
    "print('After batch normalization (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.03927354 -0.04349152 -0.10452688]\n",
      "  stds:  [ 1.01531428  1.01238373  0.97819988]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn = BatchNormalization()\n",
    "bn.initialize({'input_shape': a.shape})\n",
    "bn.gamma = np.ones(D3)\n",
    "bn.beta = np.zeros(D3)\n",
    "bn.train()\n",
    "\n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    a_norm = bn.forward(a)\n",
    "\n",
    "bn.evaluate()\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm = bn.forward(a)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bn_backward'></a>\n",
    "#### 1.3.4.2 Batch Normalization: backward  [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_BatchNormalization0/gamma error: 3.4897447838672933e-11\n",
      "grad_BatchNormalization0/beta error: 8.743146873306484e-11\n",
      "grad_X error: 4.6504237753155334e-07\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 100, 5\n",
    "X      = 5 * np.random.randn(N, D) + 12\n",
    "gamma  = np.random.randn(D)\n",
    "beta   = np.random.randn(D)\n",
    "grad_Y = np.random.randn(N, D)\n",
    "step   = 1e-5\n",
    "\n",
    "grad_checker = GradientsChecker(step=step)\n",
    "bn = BatchNormalization()\n",
    "bn.initialize({'input_shape': X.shape})\n",
    "bn.train()\n",
    "\n",
    "grad_checker(bn, X, grad_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='criterions'></a>\n",
    "## 1.4 Criterions [[toc]](#toc)\n",
    "## TODO add other criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/criterions/criterions.py\n",
    "\n",
    "import numpy as np\n",
    "#from ..sequential import Layer\n",
    "\n",
    "class Criterion(Layer):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "    def forward(self, input, target):\n",
    "        return self.update_output(input, target)\n",
    "    def backward(self, input, target):\n",
    "        return self.update_grad_input(input, target)\n",
    "    def update_output(self, input, target):\n",
    "        assert False\n",
    "    def update_grad_input(self, input, target):\n",
    "        assert False  \n",
    "\n",
    "class MulticlassLogLoss(Criterion):\n",
    "    def __init__(self, n_classes, eps=1e-20):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.n_classes = n_classes\n",
    "    def update_output(self, input, target): \n",
    "        if len(target.shape) > 1:\n",
    "            target = np.argmax(target, axis=1)\n",
    "        assert np.max(target) < self.n_classes\n",
    "        assert np.min(target) >= 0\n",
    "        input_clamp = np.clip(input, self.eps, 1 - self.eps) # Using this trick to avoid numerical errors\n",
    "        self.output = -np.sum(np.log(input_clamp[np.arange(input.shape[0]), target]))\n",
    "        return self.output\n",
    "    def update_grad_input(self, input, target):\n",
    "        if len(target.shape) == 1:\n",
    "            target = np.eye(self.n_classes)[target]\n",
    "        self.grad_input = -np.array(target).astype(np.float64, copy=False)\n",
    "        self.grad_input /= np.maximum(input, self.eps) # Using this trick to avoid numerical errors\n",
    "        self.assert_inf(self.grad_input)\n",
    "        self.assert_nans(self.grad_input)\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimizers'></a>\n",
    "## 1.5 Optimizers [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs vanilla stochastic gradient descent.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    \"\"\"\n",
    "    if config is None: \n",
    "        config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    w -= config['learning_rate'] * dw\n",
    "    return w, config\n",
    "\n",
    "\n",
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "      Setting momentum = 0 reduces to sgd.\n",
    "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "      moving average of the gradients.\n",
    "    \"\"\"\n",
    "    assert isinstance(w, np.ndarray)\n",
    "    assert isinstance(dw, np.ndarray)\n",
    "    if config is None: \n",
    "        config = {}\n",
    "    lr = config.setdefault('learning_rate', 1e-2)\n",
    "    momentum = config.setdefault('momentum', 0.9)\n",
    "    v = config.get('velocity', np.zeros_like(w))\n",
    "    v *= momentum\n",
    "    v -= lr * dw\n",
    "    next_w = w + v\n",
    "    config['velocity'] = v\n",
    "    return next_w, config\n",
    "\n",
    "\n",
    "def rmsprop(x, dx, config=None):\n",
    "    \"\"\"\n",
    "    Uses the RMSProp update rule, which uses a moving average of squared\n",
    "    gradient values to set adaptive per-parameter learning rates.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n",
    "      gradient cache.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - cache: Moving average of second moments of gradients.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    learning_rate = config.setdefault('learning_rate', 1e-2)\n",
    "    decay_rate = config.setdefault('decay_rate', 0.99)\n",
    "    epsilon = config.setdefault('epsilon', 1e-8)\n",
    "    cache = config.setdefault('cache', np.zeros_like(x))\n",
    "    \n",
    "    cache *= decay_rate\n",
    "    cache += (1 - decay_rate) * (dx ** 2)\n",
    "    next_x = x - learning_rate * dx / (np.sqrt(cache) + epsilon)\n",
    "    config['cache'] = cache\n",
    "    return next_x, config\n",
    "\n",
    "\n",
    "def adam(x, dx, config=None):\n",
    "    \"\"\"\n",
    "    Uses the Adam update rule, which incorporates moving averages of both the\n",
    "    gradient and its square and a bias correction term.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - beta1: Decay rate for moving average of first moment of gradient.\n",
    "    - beta2: Decay rate for moving average of second moment of gradient.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - m: Moving average of gradient.\n",
    "    - v: Moving average of squared gradient.\n",
    "    - t: Iteration number.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    learning_rate = config.setdefault('learning_rate', 1e-3)\n",
    "    beta1 = config.setdefault('beta1', 0.9)\n",
    "    beta2 = config.setdefault('beta2', 0.999)\n",
    "    epsilon = config.setdefault('epsilon', 1e-8)\n",
    "    m = config.setdefault('m', np.zeros_like(x))\n",
    "    v = config.setdefault('v', np.zeros_like(x))\n",
    "    t = config.setdefault('t', 0)\n",
    "    \n",
    "    m = beta1 * m + (1 - beta1) * dx\n",
    "    v = beta2 * v + (1 - beta2) * (dx ** 2)\n",
    "    mt = m / (1 - beta1 ** (t + 1))\n",
    "    vt = v / (1 - beta2 ** (t + 1))\n",
    "    next_x = x - learning_rate * mt / (np.sqrt(vt) + epsilon)\n",
    "    \n",
    "    config['m'] = m\n",
    "    config['v'] = v\n",
    "    config['t'] = t + 1\n",
    "    return next_x, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solver'></a>\n",
    "## 1.6 Solver [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../ml/neural_network/cs231n/second/solver.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, model, criterion, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new Solver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "        - data: A dictionary of training and validation data containing:\n",
    "          'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
    "          'X_val':   Array, shape (N_val, d_1, ..., d_k) of validation images\n",
    "          'y_train': Array, shape (N_train,) of labels for training images\n",
    "          'y_val':   Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "        Optional arguments:\n",
    "        - update_rule:       A string giving the name of an update rule in optim.py. Default is 'sgd'.\n",
    "        - optim_config:      A dictionary containing hyperparameters that will be passed to the chosen update rule.\n",
    "            Each update rule requires different hyperparameters (see optim.py) but all update rules require a\n",
    "           'learning_rate' parameter so that should always be present.\n",
    "           'learning_rate_decay': A scalar for learning rate decay; after each epoch the learning rate is multiplied by this value.\n",
    "        - batch_size:        Size of minibatches used to compute loss and gradient during training.\n",
    "        - num_epochs:        The number of epochs to run for during training.\n",
    "        - verbose:           Boolean; if set to False then no output will be printed during training; default is False.\n",
    "        - print_every_iter:  Integer; training losses will be printed every print_every_iter iterations; default is 1000000000.\n",
    "        - print_every_epoch: Integer; training losses will be printed every print_every_epoch epochs; default is 1000000000.\n",
    "        - num_train_samples: Number of training samples used to check training accuracy; default is None, which uses the entire training set.\n",
    "        - num_val_samples:   Number of validation samples to use to check val accuracy; default is None, which uses the entire validation set.\n",
    "        - seed:              Used to initialize an internal random generator; default is 0.\n",
    "        - dtype:\n",
    "        - checkpoint_name:   If not None, then save model checkpoints here every epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack keyword arguments\n",
    "        self.seed         = kwargs.pop('seed', 0)\n",
    "        self.dtype        = kwargs.pop('dtype', np.float64)\n",
    "        \n",
    "        self.update_rule  = kwargs.pop('update_rule', 'sgd')\n",
    "        self.optim_config = kwargs.pop('optim_config', {})\n",
    "        self.batch_size   = kwargs.pop('batch_size', 100)\n",
    "        self.num_epochs   = kwargs.pop('num_epochs', 10)\n",
    "        self.checkpoint_name   = kwargs.pop('checkpoint_name', None)\n",
    "        \n",
    "        self.verbose = kwargs.pop('verbose', False)\n",
    "        self.print_every_iter  = kwargs.pop('print_every_iter', 1000000000)\n",
    "        self.print_every_epoch = kwargs.pop('print_every_epoch', 1000000000)\n",
    "       \n",
    "        \n",
    "        # Unpacking data\n",
    "        self.X_train = data['X_train'].astype(self.dtype)\n",
    "        self.y_train = data['y_train'].astype(self.dtype)\n",
    "        self.X_val   = data['X_val'].astype(self.dtype)\n",
    "        self.y_val   = data['y_val'].astype(self.dtype)\n",
    "\n",
    "        self.gen = np.random.RandomState(self.seed)\n",
    "        self.seed += 1\n",
    "        \n",
    "        # Compiling model\n",
    "        assert isinstance(model, Sequential)\n",
    "        assert isinstance(criterion, Criterion)\n",
    "        self.model = model; self.criterion = criterion\n",
    "        self.model_params = {'input_shape': self.X_train.shape, 'seed': self.seed, 'dtype': np.float64, 'names': {}}\n",
    "        self.model_params = self.model.initialize(self.model_params)\n",
    "        if self.verbose: print(self.model_params)\n",
    "            \n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = ', '.join('\"{}\"'.format(k) for k in sorted(list(kwargs.keys())))\n",
    "            raise ValueError('Unrecognized arguments {}'.format(extra))\n",
    "\n",
    "        # Make sure the update rule exists, then replace the string\n",
    "        # name with the actual function\n",
    "        if self.update_rule == 'sgd': \n",
    "            self.update_rule = sgd\n",
    "        elif self.update_rule == 'sgd_momentum':\n",
    "            self.update_rule = sgd_momentum\n",
    "        elif self.update_rule == 'rmsprop':\n",
    "            self.update_rule = rmsprop\n",
    "        elif self.update_rule == 'adam':\n",
    "            self.update_rule = adam\n",
    "        else:\n",
    "            assert False, 'Unknown update rule \"{}\"'.format(self.update_rule)\n",
    "            \n",
    "        \"\"\"from . import optimizers\n",
    "        if not hasattr(optimizers, self.update_rule):\n",
    "            raise ValueError('Invalid update_rule \"{}\"'.format(self.update_rule))\n",
    "        self.update_rule = getattr(optimizers, self.update_rule)\"\"\"\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization. Don't call this manually.\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.n_epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history  = []\n",
    "        self.val_loss_history = []                  \n",
    "        self.val_acc_history  = []\n",
    "        self.history = {'loss_history':       self.loss_history,\n",
    "                        'train_loss_history': self.train_loss_history,\n",
    "                        'train_acc_history':  self.train_acc_history,\n",
    "                        'val_loss_history':   self.val_loss_history,\n",
    "                        'val_acc_history':    self.val_acc_history}\n",
    "        \n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for param_name in self.model.get_params():\n",
    "            self.optim_configs[param_name] = {k : v for k, v in self.optim_config.items()}\n",
    "\n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called by train() and should not be called manually.\n",
    "        \"\"\"\n",
    "        # Make a minibatch of training data\n",
    "        num_train  = self.X_train.shape[0]\n",
    "        batch_mask = self.gen.choice(num_train, self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "\n",
    "        # Compute loss and gradient\n",
    "        output = self.model.forward(X_batch)\n",
    "        loss, output_grad = self.criterion(output, y_batch)\n",
    "        self.model.backward(output, output_grad)\n",
    "        loss += self.model.get_regularization_loss()\n",
    "        self.loss_history.append(loss)\n",
    "        \n",
    "        # Perform a parameter update\n",
    "        params = self.model.get_params()\n",
    "        grad_params = self.model.get_grad_params()\n",
    "        for param_name, param_value in params().items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        if self.checkpoint_name is None:\n",
    "            return\n",
    "        checkpoint = {\n",
    "          'model':              self.model,\n",
    "          'update_rule':        self.update_rule,\n",
    "          'optim_config':       self.optim_config,\n",
    "          'batch_size':         self.batch_size,\n",
    "          'n_epoch':            self.n_epoch,\n",
    "          'loss_history':       self.loss_history,\n",
    "          'train_loss_history': self.train_loss_history,\n",
    "          'train_acc_history':  self.train_acc_history,\n",
    "          'val_loss_history':   self.val_loss_history,\n",
    "          'val_acc_history':    self.val_acc_history,\n",
    "        }\n",
    "        filename = '{}_epoch_{}.pkl'.format(self.checkpoint_name, int(self.epoch))\n",
    "        if self.verbose:\n",
    "            print('Saving checkpoint to \"{}\"'.format(filename))\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "    def eval(self, X, y, batch_size=100, eval_func=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the provided data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,)\n",
    "        - num_samples: If not None, subsample the data and only test the model\n",
    "          on num_samples datapoints.\n",
    "        - batch_size: Split X and y into batches of this size to avoid using\n",
    "          too much memory.\n",
    "\n",
    "        Returns:\n",
    "        - metric_value: Scalar giving the the value of the required metric.\n",
    "        \"\"\"\n",
    "        # Compute predictions in batches\n",
    "        N = X.shape[0]\n",
    "        num_batches = N // batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        scores = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end   = (i + 1) * batch_size\n",
    "            batch_scores = self.model.forward(X[start:end])\n",
    "            scores.append(batch_scores)\n",
    "        scores = np.vstack(scores)\n",
    "        assert scores.ndim == 2\n",
    "        return eval_func(scores, y)\n",
    "\n",
    "    def _logloss(self, scores, y_true):\n",
    "        n_samples = scores.shape[0]\n",
    "        y_pred = scores - np.max(scores, axis=1, keepdims=True)\n",
    "        y_pred = np.exp(y_pred)\n",
    "        y_pred /= np.sum(y_pred, axis=1, keepdims=True)\n",
    "        y_pred = np.clip(y_pred, 1e-18, 1 - 1e-18)\n",
    "        return -np.mean(np.log(y_pred[np.arange(n_samples), y_true]))\n",
    "        \n",
    "    def _accuracy(self, scores, y_true):\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        return np.mean(y_pred == y_true)\n",
    "    \n",
    "    def _update_history(self):\n",
    "        train_acc  = self.eval(self.X_train, self.y_train, batch_size=self.batch_size, eval_func=self._accuracy)\n",
    "        train_loss = self.eval(self.X_train, self.y_train, batch_size=self.batch_size, eval_func=self._logloss)\n",
    "        val_acc    = self.eval(self.X_val, self.y_val, batch_size=self.batch_size, eval_func=self._accuracy)\n",
    "        val_loss   = self.eval(self.X_val, self.y_val, batch_size=self.batch_size, eval_func=self._logloss)\n",
    "        self.train_acc_history.append(train_acc)\n",
    "        self.val_acc_history.append(val_acc)\n",
    "        self.train_loss_history.append(train_loss)\n",
    "        self.val_loss_history.append(val_loss)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run optimization to train the model.\n",
    "        \"\"\"\n",
    "        num_train = self.X_train.shape[0]\n",
    "        num_iter_per_epoch = int(np.ceil(float(num_train) / self.batch_size))\n",
    "        num_all_iterations = num_iter_per_epoch * self.num_epochs\n",
    "        if self.verbose:\n",
    "            print('num of epochs = {}\\nnum of iterations = {}\\niterations per epoch = {}'.format(\n",
    "                self.num_epochs, num_all_iterations, num_iter_per_epoch))\n",
    "        n_all_iter = 0\n",
    "        \n",
    "        self._update_history() # Initial model quality\n",
    "        for n_epoch in range(self.num_epochs):\n",
    "            self.n_epoch = n_epoch\n",
    "            for n_iter in range(num_iter_per_epoch):\n",
    "                self._step()\n",
    "                # Maybe print training loss\n",
    "                if self.verbose & ((n_all_iter + 1) % self.print_every_iter == 0):\n",
    "                    msg = '(Iteration {}/{}) loss: {}'.format(n_all_iter + 1, num_all_iterations, \n",
    "                                                              self.loss_history[-1])\n",
    "                    print(msg)\n",
    "                n_all_iter += 1\n",
    "            self._update_history()\n",
    "            \n",
    "            # Keep track of the best model\n",
    "            val_acc = self.val_acc_history[-1]\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.best_params  = OrderedDict()\n",
    "                for param_name, param_value in self.model.get_params().items():\n",
    "                    self.best_params[param_name] = param_value.copy()\n",
    "\n",
    "            # Maybe print training loss\n",
    "            if self.verbose & ((n_epoch + 1) % self.print_every_epoch == 0):\n",
    "                msg = '(Epoch {}/{}) train acc: {:.2}; val acc: {:.2}, train loss: {:.4}; val loss: {:.4}'.format(\n",
    "                    self.n_epoch + 1, self.num_epochs, \n",
    "                    self.train_acc_history[-1],  self.val_acc_history[-1],\n",
    "                    self.train_loss_history[-1], self.val_loss_history[-1])\n",
    "                print(msg)\n",
    "            \n",
    "            # Save the model at the end of every epoch\n",
    "            self._save_checkpoint()\n",
    "            \n",
    "            # At the end of every epoch, increment the epoch counter and decay the learning rate.\n",
    "            for k in self.optim_configs:\n",
    "                optim_config = self.optim_configs[k]\n",
    "                lr_decay = optim_config.get('learning_rate_decay', 1.0)\n",
    "                optim_config['learning_rate'] *= lr_decay\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
