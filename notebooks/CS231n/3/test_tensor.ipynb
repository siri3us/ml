{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the cs231n directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ]
    }
   ],
   "source": [
    "import copy, os, sys, gc, json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "from itertools import product, chain, combinations\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "\n",
    "%matplotlib inline\n",
    "#https://matplotlib.org/users/customizing.html\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.01\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['axes.titlesize'] = 20\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "matplotlib.rcParams['image.interpolation'] = 'nearest'\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "# Configuring latex fonts\n",
    "matplotlib.rc('font', **{'family':'serif'})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage{amsmath}')\n",
    "\n",
    "current_dir = !pwd\n",
    "outer_dir, innder_dir = current_dir.get_nlstr().split('git')\n",
    "library_path = os.path.join(outer_dir, 'git/ml')\n",
    "if library_path not in sys.path:\n",
    "    sys.path.append(library_path)\n",
    "\n",
    "\n",
    "from ml.neural_network import Layer, Sequential, Model, Solver\n",
    "from ml.neural_network.layers import *\n",
    "from ml.neural_network.nonlinearities import *\n",
    "from ml.neural_network.criterions import MSECriterion, MulticlassLogLoss\n",
    "from ml.neural_network.regularizers import *\n",
    "from ml.neural_network.initializers import *\n",
    "from ml.neural_network.optimizers import *\n",
    "from ml.neural_network.decorators import *\n",
    "from ml.neural_network.gradients import *\n",
    "from ml.neural_network.dataset import Dataset, CocoDataset\n",
    "from ml.utils.image_utils import image_from_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Tensor](#tensor)\n",
    "* [Variable](#variable)\n",
    "* [Constant](#constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensor_shape'></a>\n",
    "## TensorShape [[toc]](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorShape:\n",
    "    def __init__(self, shape=None):\n",
    "        self.set_shape(shape)\n",
    "    def set_shape(self, shape):\n",
    "        self.initialized = False\n",
    "        if shape is None:\n",
    "            self.shape = None\n",
    "            self.initialized = False\n",
    "        elif isinstance(shape, (tuple, list)):\n",
    "            self.shape = tuple(shape)\n",
    "            self.initialized = True\n",
    "        elif isinstance(shape, TensorShape):\n",
    "            self.shape = shape.get_shape() # так как хранится в immutable tuple-ах, то проблем быть не должно\n",
    "            self.initialized = True\n",
    "        else:\n",
    "            self.initialized = False\n",
    "            raise TypeError('Unacceptable type \"{}\" of \"shape\"'.format(type(shape)))\n",
    "    def get_shape(self):\n",
    "        return self.shape\n",
    "    def is_initialized(self):\n",
    "        return self.initialized\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + '(' + repr(self.shape) + ')'\n",
    "    def __len__(self):\n",
    "        return len(self.shape)\n",
    "    def __getitem__(self, n):\n",
    "        return self.shape[n]\n",
    "    def __iter__(self):\n",
    "        return iter(self.shape)\n",
    "    def __eq__(self, other_shape):\n",
    "        other_shape = TensorShape(other_shape)\n",
    "        # Если хотя бы одна из форм не инициализована, то результат сравнения False\n",
    "        if (not self.is_initialized()) | (not other_shape.is_initialized()):\n",
    "            return False\n",
    "        if len(self.shape) != len(other_shape):\n",
    "            return False\n",
    "        for l, r in zip(self.shape, other_shape):\n",
    "            if (l == -1) | (r == -1):\n",
    "                continue\n",
    "            if l != r:\n",
    "                return False\n",
    "        return True\n",
    "    def __neq__(self, other_shape):\n",
    "        return not self == outher_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    Класс Tensor служит для хранения значения некоторой переменной\n",
    "    \"\"\"\n",
    "    def __init__(self, value=None, shape=None, name=None, dtype=None, validate_shape=True):\n",
    "        \"\"\"\n",
    "        -dtype: если тип dtype не задан, то\n",
    "            либо наследуется от value, если тот имеет аттрибут dtype;\n",
    "            либо полагается равным np.float64\n",
    "        -name:  если имя не задано, то\n",
    "            либо наследуется от value, в случае наличия у value аттрибута name;\n",
    "            либо полагается равным имени типа, т.е. в данном случае Tensor\n",
    "        -validate_shape: проверка последующих изменений\n",
    "        \"\"\"\n",
    "        self.set_name(value, name)\n",
    "        self.set_dtype(value, dtype)\n",
    "        self.set_value(value)\n",
    "        self.validate_shape = validate_shape\n",
    "\n",
    "    def set_name(self, value, name):\n",
    "        self._type_name = type(self).__name__\n",
    "        if name is None:\n",
    "            if hasattr(value, 'name'):\n",
    "                name = name\n",
    "            else:\n",
    "                name = self._type_name\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError('{}.__init__: name must be string.'.format(self._type_name))\n",
    "        self.name = name\n",
    "    def set_dtype(self, value, dtype):\n",
    "        if dtype is None:\n",
    "            if hasattr(value, 'dtype'):\n",
    "                dtype = value.dtype\n",
    "            else:\n",
    "                dtype = np.float64\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def set_value(self, value):\n",
    "        if isinstance(value, Tensor):\n",
    "            value = value.value\n",
    "        self.value = np.array(value, copy=False, dtype=self.dtype)\n",
    "        self.shape = TensorShape(self.value.shape)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{}\\n[name={}, dtype={}\\nshape={}\\nvalue=\\n{}\\n]'.format(\n",
    "            self._type_name, self.name, self.dtype, self.shape, self.value)\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def get_shape(self):\n",
    "        return self.shape\n",
    "    \n",
    "    def __abs__(self):\n",
    "        op_name = 'abs(' + self.name + ')'\n",
    "        return Tensor(np.abs(self.value), dtype=self.dtype, name=op_name)\n",
    "        \n",
    "    def __add__(self, right):\n",
    "        assert isinstance(right, Tensor)\n",
    "        op_name = self.name + '+' + right.name\n",
    "        return Tensor(self.value + right.value, dtype=self.dtype, name=op_name)\n",
    "        \n",
    "    def __bool__(self):\n",
    "        raise TypeError('{} cannot be treated as bool.'.format(type(self).__name__))\n",
    "        \n",
    "    def dot(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            raise TypeError('Right argument of {}.dot must be a Tensor.'.format(self._type_name))\n",
    "        if self.value.ndim != 2 or right.value.ndim != 2:\n",
    "            raise ValueError('{}.dot: values must be a 2-dim arrays but their shapes are {} and {}.'.format(\n",
    "                self._type_name, self.get_shape(), right.get_shape()))\n",
    "        if self.get_shape()[1] != right.get_shape()[0]:\n",
    "            raise ValueError('{}.dot: inconsistent shapes {} and {} of arguments.'.format(\n",
    "                self._type_name, self.get_shape(), right.get_shape()))\n",
    "        op_name = self.name + '.dot(' + right.name + ')'\n",
    "        return Tensor(np.dot(self.value, right.value), dtype=self.dtype, name=op_name)\n",
    "    \n",
    "    def multiply(self, right):\n",
    "        if not isinstance(right, Tensor):\n",
    "            raise TypeError('{}.multiply: right argument must be a Tensor.'.format(self._type_name))\n",
    "        if self.get_shape() != right.get_shape():\n",
    "            raise ValueError('{}.multiply: inconsistent shapes {} and {} of arguments.'.format(\n",
    "                self._type_name, self.get_shape(), right.get_shape()))\n",
    "        op_name = self.name + '.multiply(' + right.name + ')'\n",
    "        return Tensor(np.multiply(self.value, right.value), dtype=self.dtype, name=op_name)\n",
    "    \n",
    "    def __mul__(self, right):\n",
    "        op_name = self.name + '*' + type(right).__name__\n",
    "        return Tensor(self.value * right, dtype=self.dtype, name=op_name)\n",
    "    \n",
    "    def __rmul__(self, left):\n",
    "        op_name = type(left).__name__ + '*' + self.name\n",
    "        return Tensor(left * self.value, dtype=self.dtype, name=op_name)\n",
    "    \n",
    "    def __getitem__(self, slice_index):\n",
    "        return self.value.__getitem__(slice_index)\n",
    "    \n",
    "    def __setitem__(self, slice_index, value):\n",
    "        self.value[slice_index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor\n",
      "[name=t1, dtype=float64\n",
      "shape=TensorShape((4, 5))\n",
      "value=\n",
      "[[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      " [ 1.58248112 -0.9092324  -0.59163666  0.18760323 -0.32986996]\n",
      " [-1.19276461 -0.20487651 -0.35882895  0.6034716  -1.66478853]\n",
      " [-0.70017904  1.15139101  1.85733101 -1.51117956  0.64484751]]\n",
      "]\n",
      "Tensor\n",
      "[name=Tensor, dtype=float64\n",
      "shape=TensorShape((5, 2))\n",
      "value=\n",
      "[[-0.98060789 -0.85685315]\n",
      " [-0.87187918 -0.42250793]\n",
      " [ 0.99643983  0.71242127]\n",
      " [ 0.05914424 -0.36331088]\n",
      " [ 0.00328884 -0.10593044]]\n",
      "]\n",
      "TensorShape((4, 5)) TensorShape((5, 2))\n",
      "Tensor\n",
      "[name=t1.dot(Tensor), dtype=float64\n",
      "shape=TensorShape((4, 2))\n",
      "value=\n",
      "[[ 2.26337556  1.57343799]\n",
      " [-1.33857222 -1.4265056 ]\n",
      " [ 1.02092714  0.81005268]\n",
      " [ 1.44618905  1.91739989]]\n",
      "]\n",
      "Tensor\n",
      "[name=int*t1, dtype=float64\n",
      "shape=TensorShape((4, 5))\n",
      "value=\n",
      "[[ 0.88245497 -0.6617403   4.86154237 -0.50418426  0.21921968]\n",
      " [ 3.16496223 -1.81846481 -1.18327332  0.37520645 -0.65973992]\n",
      " [-2.38552922 -0.40975302 -0.71765789  1.20694321 -3.32957706]\n",
      " [-1.40035808  2.30278202  3.71466201 -3.02235912  1.28969502]]\n",
      "]\n",
      "Tensor\n",
      "[name=Tensor*int, dtype=float64\n",
      "shape=TensorShape((5, 2))\n",
      "value=\n",
      "[[-1.96121577 -1.71370631]\n",
      " [-1.74375837 -0.84501586]\n",
      " [ 1.99287965  1.42484254]\n",
      " [ 0.11828849 -0.72662176]\n",
      " [ 0.00657769 -0.21186088]]\n",
      "]\n",
      "[-0.98060789 -0.87187918  0.99643983  0.05914424  0.00328884]\n",
      "Tensor\n",
      "[name=Tensor, dtype=float64\n",
      "shape=TensorShape((5, 2))\n",
      "value=\n",
      "[[ 1.         -0.85685315]\n",
      " [ 1.         -0.42250793]\n",
      " [ 1.          0.71242127]\n",
      " [ 1.         -0.36331088]\n",
      " [ 1.         -0.10593044]]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "t1 = Tensor(np.random.randn(4, 5), name='t1')\n",
    "t2 = Tensor(np.random.randn(5, 2))\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t1.shape, t2.shape)\n",
    "print(t1.dot(t2))\n",
    "print(2 * t1)\n",
    "print(t2 * 2)\n",
    "print(t2[:, 0])\n",
    "t2[:, 0] = 1\n",
    "print(t2)\n",
    "#print(t1.multiply(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor\n",
       "[name=t1+t2, dtype=<class 'numpy.float64'>\n",
       "shape=TensorShape(())\n",
       "value=\n",
       "5.0\n",
       "]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(3, 't1') + Tensor(2, 't2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(name, shape=None, dtype=np.float64, initializer=None):\n",
    "    \"\"\"\n",
    "    -shape:\n",
    "    -dtype:\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(initializer, )\n",
    "    \n",
    "    return initializer(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    Прокси класс для передачи доступа к параметрам\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, shape=None, init_value=None, initializer=None, dtype=np.float64, name=None):\n",
    "        \"\"\"\n",
    "        - shape: tuple, list или TensorShape\n",
    "        - value: numpy array\n",
    "        \"\"\"\n",
    "        self.initialized = False\n",
    "        shape = TensorShape(shape)\n",
    "        if init_value is not None:\n",
    "            assert isinstance(init_value, np.ndarray)\n",
    "\n",
    "        if shape.is_initialized() & (init_value is not None):\n",
    "            init_value_shape = TensorShape(init_value.shape)\n",
    "            if shape != init_value_shape:\n",
    "                raise ValueError('Inconsistent parameters \"shape\" and \"init_value\" parameters '\\\n",
    "                                 'are passed to the {} constructor \"{}\": {} != {}.'.format(\n",
    "                                 type(self).__name__, shape, init_value_shape))\n",
    "            self.shape = init_value_shape  \n",
    "            self.initializer = ConstantInitializer(value=init_value, dtype=dtype)\n",
    "        elif shape.is_initialized() & (init_value is None):\n",
    "            if initializer is None:\n",
    "                # Используем инициализатор по умолчанию (плохая практика)\n",
    "                initializer = NormalInitializer(dtype=dtype)\n",
    "            if not isinstance(initializer, InitializerBase):\n",
    "                raise TypeError('Parameter \"initializer\" passed to the {} constructor '\\\n",
    "                                'must have type Initializer.'.format(type(self).__name__))\n",
    "            self.shape = shape\n",
    "            self.initializer = initializer\n",
    "        elif (not self.shape.is_initialized()) & (init_value is not None):\n",
    "            self.shape = TensorShape(init_value.shape)\n",
    "            self.initializer = ConstantInitializer(value=init_value, dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError('Either \"shape\" or \"init_value\" must be provided to the {} constructor.'.format(type(self).__name__))\n",
    "        self.name = name\n",
    "        self.dtype = dtype\n",
    "        assert isinstance(self.initializer, InitializerBase)\n",
    "        assert self.shape.is_initialized()\n",
    "        for dim_size in self.shape:\n",
    "            assert dim_size > 0, 'All \"shape\" dimensions must be positive integers.'\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.initialized:\n",
    "            return 'Tensor[shape={}, value=\\n{}]'.format(self.shape, self.value)\n",
    "        return 'Tensor[shape={}, value=None]'.format(self.shape)\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Инициализация аргумента value данного тензора.\n",
    "        Для возможности инициализации необходимо выполнение одного из следующих условий:\n",
    "        - задано начальное значение value тензора; доложно быть совместимо с параметром shape\n",
    "        - задано значение shape формы тензора\n",
    "        \"\"\"\n",
    "        self.value = self.initializer(shape=self.shape.get_shape())\n",
    "        self.initialized = True\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class Placeholder:\n",
    "    def __init__(self, shape):\n",
    "    \n",
    "    def get_value():\n",
    "    \n",
    "    def set_value(self, value):\n",
    "        assert self.shape == TensorShape(value.shape)\n",
    "    \n",
    "    \n",
    "        \n",
    "class TensorConstantInitializer(ConstantInitializer):\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.name = name\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        tensor_value = super().__call__(*args, **kwargs)\n",
    "        return Tensor(shape=tensor_value.shape, init_value=tensor_value, name=self.name,\n",
    "                      dtype=self.dtype)\n",
    "\n",
    "    \n",
    "class TensorNormalInitializer(NormalInitializer):\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.name = name\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        tensor_value = super().__call__(*args, **kwargs)\n",
    "        return Tensor(shape=tensor_value.shape, init_value=tensor_value, name=self.name,\n",
    "                      dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, units, use_bias=True, W_init=None, b_init=None, W_reg=None, b_reg=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - units - Integer or Long, dimensionality of the output space.\n",
    "        - W_initializer\n",
    "        - b_initializer\n",
    "        - seed - used for initializers!!!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.set_units(units)\n",
    "        self.set_use_bias(use_bias)\n",
    "        self.W_init = W_init\n",
    "        self.b_init = b_init\n",
    "        self.W_reg  = W_reg\n",
    "        self.b_reg  = b_reg\n",
    "    def set_units(self, units):\n",
    "        self._check_int_positive(units, 'units', '\"units\" must be a positive integer.')\n",
    "        self.units = units\n",
    "    def set_use_bias(self, use_bias):\n",
    "        self._check_boolean(use_bias, 'use_bias', '\"use_bias\" must be a boolean')\n",
    "        self.use_bias = use_bias\n",
    "    def __repr__(self):\n",
    "        input_size = -1\n",
    "        if self.initialized:\n",
    "            input_size = self.W.shape[0]\n",
    "        return 'Dense({}->{})'.format(input_size, self.units)   \n",
    "   \n",
    "    ################################## \n",
    "    ###       Initialization       ###\n",
    "    ##################################\n",
    "    def _initialize_input_shape(self, params):\n",
    "        super()._initialize_input_shape(params)\n",
    "        assert len(self.input_shape) == 2, 'input to Dense layer must be a 2-dim tensor.'\n",
    "        self.n_features = self.input_shape[1]\n",
    "        return params\n",
    "    def _initialize_params(self, params):\n",
    "        self._initialize_W(params)\n",
    "        self._initialize_b(params)\n",
    "        return params\n",
    "    def _initialize_W(self, params):\n",
    "        W_name = self.get_param_name('W')\n",
    "        W_shape = (self.n_features, self.units)\n",
    "        W_init = self.W_init\n",
    "        if W_init is None:\n",
    "            W_init = NormalInitializer(generator=self.generator, dtype=self.dtype, loc=0.0, scale=1.0 / np.sqrt(self.n_features))\n",
    "        W_init = get_variable(name=W_name, shape=W_shape, dtype=self.dtype, validate_shape=True, initializer=W_init)\n",
    "        self.W = W_init()\n",
    "        self.grad_W = Tensor(value=self.W.get_zeros_like(), name=W_name, validate_shape=True)\n",
    "        if self.W_reg is None:\n",
    "            self.W_reg = EmptyRegularizer()\n",
    "        assert isinstance(self.W, Tensor):\n",
    "        assert isinstance(self.grad_W, Tensor)\n",
    "        assert isinstance(self.W_reg, Regularizer), 'W_reg of layer \"{}\" must have type Regularizer.'.format(self.name)\n",
    "        self._params[W_name] = self.W\n",
    "        self._grad_params[W_name] = self.grad_W\n",
    "        self._regularizers[W_name] = self.W_reg\n",
    "        return params\n",
    "    def _initialize_b(self, params):\n",
    "        b_name = self.get_param_name('b')\n",
    "        b_shape = (self.units,)\n",
    "        b_init = self.b_init\n",
    "        if b_init is None:\n",
    "            b_init = ConstantInitializer(dtype=self.dtype)\n",
    "        b_init = get_variable(name=b_name, shape=b_shape, dype=self.dtype, validate_shape=True, initializer=b_init)\n",
    "        self.b = b_init()\n",
    "        self.grad_b = Tensor(value=sel.b.get_zeros_like(), name=b_name, validate_shape=True)\n",
    "        if self.b_reg is None:\n",
    "            self.b_reg = EmptyRegularizer()\n",
    "        assert isinstance(self.b, Tensor)\n",
    "        assert isinstance(self.grad_b, Tensor)\n",
    "        assert isinstance(self.b_reg, Regularizer), 'b_reg of layer \"{}\" must have type Regularizer.'.format(self.name)\n",
    "        self._params[b_name] = self.b\n",
    "        self._grad_params[b_name] = self.grad_b\n",
    "        self._regularizers[b_name] = self.b_reg\n",
    "        return params  \n",
    "    \n",
    "    def _initialize_output_shape(self, params):\n",
    "        self.output_shape = (-1, self.units) # Input shape for the next layer\n",
    "        params['input_shape'] = self.output_shape\n",
    "        return params\n",
    "    \n",
    "    def set_W(self, W):\n",
    "        self.W.set_value(W)\n",
    "    def set_b(self, b):\n",
    "        self.b.set_value(b)\n",
    "        \n",
    "    ################################## \n",
    "    ###     Forward propagation    ###\n",
    "    ##################################\n",
    "    # Проверки прямого распространения\n",
    "    def _forward_check_shape(self, input, target=None):\n",
    "        assert input.ndim == 2, 'Input to layer \"{}\" must be 2-dim numpy array'.format(self.name)\n",
    "        assert input.shape[1] == self.W.shape[0], 'Expected input shape (-1, {}) but received (-1, {})'.format(self.W.shape[0], input.shape[1])\n",
    "    # Прямое распространение\n",
    "    def _forward(self, input, target=None):\n",
    "        self.output = np.dot(input, self.W)  # [N x D] x [D x H] = [N x H]\n",
    "        if self.use_bias:\n",
    "            self.output += self.b[None, :]\n",
    "    \n",
    "    ################################## \n",
    "    ###    Backward propagation    ###\n",
    "    ##################################\n",
    "    # Проверка ошибок обратного распространения\n",
    "    def _backward_check_shape(self, input, grad_output):\n",
    "        assert input.ndim == 2\n",
    "        assert grad_output.ndim == 2\n",
    "        assert input.shape[1] == self.W.shape[0]\n",
    "        assert input.shape[0] == grad_output.shape[0]\n",
    "        assert grad_output.shape[1] == self.W.shape[1]\n",
    "    # Обратное распространение\n",
    "    def _backward(self, input, grad_output):\n",
    "        self.grad_input = np.dot(grad_output, self.W.T)          # [N x H] x [H x D] = [N x D]\n",
    "        self.grad_W = np.dot(input.T, grad_output)               # ([D x N] x [N x H]).T = [D, H]\n",
    "        self.W_reg.update_grad(self.W, self.grad_W)\n",
    "        if self.use_bias:\n",
    "            self.grad_b = np.sum(grad_output, axis=0)\n",
    "            self.b_reg.update_grad(self.b, self.grad_b)\n",
    "\n",
    "    ################################## \n",
    "    ###         Parameters         ###\n",
    "    ##################################\n",
    "    @check_initialized\n",
    "    def get_params(self, copy=False):  \n",
    "        params = OrderedDict()\n",
    "        params[self.name + ':W'] = self.W \n",
    "        params[self.name + ':b'] = self.b\n",
    "        return self._make_dict_copy(params, copy=copy)\n",
    "    @check_initialized\n",
    "    def get_grad_params(self, copy=False):\n",
    "        grad_params = OrderedDict()\n",
    "        grad_params[self.name + ':W'] = self.grad_W\n",
    "        grad_params[self.name + ':b'] = self.grad_b\n",
    "        return self._make_dict_copy(grad_params, copy=copy)\n",
    "        \n",
    "    ################################## \n",
    "    ###       Regularization       ###\n",
    "    ##################################\n",
    "    @check_initialized\n",
    "    def get_regularizers(self):\n",
    "        regularizers = OrderedDict()\n",
    "        regularizers[self.name + ':W'] = self.W_reg\n",
    "        regularizers[self.name + ':b'] = self.b_reg\n",
    "        return regularizers"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
