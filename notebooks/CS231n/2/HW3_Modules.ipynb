{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        assert False, \"Base version must not be called\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, input, gradOutput):\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        return []\n",
    "    \n",
    "    def training(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.outputs = []\n",
    "        for i in range(len(self.modules)):\n",
    "            input = self.modules[i].forward(input)\n",
    "            self.outputs.append(input)\n",
    "        self.output = self.outputs[-1]\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        for i in reversed(range(1, len(self.modules))):\n",
    "            gradOutput = self.modules[i].backward(self.outputs[i - 1], gradOutput)\n",
    "        self.gradInput = self.modules[0].backward(input, gradOutput)\n",
    "        return self.gradInput\n",
    "      \n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def training(self):\n",
    "        self.training = True\n",
    "        for i in range(len(self.modules)):\n",
    "            self.modules[i].training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        for i in range(len(self.modules)):\n",
    "            self.modules[i].evaluate()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        stdv = 1 / np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        assert len(input.shape) == 2\n",
    "        assert input.shape[1] == self.n_in\n",
    "        self.output = np.dot(input, self.W.T) + self.b[np.newaxis, :]\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.dot(gradOutput, self.W)\n",
    "        assert self.gradInput.shape == input.shape\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = np.dot(input.T, gradOutput).T\n",
    "        self.gradb = np.sum(gradOutput, axis=0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return \"Linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is probably the hardest but as others only takes 5 lines of code in total. \n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        a = np.exp(self.output)\n",
    "        self.output = a / np.sum(a, axis=1)[:, np.newaxis]\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        G = np.multiply(gradOutput, self.output)\n",
    "        coeffs = G.sum(axis=1)[:, np.newaxis]\n",
    "        self.gradInput = G - np.multiply(self.output, coeffs)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement a part of the layer: mean subtraction. That is, the module should calculate mean value for every feature (every column) and subtract it.\n",
    "\n",
    "Note, that you need to estimate the mean over the dataset to be able to predict on test examples. The right way is to create a variable which will hold smoothed mean over batches (exponential smoothing works good) and use it when forwarding test examples.\n",
    "\n",
    "When training calculate mean as folowing: \n",
    "```\n",
    "    mean_to_subtract = self.old_mean * alpha + batch_mean * (1 - alpha)\n",
    "```\n",
    "when evaluating (`self.training == False`) set $alpha = 1$.\n",
    "\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchMeanSubtraction(Module):\n",
    "    def __init__(self, alpha=0):\n",
    "        super(BatchMeanSubtraction, self).__init__()\n",
    "        self.train_alpha = alpha\n",
    "        self.alpha = alpha\n",
    "        self.old_mean = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if self.old_mean is None:\n",
    "            self.old_mean = np.mean(input, axis=0)\n",
    "        else:\n",
    "            self.old_mean = self.alpha * self.old_mean + (1 - self.alpha) * np.mean(input, axis=0)\n",
    "        self.output = input - self.old_mean[np.newaxis, :]\n",
    "        return self.output \n",
    "    \n",
    "    def training(self):\n",
    "        self.training = True\n",
    "        self.alpha = self.train_alpha\n",
    "    \n",
    "    def evalute(self, input):\n",
    "        self.training = False\n",
    "        self.alpha = 1.0\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = (1 - self.alpha / input.shape[0]) * np.array(gradOutput)\n",
    "        # self.gradInput = np.array(gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchMeanNormalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. \n",
    "\n",
    "This is a very cool regularizer. In fact, when you see your net is overfitting try to add more dropout.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch). When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if self.training:\n",
    "            self.mask = np.random.choice(2, input.shape, p=[self.p, 1 - self.p])\n",
    "        else:\n",
    "            self.mask = np.full(input.shape, 1 - self.p)\n",
    "        self.output = np.multiply(input, self.mask)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(self.mask, gradOutput)\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "         super(Tanh, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.tanh(input)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.dot((1 - self.output ** 2), gradOutput.T)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput, input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope=0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array(input)\n",
    "        self.output[self.output < 0] *= self.slope\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.array(gradOutput)\n",
    "        self.gradInput[input < 0] *= self.slope\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array(input)\n",
    "        self.mask = self.output < 0\n",
    "        self.output[self.mask] = self.alpha * (np.exp(self.output[self.mask]) - 1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.array(gradOutput)\n",
    "        self.gradInput[self.mask] *= self.alpha * np.exp(input[self.mask])\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.log(1 + np.exp(input))\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput, 1.0 / (1 + np.exp(-input)))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterions are used to score the models answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target, 2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput = 2 * (input - target) / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](https://www.kaggle.com/wiki/MultiClassLogLoss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15))\n",
    "        self.output = np.sum(np.multiply(target, -np.log(input_clamp))) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15))\n",
    "        self.gradInput = -np.divide(target, input_clamp) / input.shape[0]\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
