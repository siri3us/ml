{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home work 3: Basic Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design of was heavily inspired by [Torch](http://torch.ch) which is the most convenient neural network environment when the work involves defining new layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires sending **\"multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- HW3_Modules.ipynb\n",
    "- HW3_differentiation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Table of contents\n",
    "* [0. Code](#code)\n",
    "    * [0.1 Module and Sequential](#code1)\n",
    "    * [0.2 Linear and SoftMax](#code2)\n",
    "    * [0.3 BatchMeanSubtraction and Dropout](#code3)\n",
    "    * [0.4 Activations](#code4)\n",
    "    * [0.5 Criterions](#code5)\n",
    "* [1. Framework](#framework)\n",
    "    * [1.1 Helper functions](#funcs)\n",
    "        * [1.1.1 SGD optimizer](#sgd)\n",
    "        * [1.1.2 Batches generator](#bgen)\n",
    "        * [1.2.3 Class based model trainer (more convenient than function based)](#class-based)\n",
    "* [2. Toy example](#toy)\n",
    "    * [2.1 Network 1](#toy1)\n",
    "    * [2.2 Network 2](#toy2)\n",
    "* [3. Digit classification](#digits)\n",
    "    * [3.1 Test run](#test_run)\n",
    "    * [3.2 Comparison](#comp)\n",
    "        * [3.2.1 Generator of test network](#test_net_gen)\n",
    "        * [3.2.2 Comparison](#comparison)\n",
    "    * [3.3 Final run](#final_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code'></a>\n",
    "# 0.1 Code<sup>[toc](#toc)</sup>\n",
    "For debuggin purposes the model code was imported here. The same code resides in HW3_Modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code1'></a>\n",
    "### 0.1 Module and Sequential<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.trainings = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        assert False, \"Base version must not be called\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, input, gradOutput):\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        return []\n",
    "    \n",
    "    def training(self):\n",
    "        self.trainings = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.trainings = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Module\"\n",
    "    \n",
    "    \n",
    "class Sequential(Module):\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.outputs = []\n",
    "        for i in range(len(self.modules)):\n",
    "            input = self.modules[i].forward(input)\n",
    "            self.outputs.append(input)\n",
    "        self.output = self.outputs[-1]\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        for i in reversed(range(1, len(self.modules))):\n",
    "            gradOutput = self.modules[i].backward(self.outputs[i - 1], gradOutput)\n",
    "        self.gradInput = self.modules[0].backward(input, gradOutput)\n",
    "        return self.gradInput\n",
    "      \n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def training(self):\n",
    "        self.trainings = True\n",
    "        for i in range(len(self.modules)):\n",
    "            self.modules[i].training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.trainings = False\n",
    "        for i in range(len(self.modules)):\n",
    "            self.modules[i].evaluate()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code2'></a>\n",
    "### 0.2 Linear and SoftMax<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        stdv = 1 / np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_in, n_out))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        assert len(input.shape) == 2\n",
    "        assert input.shape[1] == self.n_in\n",
    "        self.output = np.dot(input, self.W) + self.b[np.newaxis, :]\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.dot(gradOutput, self.W.T)\n",
    "        assert self.gradInput.shape == input.shape\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = np.dot(input.T, gradOutput) / input.shape[0]\n",
    "        self.gradb = np.sum(gradOutput, axis=0) / input.shape[0]\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return \"Linear\"\n",
    "    \n",
    "    \n",
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        a = np.exp(self.output)\n",
    "        self.output = a / np.sum(a, axis=1)[:, np.newaxis]\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        G = np.multiply(gradOutput, self.output)\n",
    "        coeffs = G.sum(axis=1)[:, np.newaxis]\n",
    "        self.gradInput = G - np.multiply(self.output, coeffs)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code3'></a>\n",
    "### 0.3 BatchMeanSubtraction and Dropout<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchMeanSubtraction(Module):\n",
    "    def __init__(self, alpha=0):\n",
    "        super(BatchMeanSubtraction, self).__init__()\n",
    "        self.train_alpha = alpha\n",
    "        self.alpha = alpha\n",
    "        self.old_mean = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if self.old_mean is None:\n",
    "            self.old_mean = np.mean(input, axis=0)\n",
    "        else:\n",
    "            self.old_mean = self.alpha * self.old_mean + (1 - self.alpha) * np.mean(input, axis=0)\n",
    "        self.output = input - self.old_mean[np.newaxis, :]\n",
    "        return self.output \n",
    "    \n",
    "    def training(self):\n",
    "        self.trainings = True\n",
    "        self.alpha = self.train_alpha\n",
    "    \n",
    "    def evalute(self, input):\n",
    "        self.trainings = False\n",
    "        self.alpha = 1.0\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = (1 - self.alpha / input.shape[0]) * np.array(gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchMeanNormalization\"\n",
    "    \n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if self.trainings:\n",
    "            self.mask = np.random.choice(2, input.shape, p=[self.p, 1 - self.p])\n",
    "        else:\n",
    "            self.mask = np.full(input.shape, 1 - self.p)\n",
    "        self.output = np.multiply(input, self.mask)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(self.mask, gradOutput)\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code4'></a>\n",
    "### 0.4 Activations<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "         super(Tanh, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.tanh(input)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply((1 - self.output ** 2), gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh\"\n",
    "\n",
    "    \n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput, input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\"\n",
    "\n",
    "    \n",
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope=0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array(input)\n",
    "        self.output[self.output < 0] *= self.slope\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.array(gradOutput)\n",
    "        self.gradInput[input < 0] *= self.slope\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\"\n",
    " \n",
    "\n",
    "class ELU(Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array(input)\n",
    "        self.mask = self.output < 0\n",
    "        self.output[self.mask] = self.alpha * (np.exp(self.output[self.mask]) - 1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.array(gradOutput)\n",
    "        self.gradInput[self.mask] *= self.alpha * np.exp(input[self.mask])\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\"\n",
    "    \n",
    "    \n",
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array(input)\n",
    "        self.output[input < 30] = np.log(1 + np.exp(input[input < 30]))\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput, 1.0 / (1 + np.exp(-input)))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code5'></a>\n",
    "### 0.5 Criterions<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Criterion\"\n",
    "    \n",
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target, 2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput = 2 * (input - target)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\"\n",
    "    \n",
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15))\n",
    "        self.output = np.sum(np.multiply(target, -np.log(input_clamp))) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15))\n",
    "        self.gradInput = -np.divide(target, input_clamp)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='framework'></a>\n",
    "# 1. Framework<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `Modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return AND store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run HW3_Modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='funcs'></a>\n",
    "## 1.1 Helper functions<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>\n",
    "### 1.1.1 SGD optimizer<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_momentum(x, dx, config, state):\n",
    "    \"\"\"\n",
    "        This is a very ugly implementation of sgd with momentum \n",
    "        just to show an example how to store old grad in state.\n",
    "        \n",
    "        config:\n",
    "            - momentum\n",
    "            - learning_rate\n",
    "        state:\n",
    "            - old_grad\n",
    "    \"\"\"\n",
    "    \n",
    "    # x and dx have complex structure, old dx will be stored in a simpler one\n",
    "    state.setdefault('old_grad', {})\n",
    "    i = 0 \n",
    "    for cur_layer_x, cur_layer_dx in zip(x, dx): \n",
    "        for cur_x, cur_dx in zip(cur_layer_x, cur_layer_dx):\n",
    "            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(cur_dx))\n",
    "            np.add(config['momentum'] * cur_old_grad, config['learning_rate'] * cur_dx, out=cur_old_grad)\n",
    "            cur_x -= cur_old_grad\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bgen'></a>\n",
    "### 1.1.2 Batches generator<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(X, Y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_idx = indices[start:end]\n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='class-based'></a>\n",
    "### 1.2.3 Class based model trainer (more convenient than function based)<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SgdOptimizer(object):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.state = {}\n",
    "\n",
    "    def __call__(self, params, grad_params):\n",
    "        self.state.setdefault('old_grad', {})\n",
    "        n_params_set = 0\n",
    "        for n_layer, (layer_params, layer_grad_params) in enumerate(zip(params, grad_params)): \n",
    "            for cur_params, cur_grad_params in zip(layer_params, layer_grad_params):\n",
    "                cur_old_grad = self.state['old_grad'].setdefault(n_params_set, np.zeros_like(cur_params))\n",
    "                np.add(self.momentum * cur_old_grad, self.lr * cur_grad_params, out=cur_old_grad)\n",
    "                cur_params -= cur_old_grad\n",
    "                n_params_set += 1\n",
    "        \n",
    "class ModelTrainer(object):\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def train(self, X, Y, nb_epoch=10, batch_size=100, verbose=1):\n",
    "        for epoch in range(nb_epoch):\n",
    "            loss_values = []\n",
    "            for X_batch, Y_batch in self.get_batch_generator_(X, Y, batch_size):\n",
    "                self.model.zeroGradParameters()\n",
    "\n",
    "                # Forward\n",
    "                predictions = self.model.forward(X_batch)\n",
    "                loss = self.criterion.forward(predictions, Y_batch)\n",
    "                loss_values.append(loss)\n",
    "                \n",
    "                # Backward\n",
    "                dp = self.criterion.backward(predictions, Y_batch)\n",
    "                self.model.backward(X_batch, dp)\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer(self.model.getParameters(), self.model.getGradParameters()) \n",
    "       \n",
    "            mean_loss = np.mean(loss_values)\n",
    "            if verbose:\n",
    "                print('epoch = {}, loss = {}'.format(epoch, mean_loss))     \n",
    "            self.loss_history.append(mean_loss)  \n",
    "        return self.loss_history\n",
    " \n",
    "    def predict_proba(self, X, batch_size=100):\n",
    "        Ys = []\n",
    "        self.model.evaluate()\n",
    "        for X_batch, _ in self.get_batch_generator_(X, np.arange(len(X)), batch_size, shuffle=False):\n",
    "            Y_batch = self.model.forward(X_batch)\n",
    "            Ys.append(Y_batch)\n",
    "        self.model.training()\n",
    "        Y = np.concatenate(Ys, axis=0)\n",
    "        return Y\n",
    "        \n",
    "    def predict(self, X, batch_size=100):\n",
    "        Y = self.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return y\n",
    "        \n",
    "    def get_batch_generator_(self, X, Y, batch_size, shuffle=True):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_idx = indices[start:end]\n",
    "            yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear\n",
      "SoftMax\n",
      "\n",
      "epoch = 0, loss = 0.5184048063838519\n",
      "epoch = 1, loss = 0.3472760327819386\n",
      "epoch = 2, loss = 0.3222660854765577\n",
      "epoch = 3, loss = 0.30897441092505645\n",
      "epoch = 4, loss = 0.300354621066859\n",
      "epoch = 5, loss = 0.2941245631935666\n",
      "epoch = 6, loss = 0.2894276070870002\n",
      "epoch = 7, loss = 0.28520501516797103\n",
      "epoch = 8, loss = 0.28247889229385675\n",
      "epoch = 9, loss = 0.2800279076216669\n",
      "epoch = 10, loss = 0.27735586246339944\n",
      "epoch = 11, loss = 0.27561098984990173\n",
      "epoch = 12, loss = 0.2736102298486521\n",
      "epoch = 13, loss = 0.27196609599657207\n",
      "epoch = 14, loss = 0.27046120261971723\n",
      "epoch = 15, loss = 0.26937225878179066\n",
      "epoch = 16, loss = 0.2682265879900696\n",
      "epoch = 17, loss = 0.26707148895112653\n",
      "epoch = 18, loss = 0.26592426050661205\n",
      "epoch = 19, loss = 0.2648229473455473\n",
      "epoch = 20, loss = 0.2642105885478627\n",
      "epoch = 21, loss = 0.26339924205902243\n",
      "epoch = 22, loss = 0.26274398064912835\n",
      "epoch = 23, loss = 0.2619066258989408\n",
      "epoch = 24, loss = 0.26109677720863833\n",
      "epoch = 25, loss = 0.26037481167546395\n",
      "epoch = 26, loss = 0.2597850330456232\n",
      "epoch = 27, loss = 0.25919350888499093\n",
      "epoch = 28, loss = 0.2586594752581776\n",
      "epoch = 29, loss = 0.25795254661717903\n",
      "epoch = 30, loss = 0.25757346425083116\n",
      "epoch = 31, loss = 0.2570017878485266\n",
      "epoch = 32, loss = 0.2565071505948944\n",
      "epoch = 33, loss = 0.25608189538880666\n",
      "epoch = 34, loss = 0.25556842303817023\n",
      "epoch = 35, loss = 0.2552727836665329\n",
      "epoch = 36, loss = 0.2545464293044373\n",
      "epoch = 37, loss = 0.2543776103835555\n",
      "epoch = 38, loss = 0.25401271746670295\n",
      "epoch = 39, loss = 0.25365253225716267\n",
      "epoch = 40, loss = 0.2533363713223328\n",
      "epoch = 41, loss = 0.2530390844998058\n",
      "epoch = 42, loss = 0.2525180704108904\n",
      "epoch = 43, loss = 0.2522530959012492\n",
      "epoch = 44, loss = 0.2518662870439608\n",
      "epoch = 45, loss = 0.2517047184375719\n",
      "epoch = 46, loss = 0.25132996780095157\n",
      "epoch = 47, loss = 0.25099847304312156\n",
      "epoch = 48, loss = 0.25062130996729065\n",
      "epoch = 49, loss = 0.2504707661580112\n",
      "epoch = 50, loss = 0.2501541609090953\n",
      "epoch = 51, loss = 0.24976067180385306\n",
      "epoch = 52, loss = 0.24957792392598296\n",
      "epoch = 53, loss = 0.2493499305199916\n",
      "epoch = 54, loss = 0.24922724207256522\n",
      "epoch = 55, loss = 0.2490496148543474\n",
      "epoch = 56, loss = 0.24873544303110534\n",
      "epoch = 57, loss = 0.24873925439338568\n",
      "epoch = 58, loss = 0.24812261190122067\n",
      "epoch = 59, loss = 0.24801661939909214\n",
      "epoch = 60, loss = 0.24768824001404727\n",
      "epoch = 61, loss = 0.2477894957540589\n",
      "epoch = 62, loss = 0.2472936804028687\n",
      "epoch = 63, loss = 0.24712962754204665\n",
      "epoch = 64, loss = 0.24685115427518958\n",
      "epoch = 65, loss = 0.2466269837615172\n",
      "epoch = 66, loss = 0.24672027005731398\n",
      "epoch = 67, loss = 0.24628800265566295\n",
      "epoch = 68, loss = 0.24624576159514772\n",
      "epoch = 69, loss = 0.24603722396897312\n",
      "epoch = 70, loss = 0.2456154757818519\n",
      "epoch = 71, loss = 0.24572481785772363\n",
      "epoch = 72, loss = 0.2455392420942117\n",
      "epoch = 73, loss = 0.24534810552551164\n",
      "epoch = 74, loss = 0.24527953922857304\n",
      "epoch = 75, loss = 0.2449510623657015\n",
      "epoch = 76, loss = 0.2447174309473007\n",
      "epoch = 77, loss = 0.24474186930324712\n",
      "epoch = 78, loss = 0.24446565879653695\n",
      "epoch = 79, loss = 0.24433220729036723\n",
      "epoch = 80, loss = 0.24411065975169038\n",
      "epoch = 81, loss = 0.24414579126204217\n",
      "epoch = 82, loss = 0.24393575240314136\n",
      "epoch = 83, loss = 0.24366094628374135\n",
      "epoch = 84, loss = 0.2435431217269492\n",
      "epoch = 85, loss = 0.24345588504206636\n",
      "epoch = 86, loss = 0.24316196204579968\n",
      "epoch = 87, loss = 0.24306358459650065\n",
      "epoch = 88, loss = 0.24315491895800448\n",
      "epoch = 89, loss = 0.24299230195353216\n",
      "epoch = 90, loss = 0.24287770291284166\n",
      "epoch = 91, loss = 0.24265449341796036\n",
      "epoch = 92, loss = 0.2425155891053842\n",
      "epoch = 93, loss = 0.2423977734063329\n",
      "epoch = 94, loss = 0.2424469429322169\n",
      "epoch = 95, loss = 0.24204754559241365\n",
      "epoch = 96, loss = 0.24216023798452282\n",
      "epoch = 97, loss = 0.24189359588021114\n",
      "epoch = 98, loss = 0.24164174269187663\n",
      "epoch = 99, loss = 0.24189115720803958\n",
      "Final loss = 0.241891157208\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAIiCAYAAADPSN4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTSAGAKBr/z+INkw/nHWM6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6557cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
