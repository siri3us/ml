{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-алгоритм для смеси бернуллиевских случайных величин"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Содержание\n",
    "* [1. Теория](#em_algo_bern_theory)\n",
    "    * [1.1 Формулировка задачи](#bern_task)\n",
    "    * [1.2 Решение](#bern_sol)\n",
    "* [2. Реализация](#em_algo_bern_impl)\n",
    "* [3. Применение](#em_algo_bern_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 30\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "_add_to_path = True\n",
    "import math\n",
    "import scipy\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "from scipy.special import erfc\n",
    "\n",
    "from itertools import product, chain\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.01\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['axes.titlesize'] = 20\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14)\n",
    "matplotlib.rc('ytick', labelsize=14)\n",
    "matplotlib.rc('legend', fontsize=16)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}') \n",
    "matplotlib.rcParams['axes.labelsize'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _add_to_path:\n",
    "    sys.path.append('../../')\n",
    "from ml.core import Checker, Printer\n",
    "from ml.mixtures import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# Below comes the list of modules which is automatically reimported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Sum}{\\sum\\limits}$\n",
    "$\\newcommand{\\Prod}{\\prod\\limits}$\n",
    "$\\newcommand{\\Max}{\\max\\limits}$\n",
    "$\\newcommand{\\Min}{\\min\\limits}$\n",
    "$\\newcommand{\\Int}{\\int\\limits}$\n",
    "$\\newcommand{\\Exp}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{\\mathbb{V}}$\n",
    "$\\newcommand{\\Energy}{\\mathcal{E}}$\n",
    "$\\newcommand{\\Prob}{\\mathcal{P}}$\n",
    "$\\newcommand{\\N}{\\mathcal{N}}$\n",
    "\n",
    "\n",
    "$\\newcommand{\\LogLike}{\\mathcal{L}}$\n",
    "$\\newcommand{\\Like}{\\ell}$\n",
    "\n",
    "$\\newcommand{\\bolda}{\\boldsymbol{a}}$\n",
    "$\\newcommand{\\boldA}{\\boldsymbol{A}}$\n",
    "$\\newcommand{\\ba}{\\bolda}$\n",
    "$\\newcommand{\\bA}{\\boldA}$\n",
    "\n",
    "$\\newcommand{\\boldb}{\\boldsymbol{b}}$\n",
    "$\\newcommand{\\boldB}{\\boldsymbol{B}}$\n",
    "$\\newcommand{\\bb}{\\boldb}$\n",
    "$\\newcommand{\\bB}{\\boldB}$\n",
    "\n",
    "$\\newcommand{\\boldd}{\\boldsymbol{d}}$\n",
    "$\\newcommand{\\boldD}{\\boldsymbol{D}}$\n",
    "$\\newcommand{\\bd}{\\boldd}$\n",
    "$\\newcommand{\\bD}{\\boldD}$\n",
    "\n",
    "$\\newcommand{\\boldf}{\\boldsymbol{f}}$\n",
    "$\\newcommand{\\boldF}{\\boldsymbol{F}}$\n",
    "$\\newcommand{\\bf}{\\boldf}$\n",
    "$\\newcommand{\\bF}{\\boldF}$\n",
    "\n",
    "$\\newcommand{\\boldt}{\\boldsymbol{t}}$\n",
    "$\\newcommand{\\boldT}{\\boldsymbol{T}}$\n",
    "$\\newcommand{\\bt}{\\boldsymbol{\\boldt}}$\n",
    "$\\newcommand{\\bT}{\\boldsymbol{\\boldT}}$\n",
    "\n",
    "$\\newcommand{\\boldx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\boldX}{\\boldsymbol{X}}$\n",
    "$\\newcommand{\\bx}{\\boldx}$\n",
    "$\\newcommand{\\bX}{\\boldX}$\n",
    "\n",
    "$\\newcommand{\\boldY}{\\boldsymbol{Y}}$\n",
    "$\\newcommand{\\boldy}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\bY}{\\boldY}$\n",
    "$\\newcommand{\\by}{\\boldy}$\n",
    "\n",
    "$\\newcommand{\\boldZ}{\\boldsymbol{Z}}$\n",
    "$\\newcommand{\\boldz}{\\boldsymbol{z}}$\n",
    "$\\newcommand{\\bZ}{\\boldZ}$\n",
    "$\\newcommand{\\bz}{\\boldz}$\n",
    "\n",
    "$\\newcommand{\\boldTheta}{\\boldsymbol{\\Theta}}$\n",
    "$\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bTheta}{\\boldTheta}$\n",
    "$\\newcommand{\\btheta}{\\boldtheta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='em_algo_bern_theory'></a>\n",
    "# 1. Теория [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Templates](https://www.dropbox.com/s/lx47mcsq3yyhmk2/templates.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bern_task'></a>\n",
    "## 1.1 Формулировка задачи [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть всего у нас $K$ шаблонов, то есть $K$ компонент смеси. Каждый объект выборки $-$ это $D$-мерный бинарный вектор: $X = \\{ \\mathbf{x}_i \\}_{i=1}^N, \\mathbf{x}_i \\in \\{ 0, 1 \\}^D$. Каждое из $K$ бинарных распределений задает вероятность признака $d$ принимать значение 1, обозначаемое ${\\theta}_{kd}$. Соответственно вероятность признака $d$ быть равным 0 есть $1 - {\\theta}_{kd}$. Таким образом, правдоподобие объекта $\\mathbf{x}_i$ при условии принадлежности распределению $k$ есть:\n",
    "$$\n",
    "\tp(\\mathbf{x}_i | t_i = k, \\theta) = \\prod_{d=1}^D \\theta_{kd}^{x_{id}} (1 - \\theta_{kd})^{1 - x_{id}},\n",
    "$$\n",
    "где $t_i$ - скрытая переменная, принимающая значения от $1$ до $K$, кодирующие к какому распределению относится объект. Априорное распределение на $t_i$ будем полагать равномерным: $p(t_i = k) = 1 / K$.\n",
    "\n",
    "EM-алгоритм итеративно оптимизирует логарифм неполного правдоподобия:\n",
    "$$\n",
    "\t\\log p(X | \\theta) = \\sum_{i=1}^N \\log \\sum_{k = 1}^K p(\\mathbf{x}_i | t_i = k, \\theta) - N \\log K\n",
    "$$\n",
    "На E-шаге рассчитывается апостериорное распределение на скрытые переменные при старых значениях параметров:\n",
    "$$\n",
    "\tp(T | X, \\theta^{old}) = \\frac{p(T) p(X | T, \\theta^{old})}{p(X | \\theta^{old})}\n",
    "$$\n",
    "В данном случае апостериорные распределения для каждого объекта независимы, то есть, $p(T | X, \\theta) = \\prod_{i=1}^N p(t_i | \\mathbf{x}_i, \\theta)$.\n",
    "\n",
    "Затем, на M-шаге выполняется оптимизиация по параметрам распределений:\n",
    "$$\n",
    "\t\\theta^{new} = \\arg \\max_{\\theta} \\mathbb{E}_{p(T | X, \\theta^{old})} \\log p(X, T | \\theta) \n",
    "$$\n",
    "В данном случае максимум можно найти, приравняв производную по каждому из параметров к нулю.\n",
    "\n",
    "Эти два шага чередуются необходимое число итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bern_sol'></a>\n",
    "## 1.2 Формулировка задачи [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выпишите конечные формулы для апостериорных распределений $p(t_i | \\mathbf{x}_i, \\theta)$. Найдите оценки для параметров, получаемые на M-шаге.\n",
    "\n",
    "$$\n",
    "p(\\boldx_n, t_n=k;\\boldtheta) = \\pi_k p(\\boldx_n|t_n=k;\\boldtheta) = \\pi_k\\Prod_{d=1}^D \\theta_{kd}^{x_{nd}}(1-\\theta_{kd})^{1 - x_{nd}}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = p(t_n = k|\\boldx_n, \\boldtheta) = \\frac{p(\\boldx_n, t_n; \\boldtheta)}{p(\\boldx_n; \\boldtheta)} = \\frac{\\pi_k \\cdot p(\\boldx_n| t_n=k; \\boldtheta)}{\\Sum_{i=1}^K \\pi_i \\cdot p(\\boldx_n| t_n = i; \\boldtheta)} = \n",
    "\\frac{\\pi_k \\cdot \\Prod_{d=1}^D \\theta_{kd}^{x_{nd}}(1 - \\theta_{kd})^{1-x_{nd}}}{\\Sum_{i=1}^K \\pi_i \\cdot \\Prod_{d=1}^D \\theta_{id}^{x_{nd}}(1 - \\theta_{id})^{1-x_{nd}}}.\n",
    "$$\n",
    "\n",
    "На M-шаге требуется найти максимум ожидания правдоподобия\n",
    "\\begin{gather}\n",
    "\\Exp\\LogLike = \\Exp_{T \\backsim p(T|X;\\boldtheta)} \\LogLike(X, T; \\theta) = \\Sum_{n=1}^N \\Exp_{t \\backsim p(t | \\boldx_n;\\boldtheta)} \\log p(\\boldx_n, t_n; \\boldtheta) = \\Sum_{n=1}^N \\Sum_{k=1}^K p(t_n=k|\\boldx_n; \\boldtheta) \\log p(\\boldx_n, t_n = k; \\boldtheta) = \\\\ \n",
    "= \\Sum_{n=1}^N\\Sum_{k=1}^K \\gamma_{nk} \\left(\\log \\pi_k + \\Sum_{d=1}^D \\left\\{x_{nd} \\log \\theta_{kd} + (1 - x_{nd}) \\log (1 - \\theta_{nd})\\right\\} \\right)\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\Exp\\LogLike}{\\partial \\pi_k} = \\frac{\\Sum_{n=1}^N \\gamma_{nk}}{\\pi_k} - \\lambda = 0 \\Rightarrow \\pi_k \\lambda = \\Sum_{n=1}^N \\gamma_{nk} \\Rightarrow \\lambda = N, \\pi_k = \\frac{\\Sum_{n=1}^N \\gamma_{nk}}{N} = \\frac{N_k}{N}, \\text{ где } N_k = \\Sum_{n=1}^N \\gamma_{nk}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\Exp\\LogLike}{\\partial \\theta_{kd}} = \\Sum_{n=1}^N \\gamma_{nk} \\left(\\frac{x_{nd}}{\\theta_{kd}} - \\frac{1 - x_{nd}}{1 - \\theta_{kd}}\\right) = \\Sum_{n=1}^N \\gamma_{nk} \\frac{x_{nd}(1 - \\theta_{kd}) - (1 - x_{nd})\\theta_{kd}}{\\theta_{kd}(1 - \\theta_{kd})} = \\Sum_{n=1}^N \\gamma_{nk} \\frac{x_{nd}- \\theta_{kd}}{\\theta_{kd}(1 - \\theta_{kd})} \\Rightarrow \\theta_{kd} = \\frac{ \\Sum_{n=1}^N \\gamma_{nk} x_{nd}}{ \\Sum_{n=1}^N \\gamma_{nk}} = \n",
    "\\frac{ \\Sum_{n=1}^N \\gamma_{nk} x_{nd}}{N_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае $\\pi_k = 1 / K$ получаем **EM-алгоритм**:\n",
    "\n",
    "* E-шаг\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = p(t_n = k|\\boldx_n, \\boldtheta) = \\frac{\\Prod_{d=1}^D \\theta_{kd}^{x_{nd}}(1 - \\theta_{kd})^{1-x_{nd}}}{\\Sum_{i=1}^K \\Prod_{d=1}^D \\theta_{id}^{x_{nd}}(1 - \\theta_{id})^{1-x_{nd}}}.\n",
    "$$\n",
    "\n",
    "* M-шаг\n",
    "\n",
    "$$\\theta_{kd} = \\frac{ \\Sum_{n=1}^N \\gamma_{nk} x_{nd}}{ \\Sum_{n=1}^N \\gamma_{nk}} = \n",
    "\\frac{ \\Sum_{n=1}^N \\gamma_{nk} x_{nd}}{N_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите EM-алгоритм на изображениях цифр 6 и 9 для $K=2$, сделайте 30 итераций. Постройте график логарифма правдоподобия в зависимости от числа итераций, а также визуализируйте шаблоны, полученные после пересчета на каждой итерации. Удалось ли вам получить шаблоны этих цифр?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='em_algo_bern_impl'></a>\n",
    "# 2. Реализация [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMBernoulli:\n",
    "    def __init__(self, n_components, max_iter=10, loglikelihoods=True, verbose=False, eps=1e-40, \n",
    "                 omit_constants=True, batch_size=None, memory_limit=512):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            param: eps\n",
    "            param: omit_constants   - \n",
    "            param: batch_size       - \n",
    "            param: memory_limit - \n",
    "            \n",
    "        \"\"\"\n",
    "        self._n_components = n_components\n",
    "        self._max_iter = max_iter\n",
    "        self._loglikelihoods = loglikelihoods\n",
    "        self._verbose = verbose\n",
    "        self._eps = eps\n",
    "        self._omit_constants = omit_constants\n",
    "        self._batch_size = batch_size\n",
    "        self._memory_limit = memory_limit\n",
    "        \n",
    "    def get_thetas(self):\n",
    "        if self._omit_constants:\n",
    "            const_values = self.maximums[self.equal_columns]\n",
    "            thetas = np.zeros((self._n_components, self._n_dim + len(const_values)), dtype=np.float64)\n",
    "            thetas[:, self.equal_columns] = const_values[None, :]\n",
    "            thetas[:, self.nequal_columns] = self.T\n",
    "        else:\n",
    "            thetas = self.T\n",
    "        return np.array(thetas)\n",
    "    def get_gammas(self):\n",
    "        return np.array(self.G.T)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self._initialize(X)\n",
    "        while (not self._stop()):\n",
    "            self._E_step()\n",
    "            self._M_step()\n",
    "            self._print('n_iter = {}: likelihood = {}'.format(self._n_iter,  self._likelihoods[-1]))\n",
    "            self._n_iter += 1\n",
    "            \n",
    "    def _initialize(self, X):\n",
    "        X = np.array(X)\n",
    "        if self._omit_constants:\n",
    "            self.maximums = np.max(X, axis=0)\n",
    "            self.minimums = np.min(X, axis=0) \n",
    "            self.equal_columns = np.where(self.maximums == self.minimums)[0]\n",
    "            self.nequal_columns = np.where(self.maximums != self.minimums)[0]\n",
    "            self.X  = X[:, self.nequal_columns]      # [N x D]\n",
    "        else:\n",
    "            self.X = X                               # [N x D]            \n",
    "        self.Xt = self.X.T                           # [D x N]\n",
    "        self._n_samples, self._n_dim = self.X.shape\n",
    "        \n",
    "        self._find_batch_size()\n",
    "        \n",
    "        self.G = np.random.uniform(size=(self._n_components, self._n_samples)) # [K x N]\n",
    "        self.G = np.maximum(self.G, self._eps)\n",
    "        self.G /= np.sum(self.G, axis=0)[None, :]\n",
    "        self.T = np.zeros((self._n_components, self._n_dim))                   # [K x D]\n",
    "        \n",
    "        self._M_step()\n",
    "        self._n_iter = 0\n",
    "        self._likelihoods = []\n",
    "        \n",
    "    def _find_batch_size(self):\n",
    "        if self._batch_size is not None:\n",
    "            return\n",
    "        full_size = (1.0 * self._n_samples * self._n_dim * self._n_components * self.X.dtype.type(0).nbytes) / (2 ** 20)\n",
    "        if full_size > self._memory_limit:\n",
    "            alpha = 1.0 * full_size / self._memory_limit\n",
    "            self._batch_size = int(self._n_samples / alpha + 0.5)\n",
    "        else:\n",
    "            self._batch_size = self._n_samples\n",
    "        new_full_size = (1.0 * self._batch_size * self._n_dim * self._n_components * self.X.dtype.type(0).nbytes) / (2 ** 20)\n",
    "        self._print('Found batch size = {} for memory limit = {}'.format(self._batch_size, self._memory_limit))\n",
    "        self._print('\\tFull size = {}, new full size = {}'.format(int(full_size), int(new_full_size)))\n",
    "        \n",
    "    def _E_step(self):\n",
    "        T = self.T.reshape((self._n_components, -1, self._n_dim))                # [K x 1 x D]\n",
    "        if self._batch_size == 1:\n",
    "            likelihood = 0\n",
    "            for n_sample in range(self._n_samples):\n",
    "                x = self.X[[n_sample], :]                                        # [1 x D]\n",
    "                logs = np.sum(x * np.log(self.T) + (1 - x) * np.log(1 - self.T), axis=1) # [K x D] -> [K]\n",
    "                likelihood += np.sum(self.G[:, n_sample] * logs)\n",
    "                self.G[:, n_sample] = self._softmax_vector(logs)\n",
    "            self._likelihoods.append(likelihood)\n",
    "            return\n",
    "        if self._batch_size < self._n_samples:\n",
    "            Ss = []\n",
    "            for i in range(0, self._n_samples, self._batch_size):\n",
    "                left, right = i, min(i + self._batch_size, self._n_samples)\n",
    "                X = self.X[left:right].reshape((-1, right - left, self._n_dim))  # [1 x B x D]\n",
    "                S = np.sum(X * np.log(T) + (1 - X) * np.log(1 - T), axis=2)      # [K x B x D] -> [K x B]\n",
    "                Ss.append(S)                                                     # [K x N]\n",
    "            S = np.concatenate(Ss, axis=1)\n",
    "            assert S.shape == (self._n_components, self._n_samples)\n",
    "        elif self._batch_size == self._n_samples:\n",
    "            X = self.X.reshape((-1, self._n_samples, self._n_dim))               # [1 x N x D]\n",
    "            S = np.sum(X * np.log(T) + (1 - X) * np.log(1 - T), axis=2)          # [K x N x D] -> [K x N]\n",
    "            assert S.shape == (self._n_components, self._n_samples)\n",
    "        else:\n",
    "            assert False, \"Something is wrong!\"\n",
    "        likelihood = np.sum(np.multiply(self.G, S))\n",
    "        self.G = self._softmax_column(S)\n",
    "        self._likelihoods.append(likelihood)\n",
    "\n",
    "    def _M_step(self):\n",
    "        if self._batch_size < self._n_samples:\n",
    "            for n_component in range(self._n_components):\n",
    "                weights = self.G[[n_component], :]                               # [1 x N]\n",
    "                self.T[n_component, :] = np.sum(weights * self.Xt, axis=1) / np.sum(weights)\n",
    "        elif self._batch_size == self._n_samples:\n",
    "            G = self.G.reshape((self._n_components, 1, self._n_samples))\n",
    "            X = self.Xt.reshape((1, self._n_dim, self._n_samples))\n",
    "            self.T = np.sum(G * X, axis=2) / np.sum(self.G, axis=1)[:, None]\n",
    "        else:\n",
    "            assert False, \"Something is wrong!\"\n",
    "        self.T = np.maximum(self.T, self._eps)\n",
    "        self.T = np.minimum(self.T, 1 - self._eps)\n",
    "\n",
    "    def _stop(self):\n",
    "        if self._n_iter >= self._max_iter:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _softmax_vector(self, x):\n",
    "        max_prob = np.max(x)\n",
    "        x -= max_prob\n",
    "        np.exp(x, x)\n",
    "        sum_prob = np.sum(x)\n",
    "        x /= sum_prob\n",
    "        return x\n",
    "    \n",
    "    def _softmax_column(self, X):\n",
    "        max_prob = np.max(X, axis=0)[None, :]\n",
    "        X -= max_prob\n",
    "        np.exp(X, X)\n",
    "        sum_prob = np.sum(X, axis=0)[None, :]\n",
    "        X /= sum_prob\n",
    "        return X\n",
    "\n",
    "    def _print(self, msg):\n",
    "        if self._verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='em_algo_bern_usage'></a>\n",
    "# 3. Применение [[toc](#toc)]\n",
    "* [Загрузка данных](#em_algo_bern_load)\n",
    "* [K = 10](#em_algo_bern_K10)\n",
    "* [K = 15](#em_algo_bern_K15)\n",
    "* [K = 20](#em_algo_bern_K20)\n",
    "* [K = 25](#em_algo_bern_K25)\n",
    "* [K = 5 per digit](#em_algo_bern_K5_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_load></a>\n",
    "## 3.1 Загрузка данных [[toc](#toc)]\n",
    "\n",
    "Загрузите обучающую и тестовую выборки [здесь](https://www.dropbox.com/s/8092jukwxapc04o/mnist.zip?dl=0). Первый столбец $-$ это метка цифры, изображенной на данной картинке. Бинаризуйте все изображения по порогу 127."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = pd.read_csv('./mnist/mnist_train.csv', header=None)\n",
    "digits = digits.values\n",
    "labels = digits[:, 0]\n",
    "digits = digits[:, 1:]\n",
    "digits = (digits > 127).astype(np.int32)\n",
    "n_samples, n_dim = labels.shape, digits.shape\n",
    "print(n_samples, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_Ks></a>\n",
    "### Анализ при различном числе компонент [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_labels = [6, 9]\n",
    "mask = np.zeros(n_samples, dtype=np.int32)\n",
    "for label in active_labels:\n",
    "    mask += (labels == label)\n",
    "mask = mask > 0\n",
    "X = digits[mask]\n",
    "print(X.shape)\n",
    "\n",
    "em = EMBernoulli(n_components=2, max_iter=60, batch_size=None, verbose=True)\n",
    "em(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = em.get_thetas()\n",
    "thetas = thetas.reshape((2, 28, 28))\n",
    "fsize = 6\n",
    "fig, axes = plt.subplots(1, 2, figsize=(2 * fsize, fsize))\n",
    "axes[0].imshow(thetas[0], cmap='afmhot', vmin=-0.2, interpolation='none')\n",
    "axes[1].imshow(thetas[1], cmap='afmhot', vmin=-0.2, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_Ks></a>\n",
    "#### Анализ при различном числе компонент [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните аналогичное исследование для всех изображений всех цифр. Используйте 50 итераций и разные значения $K=10, 15, 20$, для каждого визуализируйте логарифм неполного правдоподобия и получаемые шаблоны после каждой итерации EM-алгоритма. Если ваша реализация работает слишком медленно, отберите некоторое количество изображений каждой цифры из всей выборки и работайте только с ними. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_templates(digits, labels, n_components, percentage=1.0, \n",
    "                   memory_limit=512, max_iter=50, em_verbose=True, batch_size=None, random_state=1234):\n",
    "    if percentage < 1.0:\n",
    "        train_digits, val_digits, train_labels, val_labels = train_test_split(digits, labels, train_size=percentage, \n",
    "                                                                              stratify=labels)\n",
    "    else:\n",
    "        train_digits, train_labels = digits, labels\n",
    "    print(train_digits.shape, train_labels.shape)\n",
    "\n",
    "    em = EMBernoulli(n_components=n_components, batch_size=batch_size, \n",
    "                     memory_limit=memory_limit, max_iter=max_iter, verbose=em_verbose)\n",
    "    em(train_digits)\n",
    "    thetas = em.get_thetas()\n",
    "    pkl.dump(thetas, open('K{}_templates.pkl'.format(n_components), 'wb'))\n",
    "    pkl.dump(em.get_gammas(), open('K{}_gammas.pkl'.format(n_components), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_templates(n_components, fsize=2, n_cols=5):\n",
    "    thetas = pkl.load(open('K{}_templates.pkl'.format(n_components), 'rb'))\n",
    "    thetas.shape = (n_components, 28, 28)\n",
    "    n_rows = int(np.ceil(n_components / n_cols))\n",
    "    fig, axarr = plt.subplots(n_rows, n_cols, figsize=(n_cols * fsize, n_rows * fsize))\n",
    "    for n_digit, (n_row, n_col) in enumerate(product(range(n_rows), range(n_cols))):\n",
    "        ax = axarr[n_row][n_col]\n",
    "        ax.imshow(thetas[n_digit], cmap='afmhot', vmin=-0.2, interpolation='none')\n",
    "        ax.set_title('{}'.format(n_digit))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_K10></a>\n",
    "#### K = 10 [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1.0\n",
    "n_components = 10\n",
    "find_templates(digits, labels, n_components, percentage, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_templates(n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_K15></a>\n",
    "#### K = 15 [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1.0\n",
    "n_components = 15\n",
    "find_templates(digits, labels, n_components, percentage, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_templates(n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_K20></a>\n",
    "#### K = 20 [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1.0\n",
    "n_components = 20\n",
    "find_templates(digits, labels, n_components, percentage, batch_size=None, memory_limit=256, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_templates(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: 6, 1: 4, 2: 0, 3: 7, 4: 8,\n",
    "           5: 3, 6: 1, 7: 6, 8: 2, 9: 3,\n",
    "           10: 5, 11: 8, 12: 7, 13: 6, 14: 9,\n",
    "           15: 4, 16: 1, 17: 2, 18: 0, 19: 5}\n",
    "pred_labels = np.array([mapping[label] for label in np.argmax(gammas, axis=1)])\n",
    "print('accuracy = ', np.mean(pred_labels == train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_K25></a>\n",
    "#### K = 25 [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1.0\n",
    "n_components = 25\n",
    "find_templates(digits, labels, n_components, percentage=percentage, max_iter=50, memory_limit=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_templates(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K =', n_components)\n",
    "gammas = pkl.load(open('K{}_gammas.pkl'.format(n_components), 'rb'))\n",
    "candidates = [[9, 7], [7],    [9, 7], [6   ], [0],\n",
    "              [3, 5], [1],    [8],    [4, 9], [0],\n",
    "              [6],    [1],    [2],    [3, 8], [1],\n",
    "              [4, 9], [3, 8], [6],    [5],    [4, 9],\n",
    "              [2],    [8],    [4, 9], [7],    [3, 8]]\n",
    "n_steps = 10\n",
    "best_accuracy = 0\n",
    "best_mapping = None\n",
    "for values in tqdm(product(*candidates)):\n",
    "    mapping = {label:digit for label, digit in enumerate(values)}\n",
    "    pred_labels = np.array([mapping[label] for label in np.argmax(gammas, axis=1)])\n",
    "    accuracy = np.mean(pred_labels == labels)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_mapping = copy.deepcopy(mapping)\n",
    "        print('accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=em_algo_bern_K5_pd></a>\n",
    "#### K = 5 per digit [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "for digit in range(10):\n",
    "    X = digits[labels == digit]\n",
    "    print('digit = {}: n_samples = {}'.format(digit, X.shape[0]))\n",
    "    em = EMBernoulli(n_components, max_iter=50, verbose=False)\n",
    "    em(X)\n",
    "    gammas = em.get_gammas()\n",
    "    thetas = em.get_thetas()\n",
    "    pkl.dump(thetas, open('K{}_D{}_thetas.pkl'.format(n_components, digit), 'wb'))\n",
    "    pkl.dump(gammas, open('K{}_D{}_gammas.pkl'.format(n_components, digit), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "templates = []\n",
    "for digit in range(10):\n",
    "    templates.append(pkl.load(open('K{}_D{}_thetas.pkl'.format(n_components, digit), 'rb')))\n",
    "templates = np.vstack(templates)\n",
    "templates = np.maximum(templates, 1e-100)\n",
    "templates = np.minimum(templates, 1 - 1e-100)\n",
    "print(templates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_templates = 50\n",
    "n_cols = 10\n",
    "n_rows = int(1.0 * n_templates / n_cols + 0.5)\n",
    "fsize = 2\n",
    "f, axarr = plt.subplots(n_rows, n_cols, figsize=(n_cols * fsize, n_rows * fsize))\n",
    "for n_label, (n_row, n_col) in enumerate(product(range(n_rows), range(n_cols))):\n",
    "    axarr[n_row][n_col].imshow(templates[n_label].reshape((28, 28)), cmap='afmhot')\n",
    "    axarr[n_row][n_col].set_xticks([])\n",
    "    axarr[n_row][n_col].set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits\n",
    "y = labels\n",
    "\n",
    "n_correct = 0\n",
    "period = 1000\n",
    "for n_sample in range(X.shape[0]):\n",
    "    x = X[[n_sample], :]\n",
    "    plt.imshow(x.reshape((28, 28)))\n",
    "    plt.title('{}'.format(y[n_sample]))\n",
    "    logs = np.sum(x * np.log(templates) + (1 - x) * np.log(1 - templates), axis=1)\n",
    "    pred_y = np.argmax(logs) // n_components\n",
    "    if pred_y == y[n_sample]:\n",
    "        n_correct += 1\n",
    "    if (n_sample + 1) % period == 0:\n",
    "        print('accuracy =', n_correct / (n_sample + 1))\n",
    "accuracy = n_correct / len(y)\n",
    "print('accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каких значений $K$ вам удалось выделить шаблоны всех цифр? Какие цифры оказались самыми сложными для распознавания и потребовали нескольких шаблонов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для одной из обученных моделей проведите следующее исследование. Вручную свяжите каждый шаблон с цифрой, которая на нем изображена. Затем, используя апостериорные распределения объектов тестовой выборки и привязку шаблонов к цифрам, определите цифру каждого тестового изображения и подсчитайте точность классификации. Согласуется ли правдоподобие обучающей и тестовой выборок с точностью классификации на тестовой выборке? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
