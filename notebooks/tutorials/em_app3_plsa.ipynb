{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='em_topic_modelling'></a>\n",
    "# Тематическое моделирование\n",
    "\n",
    "![](http://imgur.com/S8WgwBp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Содержание\n",
    "* [1. Рассуждения на тему тематического моделирования](#discussion_on_topic_modeling)\n",
    "* [2. Реализация ЕМ-алгоритма для модели PLSA и его применение](#plsa_and_application)\n",
    "    * [2.1 Загрузка данных](#load)\n",
    "    * [2.2 Реализация PLSA](#plsa)\n",
    "    * [2.3 Задание 1 [3 балла]](#plsa_task1)\n",
    "    * [2.4 Задание 2 [0.5 балла]](#plsa_task2)\n",
    "    * [2.5 Задание 3 [0.5 балла]](#plsa_task3)\n",
    "* [3. Модель LDA и визуализация](#lda_and_visualization)\n",
    "    * [3.1 Задание 1 [1 балла]](#lda_task1)\n",
    "    * [3.2 Задание 2 [1 балл]](#lda_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 30\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "_add_to_path = True\n",
    "import math\n",
    "import scipy\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "from scipy.special import erfc\n",
    "\n",
    "from itertools import product, chain\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.01\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = 20\n",
    "matplotlib.rcParams['axes.titlesize'] = 20\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14)\n",
    "matplotlib.rc('ytick', labelsize=14)\n",
    "matplotlib.rc('legend', fontsize=16)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}') \n",
    "matplotlib.rcParams['axes.labelsize'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _add_to_path:\n",
    "    sys.path.append('../../')\n",
    "from ml.core import Checker, Printer\n",
    "from ml.mixtures import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# Below comes the list of modules which is automatically reimported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='discussion_on_topic_modeling'></a>\n",
    "# 1. Рассуждения на тему тематического моделирования [[toc](#toc)]\n",
    "\n",
    "Тематическое моделирование является популярным инструментом анализа текстов. Задача заключается в поиске тем $T$, которые хорошо бы описывали документы $D$ со словарём $W$. Большинство тематических моделей оперирует данными в формате \"мешка слов\", т.е. учитывают только частоты слов в документах, а не их порядок. Одной из простейших тематических моделей является [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis), которая приводит к задаче стохастического матричного разложения: \n",
    "\n",
    "$$F \\approx \\Phi \\times \\Theta$$\n",
    "где\n",
    "- $F_{W \\times D}$— матрица распределений слов в документах (нормированные частоты)\n",
    "- $\\Phi_{W \\times T}$ — матрица распределений слов в темах (модель)\n",
    "- $\\Theta_{T \\times D}$ — матрица распределений тем в документах (результат применения модели к обучающим данным)\n",
    "\n",
    "Можно сказать, что алгоритмы тематического моделирования производят мягкую бикластеризацию данных:\n",
    " - *мягкую*, так как объекты относятся не строго к одному кластеру, а к нескольким с разными вероятностями\n",
    " - *бикластеризацию*, так как модель одновременно кластеризует слова по темам и темы по документам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 EM-алгоритм [[toc](#toc)]\n",
    "\n",
    "![](http://imgur.com/EeIuI1T.png)\n",
    "\n",
    "С вероятностной точки зрения, задача обучения модели PLSA ставится как максимизация неполного правдоподобия по параметам $\\Phi$ и $\\Theta$. ЕМ-алгоритм для модели PLSA заключается в повторении двух шагов:\n",
    "\n",
    "- **Е-шаг** — оценка распределений тем для каждого слова в каждом документе по параметрам $\\Phi$ и $\\Theta$ (шаг 6);\n",
    "- **М-шаг** — обновление параметров $\\Phi$ и $\\Theta$ на основе полученных оценок (шаги 7 и 9).\n",
    "\n",
    "Существуют различные модификации итерационного процесса, позволяющие снизить расходы по памяти. В данном случае, мы избегаем хранения трехмерной матрицы $p_{tdw}$, сразу пересчитывая $\\Theta$ для текущего документа и аккумулируя счетчики $n_{wt}$ для последующего пересчета $\\Phi$.\n",
    "\n",
    "Псевдокод алгритма записывается следующим образом:\n",
    "\n",
    "1. Инициализировать $\\phi_{wt}^0$ для всех $w \\in W$, $t \\in T$ и $\\theta_{td}^0$ для всех $t \\in T$, $d \\in D$\n",
    "2. Внешний цикл по итерациям $i = 1 ... max\\_iter$:\n",
    "3. $\\quad$ $n_{wt}^i := 0$, $n_t^i := 0$ для всех $w \\in W$ и $t \\in T$ \n",
    "4. $\\quad$ Внутренний цикл по документам $d \\in D$  \n",
    "5. $\\qquad$ $Z_w := \\sum_{t \\in T} \\phi_{wt}^{i-1}\\theta_{td}^{i-1}$ для всех $w \\in d$ $\\cfrac{}{}$\n",
    "6. $\\qquad$ $p_{tdw} := \\cfrac{ \\phi_{wt}^{i-1}\\theta_{td}^{i-1} }{ Z_w }$ (**E-шаг**)\n",
    "7. $\\qquad$ $\\theta_{td}^{i} := \\cfrac{ \\sum_{w \\in d} n_{dw} p_{tdw} }{ n_d }$ для всех $t \\in T$ (**M-шаг**)\n",
    "8. $\\qquad$ Увеличить $n_{wt}^i$ и $n_t^i$ на $n_{dw} p_{tdw}$ для всех $w \\in W$ и $t \\in T$\n",
    "9. $\\quad \\phi_{wt}^i := \\cfrac{n_{wt}^i}{n_t^i}$ для всех $w \\in W$ и $t \\in T$ (**M-шаг**)\n",
    "\n",
    "Обозначения:\n",
    " - $p_{tdw}$ — вероятность темы $t$ для слова $w$ в документе $d$\n",
    " - $\\phi_{wt}$ — элемент матрицы $\\Phi$, соответствующий вероятности слова $w$ в теме $t$\n",
    " - $\\theta_{td}$ — элемент матрицы $\\Theta$, соответствующий вероятности темы $t$ в документе $d$\n",
    " - $n_{wt}$ — элемент матрицы счётчиков отнесения слова $w$ к теме $t$ (путем нормирования этой матрицы получается матрица $\\Phi$)\n",
    " - $Z_w$ — элемент вектора вспомогательных переменных, соответствующий слову $w$\n",
    " - $n_t$ — вектор нормировочных констант для матрицы $n_{wt}$\n",
    " - $n_d$ — вектор нормировочных констант для матрицы $n_{dw}$\n",
    " - $n$ — суммарное число слов в коллекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Оценка качества [[toc](#toc)]\n",
    "\n",
    "Для оценивания качества построенной модели и контроля сходимости процесса обучения обычно используют [перплексию](http://www.machinelearning.ru/wiki/images/8/88/Voron-iip9-talk.pdf):\n",
    "\n",
    "$$\\mathcal{P} = \\exp\\bigg(- \\frac{\\mathcal{L}}{n} \\bigg) = \\exp\\bigg(- \\cfrac{1}{n}\\sum_{d \\in D}\\sum_{w \\in d} n_{dw} \\ln \\big(\\sum_{t \\in T}\\phi_{wt}\\theta_{td} \\big)\\bigg)$$\n",
    "\n",
    "Это традиционная мера качества в тематическом моделировании, которая основана на правдоподобии модели $\\mathcal{L}$. Число итераций $max\\_iter$ в алгоритме обучения следует выбирать достаточным для того, чтобы перплексия перестала существенно убывать. Однако известно, что перплексия плохо отражает интерпретируемость найденных тем, поэтому помимо нее обычно используются дополнительные меры или экспертные оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsa_and_application'></a>\n",
    "# 2. Реализация ЕМ-алгоритма для модели PLSA и его применение [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "## 2.1 Загрузка данных [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [коллекцию писем Х. Клинтон](https://www.dropbox.com/s/je8vq5fsb8xpy2u/hillary_data.zip?dl=0). \n",
    "\n",
    "Извлеките полные тексты писем из файла *Emails.csv* и подготовьте данные в формате \"мешка слов\" с помощью функции  [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) пакета sklearn. Рекомендуется произвести фильтрацию слов по частотности для удаления слишком редких и стоп-слов (рекомендованный нижний порог в пределах 10 и верхний 400-600)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_path = './hillary_data/output/Emails.csv'\n",
    "raw_data = pd.read_csv(emails_path)\n",
    "emails = list(raw_data['RawText'])\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=500, stop_words='english')\n",
    "DW = vectorizer.fit_transform(emails)\n",
    "\n",
    "word2index = dict(vectorizer.vocabulary_)\n",
    "index2word = {i:w for w,i in word2index.items()}\n",
    "print('Document-word matrix size = {}'.format(DW.shape))\n",
    "print('Dictionary size = {}'.format(len(word2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of empty documents:', np.sum(np.sum(DW, axis=1) == 0))\n",
    "print('Number of never occuring terms:', np.sum(np.sum(DW, axis=0) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оказывается, есть такие тексты, векторное представление которых содержит только нули. Эти тексты нужно убрать, так как по существу они бессмысленны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DW.shape)\n",
    "n_terms = DW.sum(axis=1).A1.flatten()\n",
    "if np.any(n_terms == 0):\n",
    "    DW = DW[np.flatnonzero(n_terms != 0)]\n",
    "print(DW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of empty documents:', np.sum(np.sum(DW, axis=1) == 0))\n",
    "print('Number of never occuring terms:', np.sum(np.sum(DW, axis=0) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsa'></a>\n",
    "## 2.2 Реализация ЕМ-алгоритма [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSA(Checker):\n",
    "    def __init__(self, n_topics, init='rand', max_iter=10, verbose=0, seed=0,\n",
    "                 eps=1e-30, check_errors=False, max_error=1e-10,\n",
    "                 memory_limit=2048):\n",
    "        super().__init__(max_error)\n",
    "        self._n_topics = n_topics\n",
    "        self._init = init\n",
    "        self._seed = seed\n",
    "        self._max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self._check_errors = check_errors\n",
    "        self._eps = eps\n",
    "        self._class_name = type(self).__name__\n",
    "        self._memory_limit = memory_limit\n",
    "        self._n_runs = 0\n",
    "        \n",
    "    def set_max_iter(self, max_iter):\n",
    "        self._check_int(max_iter, 'max_iter')\n",
    "        self._check_positive(max_iter, 'max_iter')\n",
    "        self._max_iter = max_iter\n",
    "        \n",
    "    def __call__(self, DW, copy=False, continued=False):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            :param DW - матрица документ-термин\n",
    "            :type DW  - scipy.sparse.csr_matrix размера [n_documents, n_terms]\n",
    "        \"\"\"\n",
    "        if continued:\n",
    "            self._check_continuation(DW, continued)\n",
    "        else:\n",
    "            self._set_DW(DW, copy)\n",
    "            self._initialize_phi_theta()\n",
    "            self._initialize_params()\n",
    "            self._determine_method()\n",
    "        self._printers[3](self._make_msg('initialized'))\n",
    "        output = self._method()\n",
    "        self._n_runs += 1\n",
    "        self._initial_iter = self._n_iter\n",
    "        return output\n",
    "\n",
    "    def _iterative(self):\n",
    "        for n_iter in range(self._initial_iter, self._initial_iter + self._max_iter):\n",
    "            self._n_iter = n_iter\n",
    "            self.H_wt = np.zeros_like(self.Phi)                               # [W x T] +\n",
    "            self._log_perplexity = 0\n",
    "            for n_doc in range(self._n_docs):\n",
    "                P_dwt = np.multiply(self.Phi, self.Theta[:, n_doc][None, :])  # [W x T] +\n",
    "                P_dw  = np.maximum(np.sum(P_dwt, axis=1)[:, None], self._eps) # [W, 1]  +\n",
    "                N_dw  = self.DW[n_doc].toarray().T                            # [W, 1]  +\n",
    "                self._log_perplexity -= np.sum(N_dw * np.log(P_dw)) / self._n\n",
    "                H_dwt = P_dwt / P_dw                                          # [W x T] +\n",
    "                H_dwt = np.multiply(N_dw, H_dwt)                              # [W x T]\n",
    "                self.H_wt += H_dwt                                            # [W x T]\n",
    "                self.Theta[:, n_doc] = np.sum(H_dwt, axis=0) / self._n_d[n_doc]\n",
    "                if self._check_errors:\n",
    "                    self._check_distr(self.Theta[:, n_doc], 'Theta[:, {}]'.format(n_doc))\n",
    "            self.Phi = self.H_wt / np.sum(self.H_wt, axis=0)[None, :]    # [W x T]\n",
    "            if self._check_errors:\n",
    "                for n_topic in range(self._n_topics):\n",
    "                    self._check_dsitr(self.Phi[:, n_topic], 'Phi[:, {}]'.format(n_topic))\n",
    "                self._printers[6]('n_iter = {}: errors checking activated: no errors')\n",
    "            self._log_perplexities.append(self._log_perplexity)\n",
    "            self._printers[1]('n_iter = {}: log_P = {}'.format(self._n_iter, self._log_perplexity))\n",
    "        return self.Phi, self.Theta, self._log_perplexities\n",
    "    \n",
    "    def _direct(self):\n",
    "        self.WD = self.DW.toarray().T             # [W x D] +\n",
    "        self.Theta = self.Theta.T\n",
    "        for n_iter in range(self._initial_iter, self._initial_iter + self._max_iter):\n",
    "            self._n_iter = n_iter\n",
    "            H_wdt = self.Phi.reshape((self._voc_size, 1, self._n_topics)) * \\\n",
    "                    self.Theta.reshape((1, self._n_docs, self._n_topics))  # [W x D x T] +\n",
    "            H_wdt = np.maximum(H_wdt, self._eps)\n",
    "            P_wd = np.sum(H_wdt, axis=2)           # [W x D] +\n",
    "            self._log_perplexity = -np.sum(self.WD * np.log(P_wd)) / self._n\n",
    "            H_wdt = H_wdt / P_wd[:, :, None]       # [W x D x T]\n",
    "            H_wdt = self.WD[:, :, None] * H_wdt    # [W x D x T]\n",
    "            self.Theta = H_wdt.sum(axis=0) / self._n_d[:, None]\n",
    "            self.Phi = H_wdt.sum(axis=1)           # [W x T]\n",
    "            self.Phi = self.Phi / self.Phi.sum(axis=0)[None, :]\n",
    "            self._printers[1]('n_iter = {}: log_P = {}'.format(self._n_iter, self._log_perplexity))\n",
    "        self.Theta = self.Theta.T\n",
    "        return self.Phi, self.Theta, self._log_perplexities\n",
    "            \n",
    "    def _check_continuation(self, DW, continued):\n",
    "        assert isinstance(DW, scipy.sparse.csr_matrix) \n",
    "        if continued:\n",
    "            # Проверка того, что уже запускали\n",
    "            assert self._n_runs > 0\n",
    "            # Проверка того, что размер переданной матрицы совпадает с ранее исопользованными\n",
    "            assert self._n_docs, self._voc_size == DW.shape\n",
    "\n",
    "    def _set_DW(self, DW, copy):\n",
    "        assert isinstance(DW, scipy.sparse.csr_matrix)\n",
    "        if copy:\n",
    "            self.DW = copy.deepcopy(DW)\n",
    "        else:\n",
    "            self.DW = DW\n",
    "        self._n_d = self.DW.sum(axis=1).A1\n",
    "        self._n   = np.sum(self._n_d)\n",
    "        self._n_docs, self._voc_size = self.DW.shape\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        self._log_perplexities = []\n",
    "        self._n_iter = 0\n",
    "        self._initial_iter = 0\n",
    "        \n",
    "    def _initialize_phi_theta(self):\n",
    "        if self._init == 'rand':\n",
    "            np.random.seed(self._seed)\n",
    "            self.Phi = np.maximum(np.random.rand(self._voc_size, self._n_topics), self._eps)\n",
    "            self.Theta = np.maximum(np.random.rand(self._n_topics, self._n_docs), self._eps)\n",
    "            self.Phi = self.Phi / np.sum(self.Phi, axis=0)[None, :]\n",
    "            self.Theta = self.Theta / np.sum(self.Theta, axis=0)[None, :]\n",
    "        elif self._init == 'uniform':\n",
    "            self.Phi = np.full((self._voc_size, self._n_topics), 1.0 / self._voc_size)\n",
    "            self.Theta = np.full((self._n_topics, self._n_docs), 1.0 / self._n_topics)\n",
    "        else:\n",
    "            raise ValueError(self._make_msg('Unknown value of init', method_name))        \n",
    "        \n",
    "    def _determine_method(self):\n",
    "        nbytes = self.DW.dtype.type(0).nbytes\n",
    "        required_memory_direct = (self._n_topics * self._n_docs +     # self.Theta\n",
    "                                  self._voc_size * self._n_topics +   # self.Phi\n",
    "                                  self._voc_size * self._n_docs * self._n_topics + # H_wdt\n",
    "                                  self._voc_size * self._n_docs) * nbytes * 1.0  # P_wd\n",
    "        required_memory_iter   = (self._n_topics * self._n_docs +     # self.Theta\n",
    "                                  self._voc_size * self._n_topics +   # self.Phi\n",
    "                                  self._voc_size * self._n_topics +   # H_wt\n",
    "                                  self._voc_size * self._n_topics +   # P_dwt\n",
    "                                  self._voc_size * self._n_topics +   # H_dwt\n",
    "                                  self._voc_size * 2) * nbytes * 1.0 # N_dw, P_dw\n",
    "        required_memory_direct /= 2 ** 20\n",
    "        required_memory_iter /= 2 ** 20\n",
    "        self._printers[5]('Required memory direct = {} Mb'.format(required_memory_direct))\n",
    "        self._printers[5]('Required memory iter   = {} Mb'.format(required_memory_iter))\n",
    "        self._printers[5]('Memory limit   = {} Mb'.format(self._memory_limit))\n",
    "        if required_memory_direct > self._memory_limit:\n",
    "            self._printers[5]('Choosing iterative solving method')\n",
    "            self._method = self._iterative\n",
    "        else:\n",
    "            self._printers[5]('Choosing direct sovling method')\n",
    "            self._method = self._direct\n",
    "        \n",
    "    def _make_msg(self, msg, method_name=None):\n",
    "        if method_name is None:\n",
    "            return '{}:{}'.format(self._class_name, msg)\n",
    "        return '{}:{}:{}'.format(self._class_name, method_name, msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsa_task1'></a>\n",
    "## 2.3 Задание 1 [3 балла] [[toc](#toc)]\n",
    "Примените ваш алгоритм к подготовленным данным, рассмотрите число тем T = 5. Постройте график значения перплексии в зависимости от итерации (убедитесь в корректности реализации: график перплексии должен быть невозрастающим). Выведите для каждой темы топ-20 наиболее вероятных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1.1  Применение алгоритма к подготовленным данным для числа тем T = 5. Построение графика зависимости значения перплексии в зависимости от итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa = PLSA(n_topics=5, init='rand', max_iter=100, verbose=11, seed=11)\n",
    "Phi, Theta, log_perplexities = plsa(DW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(log_perplexities, marker='x', color='b', zorder=2)\n",
    "plt.grid(linestyle='--', alpha=0.5)\n",
    "plt.title('Perplexity vs PLSA iteration');\n",
    "plt.xlabel('iteration');\n",
    "plt.ylabel('$\\log(\\mathcal{P})$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2.1.2 Вывод 20 наиболее вероятных слов для каждой темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_indices(values, n_max):\n",
    "    values = [[i, x] for i, x in enumerate(values)]\n",
    "    values = sorted(values, key=lambda x: -x[1])\n",
    "    indices = [i for i, x in values[:n_max]]\n",
    "    return np.array(indices)\n",
    "\n",
    "def get_representatives(Phi, n_max):\n",
    "    representatives = pd.DataFrame()\n",
    "    for topic in range(Phi.shape[1]):\n",
    "        words = [index2word[i] for i in get_max_indices(Phi[:, topic], max_words)]\n",
    "        representatives[topic] = words\n",
    "    return representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20\n",
    "representatives = get_representatives(Phi, max_words)\n",
    "print(representatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsa_task2'></a>\n",
    "## 2.2 Задание 2 [0.5 балла] [[toc](#toc)]\n",
    "Рассмотрите число тем T = 10, 20. Сравните между собой топ-20 наиболее вероятных слов в каждой теме (а также для модели, полученной ранее). Можно ли сказать, что конкретность каждой темы изменяется с ростом их числа?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Применение алгоритма для числа тем T = 10. Вывод топ-20 наиболее вероятных слов для каждой темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Применение алгоритма для числа тем T = 20. Вывод топ-20 наиболее вероятных слов для каждой темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, с ростом числа тем растет интерпретируемость полученных результатов, т.е. растет все большему числу тем можно поставить в соответствие некую метку. Например, найденным выше группам топ-20 наиболее вероятных слов можно присвоить следующие метки: \"политика\", \"дипломатия\", \"военные действия\", \"безопасность\", \"Персидский Залив\", \"гуманитарная помощь\", \"проживаение и путешествие\" (тема 6 для T = 20) и т.п."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plsa_task3'></a>\n",
    "## 2.3 Задание 3 [0.5 балла] [[toc](#toc)]\n",
    "Протестируйте модель для разных начальных приближений. Что можно сказать об устойчивости алгоритма (идентичны ли топовые слова в соответствующих темах моделей)?\n",
    "\n",
    "Ниже модель запускается для 4-х различных значений \"зерна\" генератора случайных чисел. Число тем равно 10, а число итераций выбрано равным 40. Во всех случаях топовые слова в темах совпали. Таким образом, можно утверждать, что алгоритм PLSA достаточно устойчив к выбору начальных приближений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda_and_visualization'></a>\n",
    "# 3. Модель LDA и визуализация [[toc](#toc)]\n",
    "\n",
    "Модель [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) является наиболее популярной тематической моделью. Единственное отличие от модели PLSA заключается в введении априорных распределений Дирихле на столбцы матриц $\\Phi$ и $\\Theta$, которое может способствовать дополнительному сглаживанию или разреживанию параметров.\n",
    "\n",
    "В этом задании предлагается воспользоваться реализацией модели [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html), обучение которой основано на вариационном байесовском выводе. Для выполнения задания вам потребуется установить пакеты [gensim](https://radimrehurek.com/gensim/install.html) и [pyldavis 2.0](https://pyldavis.readthedocs.io/en/latest/readme.html#installation).\n",
    "\n",
    "Подготовьте данные в формате, подходящем для *gensim* (полное API gensim можно найти [здесь](https://radimrehurek.com/gensim/apiref.html)). Пример обработки вывода *CountVectorizer* для gensim можно найти [здесь](https://gist.github.com/aronwc/8248457)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda_task1'></a>\n",
    "## 3.1 Задание 1 [1 балл] [[toc](#toc)]\n",
    "\n",
    "Примените [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html) к подготовленным данным (рекомендуется задать заведомо большое число итераций в параметре *passes*, например, 30). Визуально сравните полученные темы по топ-20 наиболее вероятным словам с темами, полученными вашей реализацией ЕМ-алгоритма. У какой из моделей получились более интерпретируемые темы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "emails_path = './hillary_data/output/Emails.csv'\n",
    "raw_data = pd.read_csv(emails_path)\n",
    "emails = list(raw_data['RawText'])\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=500, stop_words='english')\n",
    "DW = vectorizer.fit_transform(emails)\n",
    "\n",
    "word2index = dict(vectorizer.vocabulary_)\n",
    "index2word = {i:w for w,i in word2index.items()}\n",
    "print('Document-word matrix size = {}'.format(DW.shape))\n",
    "print('Dictionary size = {}'.format(len(word2index)))\n",
    "num_topics = 10\n",
    "passes = 40\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(DW.tocoo().transpose().tocsc())\n",
    "lda = LdaModel(corpus, num_topics=num_topics, passes=passes, id2word=index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_representatives(lda, index2word, n):\n",
    "    representatives = pd.DataFrame()\n",
    "    topics = lda.show_topics(num_topics=-1, num_words=n, formatted=False)\n",
    "    for ti, topic in topics:\n",
    "        words = [w for w, p in topic]\n",
    "        representatives[ti] = words\n",
    "    return representatives\n",
    "\n",
    "max_words = 20\n",
    "representatives = get_lda_representatives(lda, index2word, max_words)\n",
    "print(representatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, оба алгоритма PLSA и LDA выделяют одни и те же темы. Стоит отметить, что какие-то темы лучше выделены в результате применения PLSA, в то время как другие - в результате применения LDA. Однако существенных различий не наблюдается, что может быть связано с достаточно большим размером коллекции текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda_task2'></a>\n",
    "## 3.2 Задание  2 [1 балл] [[toc](#toc)]\n",
    "Визуализируйте модель из gensim с помощью ldavis (описание API LDAvis для работы с gensim есть [здесь](http://pyldavis.readthedocs.io/en/latest/modules/API.html)), пример — [здесь](https://github.com/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_matrix = lda.get_topics()\n",
    "print(topic_term_matrix.shape)\n",
    "\n",
    "doc_topic_matrix = np.zeros((DW.shape[0], num_topics))\n",
    "for n_doc in tqdm(range(DW.shape[0])):\n",
    "    dw = DW[n_doc]\n",
    "    bow = []\n",
    "    for n_row in dw.nonzero()[1]:\n",
    "        bow.append((n_row, dw[0, n_row]))\n",
    "    doc_topic_list = lda.get_document_topics(bow)\n",
    "    for topic, prob in doc_topic_list:\n",
    "        doc_topic_matrix[n_doc, topic] = prob\n",
    "print(doc_topic_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {}\n",
    "doc_topic_matrix /= np.sum(doc_topic_matrix, axis=1)[:, None]\n",
    "model_data['topic_term_dists'] = topic_term_matrix\n",
    "model_data['doc_topic_dists'] = doc_topic_matrix\n",
    "model_data['doc_lengths'] = DW.sum(axis=1).A1.flatten()\n",
    "model_data['term_frequency'] = DW.sum(axis=0).A1.flatten()\n",
    "model_data['vocab'] = [index2word[i] for i in range(DW.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "vis_data = pyLDAvis.prepare(**model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.prepare(mds='tsne', **model_data)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.prepare(mds='mmds', **model_data)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "dictionary = gensim.corpora.Dictionary.from_corpus(corpus, index2word)\n",
    "vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекомендации к выполнению [[toc](#toc)]\n",
    "Для обучения *LdaModel* и её последующей визуализации потребуется словарь формата gensim, который можно получить следующей командой\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary.from_corpus(corpora, vocab_dict)\n",
    "\n",
    "где *corpora* содержит полученное с помощью gensim представление коллекции, а *vocab_dict* — это dict, полученный после работы CountVectorizer, ставящий в соответствие каждому номеру строки в матрице данных само слово в виде строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
