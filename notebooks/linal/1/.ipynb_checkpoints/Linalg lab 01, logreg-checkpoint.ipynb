{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 6. Линейная Алгебра и Машинное Обучение  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошее знание линейной алгебры очень важно в современном машинном обучении. В этом задании Вам предлагается реализовать метод машинного обучения применив знания матричного дифференцирования и итерационных методов решения систем уравнений :) \n",
    "\n",
    "В области машинного обучения одним из самых популярных методов бинарной классификации (предсказываем один из двух классов, $+1$ или $-1$ для каждого объекта) является логистическая регрессия. Она выводится из метода максимального правдоподобия, который приводит к следующей задаче оптимизации:\n",
    "\n",
    "$$ L(w, X, y) = \\sum_{i = 0}^{N} log (1 + exp(-y_ix_i^Tw)) + \\frac{C}{2} ||w||^2 \\longrightarrow \\min_w$$\n",
    "$$X \\in R^{N \\times M}, x \\in R^{M}, w \\in R^{M}, y \\in \\{-1, 1\\}^N$$\n",
    "\n",
    "Здесь $X$ - матрица объекты-признаки для обучающей выборки (по строкам объекты, по столбцам признаки), а $y$ - вектор ответов. Коэффициент $C$, вообще говоря, нужно подбирать отдельно, поскольку разные его значения приводят к разным решениям задачи оптимизации. Но так как это уже никакого отношения не имеет к линейной алгебре, то в этой задаче мы положим $\\mathbf{C = 1}$\n",
    "\n",
    "Когда мы решили задачу оптимизации (нашли $w$), мы принимаем решение о том, к какому классу относится объект по правилу $y(x) = sign(x^Tw)$. В данной части вам необходимо применить методы линейной алгебры для решения этой задачи. \n",
    "\n",
    "План у нас такой:\n",
    "- Вычислить градиент функции $L$, эффективно запрограммировать и проверить себя\n",
    "- Вычислить гессиан функции $L$,  эффективно запрограммировать и проверить себя\n",
    "- Воспользоваться методом второго порядка для оптимизации \n",
    "- Внутри метода оптимизации вместо обращения матрицы, решать систему уравнений с помощью итерационного метода\n",
    "- Исследовать эффективность различных методов решения системы уравнений на реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sla\n",
    "import scipy.sparse as sps\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy import special\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования правильности вычисления сгенерируем аргументы небольшого размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, X, y = np.random.random(10), np.random.random((11, 10)), 2*(np.random.randint(0, 2, 11)-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6.0 (0,5 балла)**\n",
    "\n",
    "Запрограммируйте вычисление функции L, используйте только матричные операции (внутри не должно быть циклов).\n",
    "\n",
    "**Замечание**: Нигде в промежуточных вычислениях не стоит вычислять значение $exp(−y_ix^Tw)$, иначе может произойти переполнение. Вместо этого следует напрямую вычислять необходимые величины с помощью специализированных для этого функций: `np.logaddexp` для `ln(1 + exp(·))` и `sp.special.expit` для `1/(1 + exp(-(·)))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(w, X, y):\n",
    "    '''\n",
    "        logistic(w, X, y) вычисляет функцию качества лог регрессии L(w, X, y)\n",
    "        \n",
    "        w: np.array размера (M,)\n",
    "        X: np.array размера (N, M)\n",
    "        y: np.array размера (M,)\n",
    "        \n",
    "        funcw: np.float \n",
    "    '''\n",
    "    funcw = # Вычислите функцию L\n",
    "    return funcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(w, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6.1 (1 балл)**\n",
    "\n",
    "Найдите градиент функции $\\nabla_w L(w, X, y)$, запишите в терминах матричных операций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<Решение>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эффективно запрограммируйте вычисление градиента (опять же, только матричные операции!)\n",
    "\n",
    "Обратите внимание на то, что для разреженных матриц понадобится написать немного другой код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad(w, X, y):\n",
    "    '''\n",
    "        logistic_grad(w, X, y) вычисляет градиент функции качества лог регрессии dL(w, X, y)/dw\n",
    "        \n",
    "        w: np.array размера (M,)\n",
    "        X: np.array размера (N, M)\n",
    "        y: np.array размера (M,)\n",
    "        \n",
    "        gradw: np.array размера (M,)\n",
    "    '''\n",
    "    if sps.issparse(X):\n",
    "        gradw = # Вычислите градиент функции dL/dw\n",
    "    else:\n",
    "        gradw = # Вычислите градиент функции dL/dw\n",
    "    return gradw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(logistic_grad(w, X, y).shape == w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень часто при подсчёте градиента допускаются ошибки, проверьте правильность реализации подсчёта градиента с помощью конечных разностей. \n",
    "\n",
    "$$[\\nabla f(x)]_i \\approx \\frac{f(x + \\epsilon \\cdot e_i) - f(x)}{\\epsilon}~~~~$$\n",
    "\n",
    "где $e_i = (0, ... , 0, 1, 0, ..., 0)$ - i-й базисный орт, $\\epsilon \\approx 10^{-8}$\n",
    "\n",
    "Ваша функция должна корректно работать хотя бы с обыкновенными (не разреженными матрицами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_error(a, b): \n",
    "    return np.max(np.abs(a-b))\n",
    "\n",
    "def grad_finite_diff(func, w, eps=1e-8):\n",
    "    '''\n",
    "        w: np.array размера (M,)\n",
    "        func: скалярная функция от векторного аргумента w, func(w) =  число\n",
    "        eps: np.float константа для проверки градиента\n",
    "        \n",
    "        dnum: np.array размера (M,), численно посчитанный градиент\n",
    "    '''\n",
    "    w, fval, dnum = w.astype(np.float64), func(w), np.zeros_like(w)\n",
    "\n",
    "    for i in range(w.size):\n",
    "        ei = # Вектор нулей (0, ..., 0, 1, 0, ..., 0) c 1 в позиции i\n",
    "        dnum[i] = # Вычислите численный градиент d func/dw_i с помощью конечных разностей\n",
    "\n",
    "    return dnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_grad = logistic_grad(w, X, y)\n",
    "num_grad = grad_finite_diff(lambda w: logistic(w, X, y), w)\n",
    "\n",
    "err = max_error(mat_grad, num_grad)\n",
    "print('err = ', err, 'ok' if err < 1e-6 else 'ошибка очень большая =(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6.2 (1 балл)**\n",
    "\n",
    "Для некоторых задач оптимизации очень удобно использовать гессиан. \n",
    "\n",
    "Вычислите гессиан для функции L, запишите ответ в терминах матричных операций. \n",
    "\n",
    "**Упражнение**: Можно ли что-то сказать про знакоопределенность этой матрицы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<Решение>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Эффективно запрограммируйте вычисление гессиана. Не забудьте написать отдельную рутину для разреженных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hess(w, X, y):\n",
    "    '''\n",
    "        logistic_hess(w, X, y) вычисляет гессиан функции качества лог регрессии dL(w, X, y)/dw\n",
    "        \n",
    "        w: np.array размера (M,)\n",
    "        X: np.array размера (N, M)\n",
    "        y: np.array размера (M,)\n",
    "        \n",
    "        hessw: np.array размера (M, M)\n",
    "    '''\n",
    "    \n",
    "    hessw = # Гессиан dL/dw_iw_j\n",
    "    \n",
    "    return hessw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(logistic_hess(w, X, y).shape == (w.shape[0], w.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим правильность реализации подсчёта гессиана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для гессиана проверка выглядит похожим образом\n",
    "\n",
    "$$[\\nabla^2 f(x)]_{ij} \\approx \\frac{f(x + \\epsilon \\cdot e_i + \\epsilon \\cdot e_j) -f(x + \\epsilon \\cdot e_i) - f(x + \\epsilon \\cdot e_j)+ f(x)}{\\epsilon^2}~~~~~~~~~~~~~~~~~~~~~$$\n",
    "\n",
    "где $e_i = (0, ... , 0, 1, 0, ..., 0)$ - i-й базисный орт, $\\epsilon \\approx 10^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hess_finite_diff(func, w, eps=1e-5):\n",
    "    '''\n",
    "        w: np.array размера (M,)\n",
    "        func: скалярная функция от векторного аргумента w, func(w) =  число\n",
    "        eps: np.float константа для проверки градиента\n",
    "        \n",
    "        dnum: np.array размера (M,), численно посчитанный градиент\n",
    "    '''\n",
    "    w, fval, dnum = w.astype(np.float64), func(w).astype(np.float64), np.zeros((w.size, w.size), dtype=np.float64)\n",
    "    dnum = # Вычислите численный гессиан d func/dw_iw_j для всех i, j\n",
    "    return dnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_grad = logistic_hess(w, X, y)\n",
    "num_grad = hess_finite_diff(lambda w: logistic(w, X, y), w)\n",
    "\n",
    "err = max_error(mat_grad, num_grad)\n",
    "\n",
    "print('err = ', err)\n",
    "print('ok' if max_error(mat_grad, num_grad) < 1e-5 else 'ошибка оч большая =(') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6.3 (3+ баллов)**\n",
    "\n",
    "Так как наша задача оптимизации оказывается выпуклой (см. упражнение про знакоопределённость Гессиана), её можно эффективно решать методом второго порядка, например, методом Ньютона. Напомним, что в общем виде метод Ньютона для решения уравнения (системы уравнений) $H(z) = 0$ имеет вид\n",
    "\n",
    "$$z_{k+1} = w_k - \\mathbf{\\alpha_k}\\left(\\nabla H(z_k)\\right)^{-1}H(z_k)$$\n",
    "\n",
    "Множитель $a_k$ не вполне каноничен, но его введение может ускорять сходимость.\n",
    "\n",
    "Решение задачи оптимизации $f(w) \\rightarrow \\min\\limits_w$ сводится к нахождению нулей градиента $\\nabla f(w) = 0$. Получаем следующий итеративный процесс:\n",
    "\n",
    "$$w_{k + 1} = w_k - \\alpha_k\\left(\\nabla^2 f(x_k)\\right)^{-1} \\cdot \\nabla f(x_k) =: w_k - \\alpha_k d_k$$\n",
    "\n",
    "Иными словами, основная идея метода Ньютона -- на шаге $k$ выбрать направление спуска $d_k$ с помощью градиента и гессиана, определить длину шага $\\alpha_k$ по направлению $d_k$, и повторять сей процесс до сходимости (в выпуклой задаче можно считать, что это 20 итераций).\n",
    "\n",
    "В методе Ньютона каждое следующее направление оптимизации выбирается как \n",
    "\n",
    "$$d_{k+1} = -(\\nabla^2 f(x_k))^{-1} \\cdot \\nabla f(x_k)$$\n",
    "\n",
    "но, вот беда, операция поиска обратной матрицы очень дорогая и не устойчивая, поэтому будем искать $d_{k+1}$ как решение системы уравнений\n",
    "\n",
    "$$\\nabla^2 f(x_k) d_{k+1} = -\\nabla f(x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом вам нужно будет реализовать метод Ньютона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Замечание*: Если вы хорошо реализовали вычисление градиента и гессиана, то в функции `newton` вам не понадобилось отдельно обрабатывать разреженные матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.linesearch import line_search_armijo\n",
    "\n",
    "def newton(func, grad, hess, w0, solver, max_iter=20):\n",
    "    '''\n",
    "        func: скалярная функция от вектора размера shape(w0)\n",
    "        grad: функция вычисляющая градиент функции func\n",
    "        hess: функция вычисляющая гессиан  функции func\n",
    "        \n",
    "        w0: вектор, первая точка в процессе оптимизации \n",
    "        solver: функция от двух аргументов A, b находит решение системы Ax=b\n",
    "        max_iter: количество итераций метода\n",
    "    '''\n",
    "    \n",
    "    x, fvals, ngrads = x0.copy().astype(np.float), [], []\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        fvalx, gradx, hessx = func(x), grad(x), hess(x)\n",
    "        d = # Решите систему hess * x = grad \n",
    "        alpha = # Поиск шага по направлению d, с помощью быстрой одномерной оптимизации, используйте line_search_armijo\n",
    "        x = # Шаг метода по направлению d с коэффициентом alpha\n",
    "        fvals.append(fvalx)\n",
    "        ngrads.append(np.linalg.norm(gradx))\n",
    "\n",
    "    return x, fvals, ngrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример запуска\n",
    "\n",
    "func = lambda w: logistic(w, X, y)\n",
    "grad = lambda w: logistic_grad(w, X, y)\n",
    "hess = lambda w: logistic_hess(w, X, y)\n",
    "gauss_  = lambda A, b: spla.spsolve(A, b) if sps.issparse(A) else sla.solve(A, b)\n",
    "lgmres_  = lambda A, b: spla.lgmres(A, b, tol=1e-2)[0]\n",
    "cg_  = lambda A, b: spla.cg(A, b, tol=1e-2)[0]\n",
    "\n",
    "%time w_opt, fvals, ngrads = newton(func, grad, hess, w, cg_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам необходимо в зависимости от метода решения системы уравнений (гаусс, CG, GMRES) исследовать:\n",
    "- зависимость времени работы метода оптимизации от выбора метода решения системы;\n",
    "- скорость сходимости метода оптимизации в зависимости от точности решения системы уравнений (параметр `tol`);\n",
    "- какую часть времени метод тратит на решение системы уравнений и какую на вычисление гессиана и градиента.\n",
    "\n",
    "Эксперименты нужно провести на нескольких наборах данных. Рассмотрите следующие три ситуации: \n",
    "- малое число признаков d < 100\n",
    "- среднее число признаков d ~ 500 \n",
    "- большое число признаков d ~ 1000\n",
    "\n",
    "Мы будем поощрять любые дополнительные исследования, например, если вы попытаетесь проверить статистическую значимость результатов экспериментов: ведь если какой-нибудь метод показал себя лучше в одном-единственном эксперименте, то это ещё ничего не значит.\n",
    "\n",
    "**Рекомендация**: можете рассмотреть три набора данных, которые можно скачать с сайта [LIBSVM1](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html): **a9a**, **w8a** (много объектов, сравнительно немного признаков) и **colon-cancer** (в нём достаточно мало объектов, но зато гораздо больше признаков).\n",
    "\n",
    "Любой набор данных с сайта LIBSVM представляет из себя текстовый файл в формате svmlight. Чтобы считать такой текстовый файл, можно использовать функцию [load_svmlight_file](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html) из модуля sklearn.datasets. Эта функция всегда возвращает матрицу X типа sp.sparse.csr_matrix (разреженная марица). В датасете **colon-cancer** матрица X не будет разреженной, поэтому сразу же после вызова функции load_svmlight_file следует привести X к типу np.ndarray. Это можно сделать с помощью команды X = X.toarray()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 6.4 (0 балов, весь код написан за Вас, но очень красивые картинки)** \n",
    "\n",
    "Давайте визуализируем наш метод, а то хочется глазами посмотреть. Просто запустите код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def expand(X):\n",
    "    X_ = np.zeros((X.shape[0], 6))\n",
    "    X_[:,0:2] = X\n",
    "    X_[:,2:4] = X**2\n",
    "    X_[:,4] = X[:,0] * X[:,1]\n",
    "    X_[:,5] = 1;\n",
    "    return X_\n",
    "\n",
    "def visualize(X, y, w, loss, n_iter, h=0.01):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    plt.clf()\n",
    "    Z = classify(expand(np.c_[xx.ravel(), yy.ravel()]), w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.contourf(xx, yy, Z, cmap='rainbow', alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(loss)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "def viz_opt(func, gradf, hessf, X, y, n_iter=10):\n",
    "    a = None\n",
    "    loss1 = np.zeros(n_iter)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    ind = np.arange(X.shape[0])\n",
    "    \n",
    "    w, d = np.zeros(X.shape[1]), np.zeros(X.shape[1])\n",
    "    \n",
    "    for i in range(n_iter):        \n",
    "        loss1[i] += func(w)\n",
    "        visualize(X, y, w, loss1, n_iter)\n",
    "        \n",
    "        fvalx, gradx, hessx = func(w), grad(w), hess(w)\n",
    "        d = cg(hessx, -gradx)[0]\n",
    "        alpha = line_search_armijo(func, w, d, gradx, fvalx)[0]\n",
    "        w += alpha*d\n",
    "        \n",
    "    visualize(X, y, w, loss1, n_iter)\n",
    "    \n",
    "    q = plt.clf()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\n",
    "X += np.random.random(X.shape)\n",
    "\n",
    "datasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "def classify(X, w):\n",
    "    return np.sign(1.0 / (1.0 + np.exp(-X.dot(w))) - 0.5)\n",
    "\n",
    "func = lambda w: logistic(w, X, y)\n",
    "grad = lambda w: logistic_grad(w, X, y)\n",
    "hess = lambda w: logistic_hess(w, X, y)\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(func, grad, hess, X, y) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
