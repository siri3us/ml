{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import scipy.sparse.linalg as spla\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2. SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Волшебные невязки (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируйте командой `scipy.linalg.hilbert` матрицу Гильберта размера $500\\times500$. Назовём эту матрицу $A$. \n",
    "\n",
    "Возьмите вектор $z = (0,0,\\ldots,0,1)\\in\\mathbb{R}^{500}$. Вычислите $b = Az$ и решите систему уравнений $Ax = b$ вашим любимым способом.\n",
    "\n",
    "Теперь исказите вектор $b$ небольшой (по модулю не большей $0.0001$ по каждой координате) случайной ошибкой и для полученного вектора $b'$ решите систему $Ax = b'$ тем же самым способом.\n",
    "\n",
    "Сравните невязки $||A\\hat{x} - b||_2$ и $||A\\hat{x}' - b'||_2$, где $\\hat{x}$ и $\\hat{x}'$ - полученные вами решения. Попробуйте объяснить эффект.\n",
    "\n",
    "**Важно!** Баллы будут ставиться не за реализацию, а за объяснения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Неожиданно теоретическая задача (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для двух заданных матриц $A$ и $B$ одного размера найдите ортогональную матрицу $Q$, для которой норма Фробениуса разности $||QA - B||_F$ минимальна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Ваше решение*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Сжатие информации с помощью SVD (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите выложенную на странице курса фотографию вашего любимого куратора Сони.\n",
    "\n",
    "Поскольку фотография чёрно-белая, её можно проинтерпетировать как матрицу, элементы которой - это насыщенность серого цвета каждого из пикселей. Постройте сингулярное разложение этой матрицы (не нужно центрировать данные!). \n",
    "\n",
    "Визуализуйте первую главную компоненту. Ожидали ли вы увидеть именно это? Почему?\n",
    "\n",
    "Визуализуйте компонеты с первой по двадцатую, с первой по пятидесятую, с двадцатой по сотую, с двадцатой по последнюю. Сделайте выводы.\n",
    "\n",
    "Как вам кажется, сколько первых компонент нужно взять для достаточно хорошего восстановления исходного изображения? Во сколько раз меньше памяти потребуется для их хранения? Как изменится результат, если сначала вы центрируете данные?\n",
    "\n",
    "**Важное замечание.** Главные компоненты - это не скрытые признаки, а матрицы вида $u^{(i)}\\sigma_i(v^{(i)})^T$, где $u^{(i)}, v^{(i)}$ --- столбцы матриц $U$ и $V$ соответственно. В частности, сумма первых нескольких главных компонент --- это наилучшее приближение исходной матрицы матрицей данного ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "img = misc.imread(r'...\\Sonya_small.jpg', mode='L') # это матрица из интенсивностей серого цвета; её уже можно подвергать SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на Соню!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Снижение размерности с помощью SVD (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите со страницы курса файлы `messages_texts.txt`, `messages_features.txt` и `messages_vectorized.mtx`.\n",
    "\n",
    "**Внимание!** Не пытайтесь открыть файл `messages_vectorized.txt` в блокноте или, тем более, распечатать его в IPython ноутбуке. Сначала посмотрите на его объём.\n",
    "\n",
    "* `messages_texts.txt` содержит некоторое количество текстовых сообщений на английском языке. Сообщение номер k начинается с заголовка post_number_k.\n",
    "* `messages_features.txt` содержит список пар `(слово, номер соответствующего признака)` для всех слов, которые содержатся в теле сообщений, кроме самых употребительных из списка `stopwords.words(\"english\")`\n",
    "* `messages_vectorized.mtx` содержит разреженную матрицу, содержащая индексы [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) всех слов из messages_features в каждом сообщении.\n",
    "\n",
    "*Замечание* Не обязательно парсить файл `messages_texts.txt`; чтобы найти сообщение с нужным номером, вы можете просто воспользоваться поиском по файлу!\n",
    "\n",
    "В этом задании вы попробуете представить сообщения векторами небольшой размерности. Идея вот в чём. Изначально каждое сообщение у нас представлена огромным количеством признаков: tf-idf индексами всех слов. SVD позволяет значительную часть информации собрать в нескольких новых признаках; тем самым, сообщения будут представлены достаточно короткими векторами.\n",
    "\n",
    "Загрузите матрицу `messages_vectorized.mtx`. Поскольку она очень большая, не пытайтесь вычислять полное SVD; вместо этого воспользуйтесь функцией [scipy.linalg.svds](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.svds.html#scipy.sparse.linalg.svds), которая вычисляет $k$ старших сингулярных значений и векторов. Возьмите $k = 6$ и визуализуйте сообщения в пространстве первых трёх новых признаков. Какие геометричские особенности датасета становятся видны? Стоят ли за этим какие-то содержательные закономерности? Возможно, придётся запустить несколько раз, чтобы получилась хорошая картинка (а она действительно хорошая!).\n",
    "\n",
    "Какой методологический недочёт был допущен при создании матрицы `messages_vectorized.mat`? Как он повлиял на поведение SVD?\n",
    "\n",
    "Попытайтесь выяснить, какие из исходных признаков (то есть какие слова) вносят наибольший вклад в три новых признака.\n",
    "\n",
    "Сравните результаты с тем, что получилось бы, если воспользоваться [случайными гауссовскими проекциями](http://scikit-learn.org/stable/modules/random_projection.html). Они вызываются следующими заклинаниями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = # Your matrix\n",
    "transformer = random_projection.GaussianRandomProjection(n_components='''how many?''')\n",
    "X_new = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Центрировать данные** можно с помощью функции `sklearn.preprocessing.scale`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "scale(X, with_mean = True, with_std=False, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S.* Если оперативная память вам позволит (кажется, 2Гб должно хватить), можете всё-таки попробовать сделать полное SVD и сравнить полученную картинку с той, что выдаёт `svds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Низкоранговые приближения своими руками (до 6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой задаче вам предстоит поразмышлять о низкоранговых приближениях и об их месте в мироздании.\n",
    "\n",
    "Везде ниже $||\\cdot||_2$ --- это операторная $l_2$-норма.\n",
    "\n",
    "Зафиксируем некоторое $\\varepsilon > 0$. Найти низкоранговое приближение матрицы $A$ с точностью $\\varepsilon$ --- значит найти такую матрицу $Q$ с $k = k(\\varepsilon)$ ортонормированными столбцами, для которой\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\phantom{AAAAAAAAAAAAAAAAA} &\n",
    "||A - QQ^TA||_2 < \\varepsilon & \\phantom{AAAAAAAAAAAAaa}(1)\\end{matrix}$$\n",
    "В этом случае линейная оболочка столбцов матрицы $Q$ является в каком-то смысле приближённым образом $A$.\n",
    "\n",
    "Если ранг $k$ нам из каких-то соображений известен, то можно воспользоваться, например, сингулярным разложением. А если нет? Есть несколько способов этот ранг найти; мы предлагаем вам поэкспериментировать с одним из них.\n",
    "\n",
    "Идея проста: если мы возьмём образы достаточно большого количества случайных векторов (обычно их берут из стандартного нормального распределения), с хорошей вероятностью их линейная оболочка будет приближать образ с нужной нам точностью. Главный вопрос в том, когда имеет смысл остановиться. И здесь помогает следующая\n",
    "\n",
    "**Теорема.** Пусть $B\\in\\mathrm{Mat}_{m\\times n}$ --- некоторая матрица, $\\omega_1,\\ldots,\\omega_r$ --- случайные векторы, независимо выбранные из стандартного нормального распределения. Тогда\n",
    "$$P\\left\\{||B||\\leqslant 10\\sqrt{\\frac{2}{\\pi}}\\max_i{||B\\omega_i||}\\right\\} \\geqslant 1 - \\min(m,n)\\cdot10^{-r}$$\n",
    "\n",
    "\n",
    "Следующий алгоритм позволяет для матрицы $A$ размера $m\\times n$ найти ортогональную матрицу $Q$, такую что\n",
    "\n",
    "$$P\\left\\{||(E - QQ^T)A||_2\\leqslant\\varepsilon\\right\\} \\geqslant 1 - \\min\\{m,n\\}\\cdot10^{-r}$$\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align*}\n",
    "&\\textbf{Алгоритм}\\\\\n",
    "&\\mbox{Draw }\\omega^{(1)},\\ldots,\\omega^{(r)}\\sim\\mathcal{N}(0, E)\\\\\n",
    "&\\mbox{Compute }y^{(i)} = A\\omega^{(i)}\\\\\n",
    "&Q^{(0)} = []\\mbox{ (an empty matrix $m\\times 0$)}\\\\\n",
    "&j = 0\\\\\n",
    "&\\mbox{while }\\max\\left\\{|y^{(j+1)}|,\\ldots, |y^{(j+r)}|\\right\\} > \\frac{\\varepsilon}{10\\sqrt{2/\\pi}}:\\\\\n",
    "&\\qquad j = j + 1\\\\\n",
    "&\\qquad y^{(j)} = \\left(E - Q^{(j-1)}(Q^{(j-1)})^T\\right)y^{(j)}\\\\\n",
    "&\\qquad q^{(j)} = \\frac{y^{(j)}}{|y^{(j)}|}\\\\\n",
    "&\\qquad Q^{(j)} = \\left[Q^{(j-1)}\\, q^{(j)}\\right]\\mbox{ (add new column)}\\\\\n",
    "&\\qquad \\mbox{Draw }\\omega^{(j + r)}\\sim\\mathcal{N}(0, E)\\\\\n",
    "&\\qquad y^{(j + r)} = \\left(E - Q^{(j)}(Q^{(j)})^T\\right)A\\omega^{(j + r)}\\\\\n",
    "&\\qquad \\mbox{for }i = (j + 1), (j + 2), . . . , (j + r − 1):\\\\\n",
    "&\\qquad\\qquad y^{(i)} = y^{(i)} - (q^{(j)}, y^{(i)})q^{(j)}\\\\\n",
    "&\\mbox{return } Q^{(j)} \n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**Основное задание.** Напишите функцию `find_approximate(A, eps)` (у неё могут быть и другие аргументы, если вам это кажется необходимым), находящую для данной матрицы $A$ и уровня точности $\\varepsilon$ матрицу $Q$, удовлетворяющую условию (1). Поэкспериментируйте с матрицами разного размера. Получается ли ошибка $||A - QQ^TA||_2$ достаточно малой?\n",
    "\n",
    "**Дополнительные вопросы:**\n",
    "\n",
    "1. Каков вообще (геометрический? линейно алгебраический?) смысл неравенства $||A - QQ^TA||_2 < \\varepsilon$? Что мы имеем в виду говоря, что линейная оболочка столбцов матрицы $Q$ является приближённым образом $A$? Кратко (но убедительно:)) объясните, почему предложенный алгоритм действительно делает свою работу.\n",
    "\n",
    "2. Как построить приближённый SVD, если у нас уже имеется матрица $Q$, удовлетворяющая условию (1)? Найдите этим способом сингулярное разложение матрицы Гильберта и сравните его с вычисленным с помощью библиотечной функции `scipy.linalg.svd`. Удаётся ли вашей функции обогнать по времени библиотечную?\n",
    "\n",
    "3. Зачастую при поиске низкорангового приближения фиксированного ранга $k$ для матрицы $A$ работают даже не с ней. а с матрицей $(AA^T)^qA$, где $q$ --- небольшое натуральное число (скажем, $2$ или $3$). Зачем это нужно? В каких случаях это оправдано?\n",
    "\n",
    "4. У функции `make_regression` (см. ниже) есть любопытный параметр `effective_rank` (эффективный ранг). Попробуйте разобраться, что это такое. Можете попробовать дать его определение.\n",
    "\n",
    "За основное задание можно получить не более 3 баллов; остальные 3 вы сможете набрать, ответив на дополнительные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В помощь хозяйке:**\n",
    "\n",
    "Сгенерировать матрицу $X$ размера $m\\times n$ с эффективным рангом $k$ можно с помощью команды\n",
    "\n",
    "`X, y = make_regression(n_samples=m, n_features=n, n_informative=n, n_targets=1, bias=0.0, \\\n",
    "                       effective_rank=k, tail_strength=..., noise=0.0, shuffle=True, coef=False, random_state=None)`\n",
    "\n",
    "Чтобы она подключилась, введите `from sklearn.datasets import make_regression`\n",
    "\n",
    "Можете поэкспериментировать со значением `tail_strength`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Blessing of dimensionality (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это в каком-то смысле продолжение задания 3; вы попробуете сжать временной ряд с помощью тензорных разложений (то есть сделать такие же картинки, как вам показывали на семинаре).\n",
    "\n",
    "Загрузите временной ряд (если интересно, он взят [отсюда](https://www.quandl.com/data/BOE/XUDLNKG-Effective-Exchange-Rate-Index-Norwegian-Krone-1990-Average-100)) из файла `BOE-XUDLNKG.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv(r'...\\BOE-XUDLNKG.csv')['Value'].as_matrix()\n",
    "x = x[:10332]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сожмите его несколькими способами:\n",
    "- с помощью SVD (трансформировав в матрицу, близкую к квадратной), \n",
    "- с помощью HOSVD (трансформировав в тензор валентности 3, по возможности близкий к кубическому) \n",
    "- с помощью тензорного поезда.\n",
    "\n",
    "Постарайтесь не только минимизировать ранги, но и добиться, чтобы относительная ошибка --- то есть $\\frac{||x - x'||}{||x||}$ --- в каждом случае была не больше 0,05.\n",
    "\n",
    "Нарисуйте восстановленные из сжатых тензоров ряды. Во сколько раз в каждом из способов удаётся уменьшить объём хранимых данных?\n",
    "\n",
    "*Замечание* В этом задании не надо пользоваться никакими специальными библиотеками, только стандартными функциями (например, `np.tensordot`, `np.transpose`, `reshape`, `sla.svd`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
