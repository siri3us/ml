{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Sum}{\\sum\\limits}$\n",
    "$\\newcommand{\\Prod}{\\prod\\limits}$\n",
    "$\\newcommand{\\Max}{\\max\\limits}$\n",
    "$\\newcommand{\\Min}{\\min\\limits}$\n",
    "$\\newcommand{\\Int}{\\int\\limits}$\n",
    "$\\newcommand{\\Exp}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{\\mathbb{V}}$\n",
    "$\\newcommand{\\Energy}{\\mathcal{E}}$\n",
    "$\\newcommand{\\Prob}{\\mathcal{P}}$\n",
    "$\\newcommand{\\N}{\\mathcal{N}}$\n",
    "\n",
    "$\\newcommand{\\LogLike}{\\mathcal{L}}$\n",
    "$\\newcommand{\\Like}{\\ell}$\n",
    "\n",
    "$\\newcommand{\\bolda}{\\boldsymbol{a}}$\n",
    "$\\newcommand{\\boldA}{\\boldsymbol{A}}$\n",
    "$\\newcommand{\\ba}{\\bolda}$\n",
    "$\\newcommand{\\bA}{\\boldA}$\n",
    "\n",
    "$\\newcommand{\\boldb}{\\boldsymbol{b}}$\n",
    "$\\newcommand{\\boldB}{\\boldsymbol{B}}$\n",
    "$\\newcommand{\\bb}{\\boldb}$\n",
    "$\\newcommand{\\bB}{\\boldB}$\n",
    "\n",
    "$\\newcommand{\\boldd}{\\boldsymbol{d}}$\n",
    "$\\newcommand{\\boldD}{\\boldsymbol{D}}$\n",
    "$\\newcommand{\\bd}{\\boldd}$\n",
    "$\\newcommand{\\bD}{\\boldED}$\n",
    "$\\newcommand{\\bolde}{\\boldsymbol{e}}$\n",
    "$\\newcommand{\\boldE}{\\boldsymbol{E}}$\n",
    "$\\newcommand{\\be}{\\bolde}$\n",
    "$\\newcommand{\\bE}{\\boldE}$\n",
    "$\\newcommand{\\boldf}{\\boldsymbol{f}}$\n",
    "$\\newcommand{\\boldF}{\\boldsymbol{F}}$\n",
    "$\\newcommand{\\bf}{\\boldf}$\n",
    "$\\newcommand{\\bF}{\\boldF}$\n",
    "$\\newcommand{\\bolds}{\\boldsymbol{s}}$\n",
    "$\\newcommand{\\boldS}{\\boldsymbol{S}}$\n",
    "$\\newcommand{\\bs}{\\boldsymbol{\\bolds}}$\n",
    "$\\newcommand{\\bS}{\\boldsymbol{\\boldS}}$\n",
    "$\\newcommand{\\boldt}{\\boldsymbol{s}}$\n",
    "$\\newcommand{\\boldT}{\\boldsymbol{S}}$\n",
    "$\\newcommand{\\bt}{\\boldsymbol{\\boldt}}$\n",
    "$\\newcommand{\\bT}{\\boldsymbol{\\boldT}}$\n",
    "\n",
    "$\\newcommand{\\boldu}{\\boldsymbol{u}}$\n",
    "$\\newcommand{\\boldU}{\\boldsymbol{U}}$\n",
    "$\\newcommand{\\bu}{\\boldu}$\n",
    "$\\newcommand{\\bU}{\\boldU}$\n",
    "$\\newcommand{\\boldv}{\\boldsymbol{v}}$\n",
    "$\\newcommand{\\boldV}{\\boldsymbol{V}}$\n",
    "$\\newcommand{\\bv}{\\boldv}$\n",
    "$\\newcommand{\\bV}{\\boldV}$\n",
    "$\\newcommand{\\boldx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\boldX}{\\boldsymbol{X}}$\n",
    "$\\newcommand{\\bx}{\\boldx}$\n",
    "$\\newcommand{\\bX}{\\boldX}$\n",
    "$\\newcommand{\\boldY}{\\boldsymbol{Y}}$\n",
    "$\\newcommand{\\boldy}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\bY}{\\boldY}$\n",
    "$\\newcommand{\\by}{\\boldy}$\n",
    "$\\newcommand{\\boldZ}{\\boldsymbol{Z}}$\n",
    "$\\newcommand{\\boldz}{\\boldsymbol{z}}$\n",
    "$\\newcommand{\\bZ}{\\boldZ}$\n",
    "$\\newcommand{\\bz}{\\boldz}$\n",
    "\n",
    "$\\newcommand{\\boldTheta}{\\boldsymbol{\\Theta}}$\n",
    "$\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\bTheta}{\\boldTheta}$\n",
    "$\\newcommand{\\btheta}{\\boldtheta}$\n",
    "\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\diag}{\\text{diag}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Содержание\n",
    "* [Теория](#theory)\n",
    "    * [PCA (Principal Component Analysis)](#pca)\n",
    "    * [Probabilistic PCA](#probabilistic)\n",
    "* [Методы нахождения собственных векторов](#methods)\n",
    "* [Применение](#application)\n",
    "* []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим выбрать плоскость такую, что в перпендикулярном ей направлении дисперсия значений проекций точек максимальна. Пусть $u$ --- перпендикуляр к плоскости. Пусть $F(\\boldu;\\boldX)$ --- среднее значение квадрата проекций точек на направление $\\boldu$:\n",
    "\\begin{gather}\n",
    "F(\\boldu;\\boldX) = \\frac{1}{N}\\Sum_{i=1}^N \\left\\|\\frac{(\\boldx_i - \\bar{\\boldx}, \\boldu)}{(\\boldu, \\boldu)} \\boldu\\right\\|^2_2 = \n",
    "\\frac{1}{N}\\frac{\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}, \\boldu)^2}{(\\boldu, \\boldu)} \\rightarrow \\max_{\\boldu \\in \\mathbb{R}^N} \n",
    "\\end{gather}\n",
    "Пусть вектор $\\boldu$ нормирован. Тогда приходим к задаче оптимизации\n",
    "\\begin{gather}\n",
    "F(\\boldu;\\boldX)  = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}, \\boldu)^2 \\rightarrow \\max_{\\|\\boldu\\|^2_2 = 1} \n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лагранжиан имеет вид\n",
    "\\begin{gather}\n",
    "L(\\boldu,\\lambda) = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}, \\boldu)^2 - \\frac{\\lambda}{2} (\\|\\boldu\\|_2^2 - 1)\n",
    "\\end{gather}\n",
    "Дифференцируя $L(\\boldu, \\lambda)$ по $\\boldu$, получаем\n",
    "\\begin{gather}\n",
    "\\frac{\\partial L(\\boldu, \\lambda)}{\\partial \\lambda} = \\frac{2}{N}(\\boldx_i - \\bar{\\boldx}, \\boldu) \\cdot (\\boldx_i - \\bar{\\boldx}) - 2\\lambda \\boldu \\\\\n",
    "\\frac{\\partial L(\\boldu, \\lambda)}{\\partial \\lambda}  = 0 \\Leftrightarrow \\lambda \\boldu = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}) \\cdot (\\boldx_i - \\bar{\\boldx}, \\boldu) = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}) (\\boldx_i - \\bar{\\boldx})^T \\cdot \\boldu\n",
    "\\end{gather}\n",
    "Здесь появляется матрица ковариации $\\boldS$:\n",
    "\\begin{gather}\n",
    "\\boldS = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}) (\\boldx_i - \\bar{\\boldx})^T\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, искомое направление задается собственный вектором матрицы ковариаций $\\boldS$. При этом среднее значение квадратов длин проекций равно собственному значению.\n",
    "\\begin{gather}\n",
    "F(\\boldu;\\boldX) = \\frac{1}{N}\\Sum_{i=1}^N (\\boldx_i - \\bar{\\boldx}, \\boldu)^2 = \\frac{1}{N}\\Sum_{i=1}^N \\boldu^T (\\boldx_i - \\bar{\\boldx})(\\boldx_i - \\bar{\\boldx})^T \\boldu = \\boldu^T \\boldS \\boldu = \\lambda \\|\\boldu\\|_2^2 = \\lambda.\n",
    "\\end{gather}\n",
    "Выбрав собственный вектор $\\boldu_1$, которому соответствует максимальное собственное значение $\\lambda_1$ матрицы ковариаций $\\boldS$, получим искомое направление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итеративная процедура нахождения направлений максимальной дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $\\lambda_1, \\lambda_2, \\dots, \\lambda_D$ --- собственные векторы матрицы $S$, в порядке убывания, а $\\boldu_1, \\boldu_2, \\dots, \\boldu_D$ --- соответствующие им собственные значения. Докажем, что описанная выше процедура итеративного нахождения направлений наибольшей дисперсии приведт в конечном счете к векторам  $\\boldu_1, \\boldu_2, \\dots, \\boldu_D$.\n",
    "\n",
    "На первом этапе $\\boldS_1 = \\boldS$ получили вектор направления $\\boldu_1$ с собственным значением $\\lambda_1$. Теперь найдем следующий вектор. Выясним, как будет выглядеть матрица ковариаций на второй итерации $\\boldS_2$. \n",
    "\n",
    "Вычтем из каждого вектора $\\boldx_i$ его проекцию на $\\boldu_1$ и для полученных векторов найдем матрицу ковариации.\n",
    "\\begin{gather}\n",
    "\\boldS_2 = \\frac{1}{N}\\Sum_{i=1}^N\\left(\\boldx_i - (\\boldx_i, \\boldu_1)\\boldu_1 - (\\bar{\\boldx} - (\\bar{\\boldx}, \\boldu_1)\\boldu_1)\\right)\\left(\\boldx_i - (\\boldx_i, \\boldu_1)\\boldu_1 - (\\bar{\\boldx} - (\\bar{\\boldx}, \\boldu_1)\\boldu_1)\\right)^T = \\\\ = \\frac{1}{N}\\Sum_{i=1}^N\\left(\\boldx_i - \\bar{\\boldx} - (\\boldx_i - \\bar{\\boldx}, \\boldu_1)\\boldu_1 \\right) \\left(\\boldx_i - \\bar{\\boldx} - (\\boldx_i - \\bar{\\boldx}, \\boldu_1)\\boldu_1 \\right)^T = \\\\ =\\frac{1}{N} \\Sum_{i=1}^N \\left\\{(\\boldx_i - \\bar{\\boldx})(\\boldx_i - \\bar{\\boldx})^T - 2 (\\boldx_i - \\bar{\\boldx}) \\cdot (\\boldx_i - \\bar{\\boldx}, \\boldu_1)  \\cdot \\boldu_1^T + \\boldu_1 \\boldu_1^T (\\boldx_i - \\bar{\\boldx}, \\boldu_1)^2 \\right\\} = \\\\ =  \\frac{1}{N}\\Sum_{i=1}^N \\left\\{(\\boldx_i - \\bar{\\boldx})(\\boldx_i - \\bar{\\boldx})^T - \\boldu_1\\boldu_1^T (\\boldx_i - \\bar{\\boldx})(\\boldx_i - \\bar{\\boldx})^T \\right\\} = \\boldS_1(\\boldE - \\boldu_1\\boldu_1^T) = (\\boldE - \\boldu_1\\boldu_1^T) \\boldS_1.\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что собственные значения $\\lambda_2 \\ge \\dots \\ge \\lambda_D \\ge 0$ матрицы $\\boldS$ являются также собственными значениями матрицы ковариаций $\\boldS_2$. Действительно,\n",
    "$$\n",
    "\\boldS_2 \\boldu_2 = (\\boldE - \\boldu_1\\boldu_1^T) \\boldS_1 \\boldu_2 = \\lambda_2(\\boldE - \\boldu_1\\boldu_1^T) \\boldu_2 =  \\lambda_2\\boldu_2 - \\boldu_1\\boldu_1^T \\boldu_2 = \\lambda_2\\boldu_2.\n",
    "$$\n",
    "Здесь учтено, что векторы $\\boldu_1, \\dots, \\boldu_D$ ортогональны.\n",
    "\n",
    "Заметим что $\\lambda_2, \\dots, \\lambda_D$ --- это первые $D-1$ собственных значений. Последнее дополнительное собственное значение $\\boldS_2$ равно нулю. Ему соответствует собственный вектор $\\boldu_1$:\n",
    "$$\n",
    "S_2\\boldu_1 = (1-\\boldu_1\\boldu_1^T)\\boldS_1 \\boldu_1 = \\lambda_1 (1-\\boldu_1\\boldu_1^T) \\boldu_1 = \\lambda_1(\\boldu_1 - \\boldu_1) = 0.\n",
    "$$\n",
    "\n",
    "Таким образом, на втором этапе итеративной процедуры будет выбрано направление $\\boldu_2$, так как ему соответствует максимальная дисперсия $\\lambda_2$. В общем случае на $i$-ом этапе будет выбран вектор $\\boldu_i$ с дисперсией $\\lambda_i$. Что и требовалось доказать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смылс доказанного выше сводится к тому, что в итеративной процедуре как таковой нет смысла: достаточно найти собственные вектора и собственные значения для исходной матрицы ковариаций.\n",
    "\n",
    "#### Смысл проекции\n",
    "* Собственные значения --- дисперсия точек вдоль направления соответствующего собственного вектора\n",
    "* Если $M > 0$ собственных значений равны нулю или близки к нему, то данные сосредоточены в подпространстве размерности $D - M$\n",
    "* Достаточно выбрать только первые $K$ собственных векторов, соответствующих максимальным собственным значениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сингулярное разложение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Пусть дана матрица $X \\in \\RR^{m\\times n}$. В задачах машинного обучения это матрица объекты-признаки. В данном случае положим, что $m$ --- число признаков, а $n$ --- число объектов, т.е. признаки записаны в столбцы, а не строки.  \n",
    "\n",
    "Требуется спроектировать векторы $\\boldx_1, \\dots, \\boldx_n$ из пространства размерности $m$ на некоторое его $k$-мерное подпространство, так чтобы сумма квадратов длин проекций была максимальна. \n",
    "\n",
    "Для всякой матрицы $X \\in \\RR^{m\\times n}$ существует сингулярное разложение:\n",
    "$$\n",
    "X = U \\Sigma V^T,\n",
    "$$\n",
    "где $U \\in \\RR^{m\\times m}$, $V \\in \\RR^{n\\times n}$, $\\Sigma = \\text{diag}^*_{m,n}[\\sigma_1, \\dots, \\sigma_{\\min(m,n)}] \\in \\RR^{m\\times n}$. Матрицы $U$ и $V$ --- ортогональные матрицы, матрица $\\Sigma$ --- псевдоортогональная. Пусть для определенности $m \\le n$ (такой случай наиболее характерен на парктике).\n",
    "\n",
    "* Матрица $U = [\\boldu_1, \\dots, \\boldu_m]$ --- матрица собственных векторов отображения $XX^T$:\n",
    "\\begin{gather}\n",
    "XX^T = U\\Sigma V^T \\cdot V \\Sigma^T U^T = U\\Sigma \\Sigma^T U^T = U ~\\underbrace{\\diag[\\sigma_1^2, \\dots, \\sigma_m^2]}_{m\\times m}U^T\\\\\n",
    "XX^T U = U \\diag[\\sigma_1^2, \\dots, \\sigma_m^2] U^T U = [\\sigma_1^2 \\boldu_1, \\dots, \\sigma_m^2 \\boldu_m].\n",
    "\\end{gather}\n",
    "* Матрица $V = [\\boldv_1, \\dots, \\boldv_n]$ --- матрица собственных векторов отображения $X^TX$:\n",
    "\\begin{gather}\n",
    "X^TX = V\\Sigma^T U^T \\cdot U \\Sigma V^T = U\\Sigma \\Sigma^T U^T = V ~\\underbrace{\\diag[\\sigma_1^2, \\dots, \\sigma_m^2, 0, \\dots, 0]}_{n\\times n}V^T\\\\\n",
    "X^TXV = V ~\\diag[\\sigma_1^2, \\dots, \\sigma_m^2, 0, \\dots, 0]V^T V = [\\sigma_1^2\\boldv_1, \\dots,\\sigma_m^2 \\boldv_m, 0, \\dots, 0]\n",
    "\\end{gather}\n",
    "\n",
    "Определив матрицы $U$ и $\\Sigma$, можем затем найти матрицу $V$.\n",
    "\\begin{gather}\n",
    "XV = U\\Sigma V^T V = U \\Sigma = [\\sigma_1\\boldu_1, \\dots, \\sigma_m\\boldu_m, 0, \\dots, 0]\n",
    "\\end{gather}\n",
    "Заметим, что если все $m$ собственных значений $\\sigma_1^2$ матрицы $XX^T$ (или $X^TX$) отличны от нуля, то\n",
    "выше будут найдены все собственные векторы. В противном случае останется вопрос о нахождении собственных векторов матрицы $XX^T$ для нулевых собственных значений. Но такие векторы нам в дальнейшем не потребуются, поэтому этот вопрос мы опустим.\n",
    "\n",
    "Аналогично, определив матрицы $V$ и $\\Sigma$, можем затем найти матрицу $U$.\n",
    "\\begin{gather}\n",
    "X^T U = V\\Sigma^T U^T U = V \\Sigma^T = [\\sigma_1\\boldv_1, \\dots, \\sigma_m\\boldv_m]\n",
    "\\end{gather}\n",
    "В данном случае также как и ранее находим все собственные векторы матрицы $X^TX$ с ненулевыми собственными значениями.\n",
    "\n",
    "TODO\n",
    "\n",
    "Таким образом, для нахождения сингулярного разложения достаточно найти собственные векторы и собственные значения матрицы $XX^T$ или $X^TX$ (в зависимости от того, какая из задач проще). В нашем случае выгодно рассматривать матрицу $XX^T$. \n",
    "* Нахождение $m$ собственных векторов $U$ матрицы $XX^T \\in \\RR^{m \\times m}$ и соответствующих собственных значений $\\sigma_1 \\ge \\dots \\ge \\sigma_m \\ge 0$. Пусть число ненулевых собственных значений равно $r$: $\\sigma_1 \\ge \\dots \\ge \\sigma_r > \\sigma_{r+1} = \\dots \\sigma_m = 0$.\n",
    "* Нахождение $m - r$ собственных векторов матрицы $XX^T$ с нулевыми собственными значениями.\n",
    "* Нахождение первых $r$ собственных векторов $V$ матрицы $X^TX \\in \\RR^{n \\times n}$, соответствующих собственным значениям $\\sigma_1 \\ge \\dots \\ge \\sigma_m \\ge 0$:\n",
    "    $$\n",
    "    X^T U = [\\sigma_1\\boldv_1, \\dots, \\sigma_r\\boldv_r, 0, \\dots, 0].\n",
    "    $$\n",
    "* Нахождение оставшихся $n - r$ собственных значений матрицы $X^TX$ с нулевыми собственными значениями. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='methods'></a>\n",
    "# Методы нахождения собственных значений<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итерационная процедура нахождения собственных значений [[toc](#toc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\|A - X\\|_2^2 \\rightarrow \\min_{X \\in \\RR^{m \\times n}}$$.\n",
    "В такой постановке ответ очевиден --- это просто $X = A$. Поэтому наложим дополнительное ограничение $\\rank X = 1$. Пусть \n",
    "$A = U\\Sigma V^T$. Тогда $X = U X_1 V^T$ [TODO why].\n",
    "$$\n",
    "\\|A - X\\|_2^2 = \\|U(\\Sigma - X_1)V^T\\|_2^2 = \\|\\Sigma - X_1\\|_2^2.\n",
    "$$\n",
    "В таком случае $X_1$ --- это просто матрица с единственным ненулевым элементом, равным $\\sigma_1$. При этом\n",
    "$X = UX_1V^T = \\sigma_1 \\boldu_1 \\boldv_1^T$. \n",
    "\n",
    "Обозначим $X = \\boldx \\boldy^T$ и будем решать оптимизационную задачу\n",
    "$$\n",
    "\\frac{1}{2}\\Sum_{i,j}(a_{i,j} -x_i y_j)^2 \\rightarrow \\min.\n",
    "$$\n",
    "В таком случае \n",
    "\\begin{gather}\n",
    "x_i = \\frac{\\Sum_j a_{i,j}y_j}{\\Sum_j y_j^2}, \\qquad y_j = \\frac{\\Sum_i a_{i,j} x_i}{\\Sum_i x^2_i}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.linalg.linalg.norm>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_eigenvalue(A):\n",
    "    A = A.copy()\n",
    "    eig_vals = []\n",
    "    eig_vecs_left = []\n",
    "    eig_vecs_right = []\n",
    "    n_eig = 0\n",
    "    while 1:\n",
    "        x = np.random.rand(A.shape[0])\n",
    "        y = np.random.rand(A.shape[1])\n",
    "        Fs = [np.linalg.norm(A - x[:, None] * y[None, :], 'fro')]\n",
    "        delta = np.inf\n",
    "        n_iter = 0\n",
    "        while np.abs(delta) > 1e-12:\n",
    "            x = np.sum(A * y[None, :], axis=1) / np.sum(y**2)\n",
    "            y = np.sum(A * x[:, None], axis=0) / np.sum(x**2)\n",
    "            F = np.linalg.norm(A - x[:, None] * y[None, :], 'fro')\n",
    "            delta = Fs[-1] - F\n",
    "            Fs.append(F)\n",
    "            print('n_eig = {}, n_iter = {}: delta = {}.'.format(n_eig, n_iter, delta))\n",
    "            n_iter += 1\n",
    "        eig_val = np.sum(x**2) * np.sum(y**2)\n",
    "        if eig_val > 1e-10:\n",
    "            eig_vals.append(eig_val)\n",
    "            eig_vecs_left.append(x[:, None])\n",
    "            eig_vecs_right.append(y[:, None])\n",
    "            n_eig += 1\n",
    "            if n_eig == min(A.shape):\n",
    "                print('All eigenvalues was found.')\n",
    "                break\n",
    "            A -= x[:, None] * y[None, :]\n",
    "        else:\n",
    "            break\n",
    "    return eig_vals, eig_vecs_left, eig_vecs_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5488135   0.71518937  0.60276338  0.54488318  0.4236548 ]\n",
      " [ 0.64589411  0.43758721  0.891773    0.96366276  0.38344152]\n",
      " [ 0.79172504  0.52889492  0.56804456  0.92559664  0.07103606]]\n",
      "n_eig = 0, n_iter = 0: delta = 1.464341909284908.\n",
      "n_eig = 0, n_iter = 1: delta = 0.0048035466556891615.\n",
      "n_eig = 0, n_iter = 2: delta = 3.849789232068801e-06.\n",
      "n_eig = 0, n_iter = 3: delta = 3.0807141282451767e-09.\n",
      "n_eig = 0, n_iter = 4: delta = 2.4685808952540356e-12.\n",
      "n_eig = 0, n_iter = 5: delta = 1.9984014443252818e-15.\n",
      "n_eig = 1, n_iter = 0: delta = 0.935611644668977.\n",
      "n_eig = 1, n_iter = 1: delta = 0.007029025868303396.\n",
      "n_eig = 1, n_iter = 2: delta = 0.002573123895356355.\n",
      "n_eig = 1, n_iter = 3: delta = 0.0008794256265877709.\n",
      "n_eig = 1, n_iter = 4: delta = 0.0002934113825127649.\n",
      "n_eig = 1, n_iter = 5: delta = 9.710293016712557e-05.\n",
      "n_eig = 1, n_iter = 6: delta = 3.2049282698454196e-05.\n",
      "n_eig = 1, n_iter = 7: delta = 1.0568611270411754e-05.\n",
      "n_eig = 1, n_iter = 8: delta = 3.48409574524311e-06.\n",
      "n_eig = 1, n_iter = 9: delta = 1.148471484746505e-06.\n",
      "n_eig = 1, n_iter = 10: delta = 3.7856154883053605e-07.\n",
      "n_eig = 1, n_iter = 11: delta = 1.2478092970003019e-07.\n",
      "n_eig = 1, n_iter = 12: delta = 4.1129973427089794e-08.\n",
      "n_eig = 1, n_iter = 13: delta = 1.3557142020115265e-08.\n",
      "n_eig = 1, n_iter = 14: delta = 4.468663938883566e-09.\n",
      "n_eig = 1, n_iter = 15: delta = 1.472947264158364e-09.\n",
      "n_eig = 1, n_iter = 16: delta = 4.855083002297533e-10.\n",
      "n_eig = 1, n_iter = 17: delta = 1.6003176561696364e-10.\n",
      "n_eig = 1, n_iter = 18: delta = 5.2749193901746594e-11.\n",
      "n_eig = 1, n_iter = 19: delta = 1.7387036255200883e-11.\n",
      "n_eig = 1, n_iter = 20: delta = 5.731026764266289e-12.\n",
      "n_eig = 1, n_iter = 21: delta = 1.889044476399704e-12.\n",
      "n_eig = 1, n_iter = 22: delta = 6.22668583361019e-13.\n",
      "n_eig = 2, n_iter = 0: delta = 1.5622482466888834.\n",
      "n_eig = 2, n_iter = 1: delta = 2.043831666127275e-16.\n",
      "All eigenvalues was found.\n",
      "my eigenvalues: [5.9574567179112945, 0.16869241232893453, 0.096850147127321443]\n",
      "np eigenvalues: [ 5.95745672  0.16869241  0.09685015]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A = np.random.rand(3, 5)\n",
    "print(A)\n",
    "eig_vals, eig_vecs_left, eig_vecs_right = find_eigenvalue(A)\n",
    "print('my eigenvalues:', eig_vals)\n",
    "print('np eigenvalues:', np.linalg.eigvals(np.dot(A, A.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59536"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.44 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='application'></a>\n",
    "# Применение PCA<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='app_images'></a>\n",
    "## Применение к сжатию изображений<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='app_recover'></a>\n",
    "## Задача восстановления профиля крыла<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
