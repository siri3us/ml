\documentclass{report}
\usepackage[english, russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{paralist}
\usepackage{amsthm, amsmath, amsfonts, amssymb}
%\usepackage{bbold}
\usepackage{dsfont}
\usepackage{xspace}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage{multirow}
\usepackage{comment}

% Next goes different graphic related issues are discussed
\usepackage{xcolor, colortbl}
\usepackage{tikz}
\usepackage{xifthen}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{pgf}
\usepackage{pgfplots}


% Python style for highlighting
\usepackage{listings}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{8} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{8}  % for normal
\newcommand\pythonstyle{\lstset{
		language=Python,
		basicstyle=\ttm,
		otherkeywords={self},             % Add keywords here
		keywordstyle=\ttb\color{deepblue},
		emph={MyClass,__init__},          % Custom highlighting
		emphstyle=\ttb\color{deepred},    % Custom highlighting style
		stringstyle=\color{deepgreen},
		frame=tb,                         % Any extra options here
		showstringspaces=false            % 
	}}

	
% Python environment
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{#1}
}
{}
	

\definecolor{grey1}{RGB}{192,192,192}
\definecolor{lemonchiffon}{RGB}{255,250,205}
\definecolor{yellowgreen}{RGB}{154,205,50}
\definecolor{chocolate}{RGB}{210,105,30}
\definecolor{purple}{RGB}{128,0,128}

\usepackage{sectsty}
%\subsectionfont{\color{blue}}
\subsubsectionfont{\color{red}}  % sets colour of sections

%\usepackage{caption,subcaption}

\usepackage{bm}

\usepackage{indentfirst} % Отступ в начале chapter, section, subsection и т.д.

\newtheorem{theorem}{Теорема}
\numberwithin{theorem}{chapter}

\newtheorem{statement}{Утверждение}
\numberwithin{statement}{chapter}

\newtheorem{lemma}{Лемма}
\numberwithin{lemma}{chapter}

\newtheorem{consequence}{Следствие}

\theoremstyle{definition}
\newtheorem{task}{Задание}
\numberwithin{task}{chapter}

\theoremstyle{remark}
\newtheorem{example}{Пример}
\numberwithin{example}{chapter}

\newtheorem{assumption}{Предположение}

\theoremstyle{definition}
\newtheorem{definition}{Определение}
\numberwithin{definition}{chapter}

\theoremstyle{remark}
\newtheorem{note}{Замечание}
\theoremstyle{remark}
\newtheorem{lyrics}{Лирическое отступление}
\numberwithin{lyrics}{section}

%\renewenvironment{itemize}[1]{\begin{compactitem}#1}{\end{compactitem}}
%\renewenvironment{enumerate}[1]{\begin{compactenum}#1}{\end{compactenum}}
%\renewenvironment{description}[0]{\begin{compactdesc}}{\end{compactdesc}}

\newcommand{\param}[1]{\textbf{#1}}
\newcommand{\numberof}[1]{\#[\text{#1}]}
\newcommand{\underquestion}[1]{\textbf{#1 (Check!)}}
\newcommand{\TODO}[1]{\textbf{[TODO:#1]}}

% Комманды для комментариев
\newcommand{\ignore}[1]{}
\newcommand{\hidden}[1]{}
\newcommand{\translation}[1]{}

\DeclareMathAlphabet{\mathbbold}{U}{bbold}{m}{n}


\begin{document}
\title{Глубинное Обучение}
\author{Иванов Александр \\ ИППИ РАН}
\date{}
\maketitle
\tableofcontents
	
\include{commands}

\chapter{Полносвязные Сети}

\begin{itemize}
	\item $\NumOfSamples$ --- размер батча
	\item $\NumOfDims$ --- размерность вектора признаков
\end{itemize}

\section{Dense Layer}

\paragraph{Векторный вид}

\begin{gather}
\boldy^T = \boldx^T \boldW + \boldb^T \\
\boldy = \boldW^T \boldx + \boldb
\end{gather}

\begin{gather}
\partder{\Loss}{\boldx} = \partder{\boldy}{\boldx} \partder{\Loss}{\boldy} = \boldW \partder{\Loss}{\boldy}
\end{gather}

\begin{gather*}
\partder{\Loss}{\boldw_i} = \partder{\boldy}{\boldw_i} \partder{\Loss}{\boldy} = \partder{y_i}{\boldw_i} \partder{\Loss}{y_i} = \boldx \cdot \partder{\Loss}{y_i} \Rightarrow \partder{\Loss}{\boldW} = \boldx \left(\partder{\Loss}{\boldy}\right)^T
\end{gather*}

\begin{gather*}
\partder{\Loss}{\boldb} = \partder{\boldy}{\boldb} \partder{\Loss}{\boldy} = \partder{\Loss}{\boldy}.
\end{gather*}


\paragraph{Матричный вид}

\begin{gather*}
\boldY = \boldX \boldW + \boldB, 
\end{gather*}
где 
$$\boldB = [\underbrace{\boldb, \boldb, \dots, \boldb}_{\NumOfSamples}]^T \in \mathbb{R}^{\NumOfSamples \times \NumOfDims}.$$
В таком случае 
\begin{gather*}
\partder{\Loss}{\boldX} = [\partder{\Loss}{\boldx_1}, \dots, \partder{\Loss}{\boldx_\NumOfSamples}]^T = [\boldW  \partder{\Loss}{\boldy_1}, \dots,  \boldW \partder{\Loss}{\boldy_{\NumOfSamples}}]^T =  [\partder{\Loss}{\boldy_1}, \dots,  \partder{\Loss}{\boldy_{\NumOfSamples}}]^T \boldW^T =  \partder{\Loss}{\boldY} \boldW^T.
\end{gather*}

\begin{gather*}
\partder{\Loss}{\boldw_i} = \Sum_{k=1}^{\NumOfSamples} \partder{y_i^{(k)}}{\boldw_i} \partder{\Loss}{y_i^{(k)}} = \Sum_{k=1}^{\NumOfSamples} \boldx^{(k)} \partder{\Loss}{y_i^{(k)}} \Rightarrow \partder{\Loss}{\boldW} = \Sum_{k=1}^{\NumOfSamples} \boldx^{(k)} \left(\partder{\Loss}{\boldy^{(k)}}\right)^T = \boldX^T \partder{\Loss}{\boldY}.
\end{gather*}

\begin{gather*}
\partder{\Loss}{\boldb} = \Sum_{k=1}^{\NumOfSamples} \partder{\boldy^{(k)}}{\boldb} \partder{\Loss}{\boldy^{(k)}} = \Sum_{k=1}^{\NumOfSamples} \partder{\Loss}{\boldy^{(k)}}
\end{gather*}

\section{SoftMax}

\subsection{Forward propagation}

\subsection{Backward propagation}

\begin{gather*}
\partder{\Loss}{\boldx} = \partder{\boldy}{\boldx} \partder{\Loss}{\boldy}
\end{gather*}

\begin{gather*}
\partder{y_i}{x_j} = \partial \left(\frac{e^{x_j}}{\Sum_d e^{x_d}}\right) / \partial x_j = \frac{1}{\Sum_{d=1}^{\NumOfDims} e^{x_d}} \partder{e^{x_i}}{x_j} - \frac{e^{x_i}}{\left(\Sum_{d=1}^{\NumOfDims} e^{x_d}\right)^2} \partder{\left(\Sum_{d=1}^{\NumOfDims} e^{x_d}\right)}{x_j} = \frac{e^{x_i}\delta_{ij}}{\Sum_{d=1}^{\NumOfDims} e^{x_d}} - \frac{e^{x_i}}{\Sum_{d=1}^{\NumOfDims} e^{x_d}} \cdot \frac{e^{x_j}}{\Sum_{d=1}^{\NumOfDims} e^{x_d}} = y_j \delta_{ij} - y_i y_j.
\end{gather*}
Таким образом,
\begin{gather*}
\partder{\boldy}{\boldx} = \diag[y_1, y_2, \dots, y_\NumOfDims] - \boldy \boldy^T
\end{gather*}
\begin{gather*}
\partder{\Loss}{\boldx} = \partder{\boldy}{\boldx} \partder{\Loss}{\boldy} = \left(\diag[y_1, y_2, \dots, y_\NumOfDims] - \boldy \boldy^T\right) \partder{\Loss}{\boldy} = \diag[y_1, y_2, \dots, y_\NumOfDims] \partder{\Loss}{\boldy} - \boldy \cdot \left\langle \boldy, \partder{\Loss}{\boldy} \right\rangle = \\
= \boldy \otimes \partder{\Loss}{\boldy} - \boldy \left\langle \boldy, \partder{\Loss}{\boldy} \right\rangle
\end{gather*}

\section{SoftMax + LogLoss}
Зачастую полезной оказывается связка сразу двух уровней

\subsection{Forward propagation}

$$
\Loss = - \frac{1}{\NumOfSamples} \Sum_{n=1}^{\NumOfSamples} \Sum_{c=1}^{\NumOfClasses} \boldY_c^n \log \boldP_c^n
$$


$$
\Loss = \Sum_{c=1}^{\NumOfClasses} \boldy_c \log \boldp_c,
$$
где 
$$
\boldp_c = \frac{e^{\boldx_c}}{\Sum_{i=1}^\NumOfSamples e^{\boldx_i}}
$$


\begin{gather*}
\frac{d\Loss}{d \boldx_i} = \left(\frac{d \boldp}{d \boldx_i}\right)^T \frac{d \Loss}{d \boldp} = 
\end{gather*}

\begin{gather*}
\frac{d\boldp_j}{d \boldx_i} = \boldp_j \delta_{ij} - \boldp_i \boldp_j
\end{gather*}
\begin{gather*}
\frac{d \Loss}{d \boldp_j} = -\frac{\boldy_j}{\boldp_j}
\end{gather*}
\begin{gather*}
\frac{d\Loss}{d \boldx_i} = \left(\frac{d \boldp}{d \boldx_i}\right)^T \frac{d \Loss}{d \boldp} = 
\Sum_{j=1}^{\NumOfClasses} \frac{d \boldp_j}{d \boldx_i} \frac{d \Loss}{d \boldp_j} = - \Sum_{j=1}^{\NumOfClasses}  \left(\boldp_i \delta_{ij} - \boldp_i \boldp_j \right) \frac{\boldy_j}{\boldp_j} = \Sum_{j=1}^{\NumOfClasses}  \left(\boldp_i \boldy_j - \frac{\boldp_i}{\boldp_j} \boldy_j \delta_{ij}\right) = \boldp_i - \boldy_i
\end{gather*}

\begin{tabularx}{\textwidth}{|p{4cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
Покомпонентные производные                        &Производные по векторам и матрицам          &Minibatch-призводные &Стандартные minibatch-производные \\ \hline
$\frac{d\Loss}{d \boldx_i} = \boldp_i - \boldy_i$ &$\frac{d\Loss}{d \boldx} = \boldp - \boldy$ &$\frac{d\Loss}{d \boldX} = \boldP - \boldY$ &$\frac{d\Loss}{d \boldX} = \boldP - \boldY$  \\ \hline
\end{tabularx}


\subsection{Backward propagation}

\section{Batch Normalization}

Пусть $\boldx$ --- это вектор на входе слоя ($x \in \mathbb{R}^{\NumOfSamples}$). Тогда вектор $\boldy$ на выходе слоя есть
$$
\boldy = \gamma \cdot \frac{\boldx - \mu}{\sqrt{\sigma^2 + \eps}} + \boldbeta,
$$
где $$\mu = \frac{1}{\NumOfSamples}\Sum_{i=1}^\NumOfSamples x_i, \qquad \sigma^2 = \frac{1}{\NumOfSamples}\Sum_{i = 1}^\NumOfSamples (x_i - \mu)^2 = \frac{1}{\NumOfSamples} x_i^2 - \left(\frac{1}{\NumOfSamples} x_i \right)^2.$$

Пусть дана производная функции потерь по $\boldy$, т.е. $\partial \Loss / \partial \boldy$. Найдем производные функции потерь по $\boldx$, $\boldbeta$, $\gamma$.
Сначала найдем $\partial \Loss / \partial \boldx$:
\begin{gather}
\partder{\Loss}{\boldx} = \underbrace{\partder{\boldy}{\boldx}}_{\mathbb{R}^{\NumOfSamples \times \NumOfSamples}} \cdot \underbrace{\frac{\Loss}{\boldy}}_{\mathbb{R}^{\NumOfSamples}},
\end{gather}
где 
\begin{equation}
\partder{\boldy}{\boldx} = 
\begin{pmatrix}
\partder{y_1}{x_1} &\partder{y_2}{x_1} &\dots &\partder{y_\NumOfSamples}{x_1} \\
\partder{y_1}{x_2} &\partder{y_2}{x_2} &\dots &\partder{y_\NumOfSamples}{x_2} \\
\vdots 			   &\vdots 			   &\ddots &\vdots                        \\
\partder{y_1}{x_\NumOfSamples} &\partder{y_2}{x_\NumOfSamples} &\dots &\partder{y_\NumOfSamples}{x_\NumOfSamples} \\
\end{pmatrix}
\end{equation}

\begin{gather*}
\partder{\boldy}{\boldx} = \gamma \partder{\left(\frac{\boldx - \mu}{\sqrt{\sigma^2 + \eps}}\right)}{\boldx}.
\end{gather*}
Рассмотрим производную 
\begin{gather*}
\partder{y_i}{x_j} = \gamma \partial\left(\frac{x_i - \mu}{\sqrt{\sigma^2 + \eps}}\right)/\partial x_j = 
\gamma \partder{(x_i - \mu)}{x_j} \frac{1}{\sqrt{\sigma^2 + \eps}} - \frac{\gamma}{2} \frac{x_i - \mu}{(\sigma^2 + \eps)^{3/2}} \partder{(\sigma^2 + \eps)}{x_j} = \\
= \gamma \left(\delta_{ij} - \frac{1}{\NumOfSamples}\right) \frac{1}{\sqrt{\sigma^2 + \eps}} - \frac{\gamma}{2} \frac{x_i - \mu}{(\sigma^2 + \eps)^{3/2}} \cdot 2 \left(\frac{x_j}{\NumOfSamples} -  \frac{\mu}{\NumOfSamples} \right) =  \gamma \left(\delta_{ij} - \frac{1}{\NumOfSamples}\right) \frac{1}{\sqrt{\sigma^2 + \eps}} - \gamma \frac{(x_i - \mu)(x_j - \mu)}{\NumOfSamples(\sigma^2 + \eps)^{3/2}}.
\end{gather*}
Таким образом
\begin{gather*}
\partder{\boldy}{\boldx} = \frac{\gamma}{\sqrt{\sigma^2 + \eps}} \left(\left(\boldI - \frac{1}{\NumOfSamples}\boldE\right) -  \frac{\boldC}{\sigma^2 + \eps}\right),
\end{gather*}
где $\boldC = \frac{1}{\NumOfSamples}(\boldx - \boldmu)(\boldx - \boldmu)^T$, $\boldI$ --- единичная матрица размера $\NumOfSamples \times \NumOfSamples$, $\boldE$ --- матрица, полностью состящая из единиц, размера $\NumOfSamples \times \NumOfSamples$. Тогда
\begin{gather*}
\partder{\Loss}{\boldx} = \frac{\gamma}{\sqrt{\sigma^2 + \eps}} \left(  \left(\boldI - \frac{1}{\NumOfSamples}\boldE\right) -  \frac{\boldC}{\sigma^2 + \eps}\right) \partder{\Loss}{\boldy} = 
\frac{\gamma}{\sqrt{\sigma^2 + \eps}} \left(\partder{\Loss}{\boldy} - \bolde \cdot \overline{\partder{\Loss}{\boldy}} - \frac{\boldx - \mu}{\NumOfSamples(\sigma^2 + \eps)} \cdot \langle \boldx - \mu, \frac{\Loss}{\boldy}\rangle \right),
\end{gather*}
где $\bolde$ --- столбец из единиц размерности $\NumOfSamples$, $\overline{\partder{\Loss}{\boldy}}$ --- среднее значение элементов вектора $\partder{\Loss}{\boldy}$. Обозначив через $\boldz$ ``стандартизованный'' вектор $\boldx$ ($\boldz = (\boldx - \mu) / \sigma$), получим
\begin{gather*}
\partder{\Loss}{\boldx} =\frac{\gamma}{\sqrt{\sigma^2 + \eps}} \left(\partder{\Loss}{\boldy} - \bolde \cdot \overline{\partder{\Loss}{\boldy}} - \frac{\boldx - \mu}{\NumOfSamples(\sigma^2 + \eps)} \cdot \langle \boldx - \mu, \frac{\Loss}{\boldy}\rangle \right) = 
\frac{\gamma}{\sqrt{\sigma^2 + \eps}} \left(\partder{\Loss}{\boldy} - \bolde \cdot \overline{\partder{\Loss}{\boldy}}  - \frac{\boldz}{\NumOfSamples} \cdot \langle \boldz, \frac{\Loss}{\boldy}\rangle \right).
\end{gather*}

Теперь найдем производные $\partial \Loss / \partial \boldbeta$ и $\partial \Loss / \partial \gamma$:
\begin{gather*}
\partder{\Loss}{\boldbeta} = \partder{\boldy}{\boldbeta} \partder{\Loss}{\boldy} = \boldI \partder{\Loss}{\boldy} = \partder{\Loss}{\boldy}, \\
\partder{\Loss}{\gamma} = \left(\partder{\boldy}{\gamma}\right)^T \partder{\Loss}{\boldy} = \frac{\boldx - \mu}{\sqrt{\sigma + \eps}}
\end{gather*}

\subsection{Реализация в Python}

Представленная ниже реализация предполагает, что 
\begin{itemize}
	\item $\boldX \in \mathbb{R}^{\NumOfSamples \times \NumOfDims}$ --- матрица объектов-признаков
	\item $\boldZ \in \mathbb{R}^{\NumOfSamples \times \NumOfDims}$ --- ``стандартизованная'' вдоль оси 0 матрица $\boldX$, т.е. выход слоя \texttt{BatchNormalization}
	\item $\boldmu \in \mathbb{R}^{\NumOfSamples}$ --- средние значения для признаков
	\item $\boldsigma \in \mathbb{R}^{\NumOfSamples}$ --- стандартные отклонения для признаков
\end{itemize}

\begin{python}
def batchnorm_backward_alt(output_grad, cache):
  X, Z, mu, sigma, gamma, beta, eps = cache
  N, D = X.shape
  X_grad = gamma / np.sqrt(sample_var + eps)[None, :] *\ 
           ((output_grad - np.mean(output_grad, axis=0)[None, :]) -\
           Z * np.mean(np.multiply(Z, output_grad), axis=0)[None, :]) 
  beta_grad = np.sum(output_grad, axis=0)
  gamma_grad = np.sum(np.multiply(X_norm, output_grad), axis=0)
  return X_grad, gamma_grad, beta_grad
\end{python}

\section{MulticlassLogLoss}
На вход данного уровня поступает вектор $\boldp \in \mathbb{R}^{\NumOfDims}$ или же в случае использования батчей матрица $\boldP \in \mathbb{R}^{\NumOfSamples \times \NumOfDims}$. Данный уровень обычно является последним уровнем нейронной сети и используется для оценки вероятностей принадлежности объектов к классам. Поэтому далее будем считать, что $\NumOfDims = \NumOfClasses$, где $\NumOfClasses$ --- количество классов в выборке.

\begin{gather*}
\Loss = \frac{1}{\NumOfSamples} \Sum_{n = 1}^{\NumOfSamples} \Sum_{k = 1}^{\NumOfClasses} y_k^{(n)} \log p_n^{(k)} = \frac{1}{\NumOfSamples} \Sum_{n=1}^{\NumOfSamples} \Loss_n.
\end{gather*}

\begin{gather*}
\partder{\Loss}{p_n^{(k)}} = \frac{1}{\NumOfSamples} \frac{y_n^{(k)}}{p_n^{(k)}}
\end{gather*}
\begin{gather*}
\partder{\Loss}{\boldp_n} = [0, \dots,  \frac{1}{\NumOfSamples} \frac{y_n^{(k_n^*)}}{p_n^{(k_n*)}}, \dots, 0], 
\end{gather*}
где $k_n^*$ --- это истинный класс объекта $n$. Таким образом $\partder{\Loss}{\boldP}$ --- это матрица размера $\NumOfSamples \times \NumOfClasses$, имеющая вид
\begin{gather*}
\partder{\Loss}{\boldP} = \frac{1}{\NumOfSamples}
\begin{bmatrix}
\frac{y_{1, k^*_1}}{p_{1, k^*_1}} &\dots  &0 &\dots  &0 \\
0 &\dots  &\frac{y_{2, k^*_2}}{p_{2, k^*_2}} &\dots  &0 \\
\vdots &\vdots  &\vdots &\ddots  &\vdots \\
0 &\dots  &0 &\dots  &\frac{y_{\NumOfSamples, k^*_\NumOfSamples}}{p_{\NumOfSamples, k^*_\NumOfSamples}} \\
\end{bmatrix}
\end{gather*}

\chapter{Сверточные Сети}

\section{Обратное распространение ошибки}

\subsection{Случай $\text{stride} = 1$}

\begin{equation}
\partder{\Loss}{\Input[c, h, w]}= \Sum_{(i, j) \in \AffectField(h, w)} 
\partder{\Output[c, i, j]}{\Input[c, h, w]} \cdot \partder{\Loss}{\Output[c, i, j]}
\end{equation}

Введем вспомогательные термины. 

\paragraph{Область действия.}
Рассмотрим элемент $\Input[h, w]$ входа сверточного слоя ($h \in [0, H - 1]$, $w \in [0, W - 1]$). \textbf{Областью действия} $\AffectField(h, w)$ элемента $\Input[h, w]$ называется множество координат элементов матрицы $\Output$, которые зависят от элемента $I[h, w]$:
\begin{equation}
\AffectField(h, w) \triangleq \{(i, j)\colon i \in \AffectField(h), j\in \AffectField(w)\},
\end{equation}
где
\begin{align}
\AffectField(h)& = \{i \colon \max\{0, h - \FilterSize[h]\} \le i \le \min\{h, H - \FilterSize[h]\}\}, \\
\AffectField(w)& = \{j \colon \min\{0, w - \FilterSize[w]\} \le j \le \max\{w, W - \FilterSize[w]\}\}.
\end{align}

\paragraph{Область восприимчивости.}
Теперь рассмотрим элемент $\Output[i, j]$ выхода сверточного слоя ($i \in [0, H - \FilterSize_h]$, $j \in [0, W - \FilterSize_w]$). \textbf{Областью восприимчивости} $\ReceptiveField(h, w)$ элемента $\Output[i, j]$ называется множество координат элементов матрицы $\Input$, от которых зависит значение элемента $\Output[i, j]$:
\begin{equation}
\label{eq:receptive_field}
\ReceptiveField(i, j) = \{(h, w) \colon h \in \ReceptiveField(i), w \in \ReceptiveField(j)\},
\end{equation}
где
\begin{align}
\label{eq:receptive_field:add}
\ReceptiveField(i) &= \{h \colon i \le h \le \min\{i + \FilterSize_h - 1, H - 1\}\}, \\
\ReceptiveField(j) &= \{w \colon j \le w \le \min\{j + \FilterSize_w - 1, W - 1\}\}.
\end{align}


\begin{equation}
\label{eq:conv:backprop:1}
\partder{\Loss}{\Input[c, h, w]}= \Sum_{(i, j) \in \AffectField(h, w)} 
\partder{\Output[c, i, j]}{\Input[c, h, w]} \cdot \partder{\Loss}{\Output[c, i, j]} = 
\Sum_{(i, j) \in \AffectField(h, w)} W[c, h - i, w - j] \partder{\Loss}{\Output[c, i, j]}
\end{equation}
Уже здесь можно заметить, что нахождение производных по $\Input$ напоминает свертку производных по выходу $\partder{\Loss}{\Output}$
с транспонированной (вдоль второго и третьего измерений) матрицей весов $W$.

Рассмотрим пока для определенности только точки $(h, w)$ из диапазона $(h, w) \in [\FilterSize[h] - 1, H - \FilterSize[h]] \times [\FilterSize[w] - 1, W - \FilterSize[w]]$. В этом случае сумму из~\eqref{eq:conv:backprop:1} можно записать в виде
\begin{equation}
\label{eq:conv:backprop:2}
\partder{\Loss}{\Input[c, h, w]} = \Sum_{\substack{i \in [h - \FilterSize[h] + 1, h] \\ j \in [w - \FilterSize[w] + 1, w]}} W[c, h - i, w - j] \partder{\Loss}{\Output[c, i, j]},
\end{equation}
не содержащим граничных условий на $i$ и $j$. Далее введем транспонированную вдоль второго и третьего измерений матрицу весов $W'$:
\begin{equation}
W'[i, j] = W[c, \FilterSize[h] - 1 - i, \FilterSize[w] - 1 - j], \quad c \in [0, \NumOfFilters - 1], i \in [0, \FilterSize[h] - 1], j \in [0, \FilterSize[w] - 1].
\end{equation}
В таком случае~\eqref{eq:conv:backprop:2} можно далее записать в виде
\begin{equation}
\partder{\Loss}{\Input[c, h, w]} = \Sum_{\substack{i \in [h - \FilterSize[h] + 1, h] \\ j \in [w - \FilterSize[w] + 1, w]}} W'[c, i, j] \partder{\Loss}{\Output[c, i, j]}.
\end{equation}

Можно в принципе избавиться от граничных условий и записать для любых $(h, w) \in [0, H - 1] \times 0, W - 1]$, если положить, что значения по индексам, отсутствующим в матрице $\partial \Loss / \partial I$, равны нулю:
\begin{equation}
\partder{\Loss}{\Input[c, h, w]} = \Sum_{\substack{i \in [h - \FilterSize[h] + 1, h] \\ j \in [w - \FilterSize[w] + 1, w]}} W'[c, i, j] \partder{\Loss}{\Output[c, i, j]}.
\end{equation}
Фактически это эквивалентно тому, что матрица $\partder{\Loss}{\Output[c, i, j]}$ дополняется $\FilterSize[h] - 1$ вдоль второго измерения и $\FilterSize[w] - 1$ вдоль третьего измерения с каждой стороны. Полученная матрица имеет размер $(H + \FilterSize[h] - 1) \times (W + \FilterSize[w] - 1)$. 

\subsection{Общий случай $\text{stride} \ge 1$}
В данном случае размер матрицы $\Input$ как и ранее есть $H \times W$. В общем случае $\StrideSize_h \ge 1$ и/или $\StrideSize_w \ge 1$ для размеры $H' $ и $W'$ матрицы $\Output$ равны
\begin{equation}
H' = \frac{H - \FilterSize_h}{\StrideSize_h} + 1, \qquad W' = \frac{W - \FilterSize_w}{\StrideSize_w} + 1.
\end{equation}
Здесь как и ранее полагаем, что значения $H$, $W$, $\StrideSize$, $\FilterSize$ согласованы путем, например, предварительного дополнения входа сверточного слоя нулями, так что $H'$  и $W'$ --- целые числа.

В данном случае требуется скорректировать ранее введенные области действия и восприимчивости.

\paragraph{Область восприимчивости $\ReceptiveField(i, j)$}
Рассмотрим элемент $\Output[i, j]$ ($i \in [0, H'- 1]$, $j \in [0, W' - 1]$). Этот элемент зависит от элементов матрицы $\Input$, расположенных по следующим координатам $\{(h, w)\}$:
\begin{gather*}
h \in [i \cdot \StrideSize_h, i \cdot \StrideSize_h + \FilterSize_h - 1], \\
w \in [j \cdot \StrideSize_w, j \cdot \StrideSize_w + \FilterSize_w - 1].
\end{gather*}
Заметим, что при всех допустимых $i \in [0, H'- 1]$ и $j \in [0, W' - 1]$ значения $h$ и $w$ лежат в допустимых множествах $[0, H - 1]$ и $[0, W - 1]$. Таким образом, область восприимчивости имеет вид:
\begin{equation}
\ReceptiveField(i, j) = \{(h, w)\colon h \in [i \cdot \StrideSize_h, i \cdot \StrideSize_h + \FilterSize_h - 1], w \in [j \cdot \StrideSize_w, j \cdot \StrideSize_w + \FilterSize_w - 1]\}, \quad i \in [0, H'- 1], j \in [0, W' - 1].
\end{equation}

\paragraph{Область влияния (действия) $\AffectField(h, w)$.}
Рассмотрим, на какие элементы матрицы $\Output$ влияет элемент $\Input[h, w]$. Чтобы элемент $\Input[h, w]$ влиял на $\Output[i, j]$, нужно, чтобы $(h, w) \in \ReceptiveField(i, j)$, т.е.
\begin{equation}
i \cdot \StrideSize_h \le h \le i \cdot \StrideSize_h + \FilterSize_h - 1, \qquad j \cdot \StrideSize_w \le w \le j \cdot \StrideSize_w + \FilterSize_w - 1.
\end{equation}
Осталось лишь определить все те пары $(i, j)$, для которых данное условие выполнено. 
Из условий выше следует, что
\begin{equation}
\frac{h - \FilterSize_h + 1}{\StrideSize_h} \le i \le \frac{h}{\StrideSize_h}, \qquad \frac{w - \FilterSize_w + 1}{\StrideSize_w} \le j \le \frac{w}{\StrideSize_w}. 
\end{equation}
С граничными условиями, данные неравенства для определения области влияния $\AffectField(h, w)$ переходят в

С учетом граничных условий, области влияния $\AffectField(h)$ и $\AffectField(w)$ вдоль высоты и ширины определяются как
\begin{gather*}
\AffectField(h; \StrideSize_h) = \left\{i \in \mathbb{Z} \colon \max\{\frac{h - \FilterSize_h + 1}{\StrideSize_h}, 0\} \le i \le \min\{\frac{h}{\StrideSize_h}, \frac{H - \FilterSize_h}{\StrideSize_h}\}\right\},\\
\AffectField(w; \StrideSize_w) = \left\{j \in \mathbb{Z} \colon \max\{\frac{w - \FilterSize_w + 1}{\StrideSize_w}, 0\} \le j \le \min\{\frac{w}{\StrideSize_w}, \frac{W - \FilterSize_w}{\StrideSize_w}\}\right\}.\\
\end{gather*}
%Граничные условия активируются при следующих значениях $h$ и $w$ соответственно
%\begin{gather*}
%h \in [0, \FilterSize_h - 1) \cup [H - \FilterSize_h + 1, H),\\
%w \in [0, \FilterSize_w - 1) \cup [W - \FilterSize_w + 1, W).
%\end{gather*}

Пусть $h = q_h\StrideSize_h + r_h$, $q_h \in \mathbb{Z}$, $r_h \in [0, \StrideSize_h - 1]$.
\begin{equation*}
\frac{h - \FilterSize_h + 1}{\StrideSize_h} = q_h + \frac{r_h}{\StrideSize_h} -\frac{\FilterSize_h - 1}{\StrideSize_h} \le i \le q_h + \frac{r_h}{\StrideSize_h} = \frac{h}{\StrideSize_h}.
\end{equation*}

\subsubsection{Производные функции потерь $\Loss$}
\paragraph{Производная по входу $\Input$.}
%\substack{i \in \AffectField(h; \StrideSize_h) \\ j \in \AffectField(w; \StrideSize_w)}}
\begin{equation}
\partder{\Loss}{\Input[h, w]}= \Sum_{(i, j) \in \AffectField(h, w; \StrideSize)} 
\partder{\Output[i, j]}{\Input[h, w]} \cdot \partder{\Loss}{\Output[i, j]} =
\Sum_{(i, j) \in \AffectField(h, w; \StrideSize)} \Weights[h - i \cdot \StrideSize_h, w - j \cdot \StrideSize_w] \cdot \partder{\Loss}{\Output[i, j]}.
\end{equation}
Так как множество $\AffectField(h, w; \StrideSize)$ уже найдено, то поточечное вычисление данной производной не представляет проблем.

В случае наличия нескольких каналов производная по входу $\Input[c, h, w]$ вычисляется следующим образом:
\begin{equation}
\partder{\Loss}{\Input[c, h, w]}= \Sum_{l=0}^{\NumOfChannels' - 1}\Sum_{(i, j) \in \AffectField(h, w; \StrideSize)} 
\partder{\Output[l, i, j]}{\Input[c, h, w]} \cdot \partder{\Loss}{\Output[i, j]} =
\Sum_{(i, j) \in \AffectField(h, w; \StrideSize)} \Weights[c, h - i \cdot \StrideSize_h, w - j \cdot \StrideSize_w] \cdot \partder{\Loss}{\Output[l, i, j]}.
\end{equation}

\paragraph{Производная по весам $\Weights$.}
В случае одного входного и выходного каналов производная по весу имеет вид:
\begin{equation}
\partder{\Loss}{\Weights[h, w]} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Output[i', j']}{\Weights[h, w]} \cdot  \partder{\Loss}{\Output[i', j']} =  \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \Input[i' \cdot \StrideSizeH + h, j' \cdot \StrideSizeW + w] \cdot \partder{\Loss}{\Output[i', j']}.
\end{equation}

В случае наличия $\NumOfChannels\ge 1$ входных каналов и одного выходного производная принимает вид
\begin{equation}
\partder{\Loss}{\Weights[c, h, w]} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Output[i', j']}{\Weights[c, h, w]} \cdot  \partder{\Loss}{\Output[i', j']} =  \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \Input[c, i' \cdot \StrideSizeH + h, j' \cdot \StrideSizeW + w] \cdot \partder{\Loss}{\Output[i', j']}.
\end{equation}

В наиболее общем случае $\NumOfChannels\ge 1$ входных каналов и одного выходного производная принимает вид
\begin{gather}
\partder{\Loss}{\Weights[c', c, h, w]} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Output[c', i', j']}{\Weights[c', c, h, w]} \cdot  \partder{\Loss}{\Output[c', i', j']} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \Input[c, i' \cdot \StrideSizeH + h, j' \cdot \StrideSizeW + w] \cdot \partder{\Loss}{\Output[c', i', j']}.
\end{gather}

\paragraph{Производная по смещению $\Biases$.}
Каждому каналу на выходе соответствует свое значение смещения.

\begin{equation}
\partder{\Loss}{\Bias} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Output[i', j']}{\Bias} \cdot \partder{\Loss}{\Output[i', j']} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Loss}{\Output[i', j']}.
\end{equation}

\begin{equation}
\partder{\Loss}{\Biases[c']} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Output[c', i', j']}{\Biases[c']} \cdot \partder{\Loss}{\Output[c', i', j']} = \Sum_{\substack{0 \le i' < H' \\ 0 \le j' < W'}} \partder{\Loss}{\Output[c', i', j']}.
\end{equation}

\subsection{Альтернативная реализация}
Пусть 
\begin{itemize}
	\item $\boldX$ --- входной тензор сверточного слоя
	\item $\boldY$ --- тензор на выходе сверточного слоя
	\item Свертка считается в режиме \texttt{VALID}
\end{itemize}

Для начала рассмотрим случай двумерного входного тензора, т.е.
\begin{itemize}
	\item $\boldX$ имеет размер $\InputH \times \InputW$
	\item $\boldY$ имеет размер $\OutputH \times \OutputW$
\end{itemize}

Пусть сначала считается полная свертка с $\StrideSizeH = \StrideSizeW = 1$, тогда на выходе сначала получим тензор $\boldZ$ размера $(\InputH - \FilterSizeH + 1) \times (\InputW - \FilterSizeW + 1)$. Однако не все его значения нам нужны


\section{Слой Max Pooling}

\subsection{Прямое распространение}

\subsection{Обратное распространение}

\chapter{Рекуррентные сети (Recurrent neural networks)}


\begin{equation*}
\boldy = f(\boldV \boldh + \boldW \boldx + \boldb)
\end{equation*}

\begin{eqnarray*}
\boldh_1 &= &f(\boldV \boldh_0 + \boldW \boldx_0 + \boldb)\\
\boldh_2 &= &f(\boldV \boldh_1 + \boldW \boldx_1 + \boldb)\\
\boldh_3 &= &f(\boldV \boldh_2 + \boldW \boldx_2 + \boldb)\\
\dots \\
\boldh_k &= &f(\boldV \boldh_{k-1} + \boldW \boldx_{k-1} + \boldb)\\
\end{eqnarray*}

Если формулы прямого распространения не вызывают особых сложностей, то обратное распространение требует значительно большего количества выкладок.

\section{Обратное распространение ошибки}

$$
\boldy = \boldW \boldx + \boldb
$$
\begin{table}
	\centering
	\begin{tabular}{lllclll}
		$\frac{d \Loss}{d \boldx}$ &$=$ &$\boldW^T \frac{d \Loss}{d \boldy}$ & & $\frac{d \Loss}{d \boldX}$ &$=$ &$\boldW^T \frac{d \Loss}{d \boldY}$ \\
		$\frac{d \Loss}{d \boldW}$ &$=$ &$\frac{d \Loss}{d \boldy} \boldx^T$ & & $\frac{d \Loss}{d \boldW}$ &$=$ &$\frac{d \Loss}{d \boldY} \boldX^T$ \\
		$\frac{d \Loss}{d \boldb}$ &$=$ &$\frac{d \Loss}{d \boldy}$          & & $\frac{d \Loss}{d \boldb}$ &$=$ &$\Sum_{i=1}^{\NumOfSamples}\frac{d \Loss}{d \boldY_i}$
	\end{tabular}
\end{table}

\begin{gather*}
\boldy = f(\boldV f(\boldV f( \dots (\boldV f(\boldV \boldh_0 + \boldW \boldx_0 + \boldb) + \boldW \boldx_1 + \boldb) \dots) +  \boldW \boldx_{k-2} + \boldb ) + \boldW \boldx_{k-1} + \boldb)
\end{gather*}
Требуется найти частные производные $\partial \Loss / \partial \boldW$, $\partial \Loss / \partial \boldV$, $\partial \Loss / \partial \boldb$ при заданной производной $\partial \Loss / \partial \boldy$.

Посмотрим на некоторый уровень рекуррентной сети. Этот уровень осуществляет преобразование вида
\begin{equation*}
\boldy = f(\boldV \boldh + \boldW \boldx + \boldb)
\end{equation*}
Далее за этим уровнем следует еще какое-то число точно таких же уровней. Далее допустим, что 
\begin{itemize}
	\item Нам известна производная функции потерь $\partial \Loss / \partial \boldy$ по выходу конкретно данного уровня
	\item Нам известны производные функции потерь $\Loss$ по $\boldW$, $\boldV$, $\boldb$ как параметрам последующих уровней.
\end{itemize}
Иными словами, функция потерь $\Loss$ представляется как функция от выхода данного слоя $\boldy$, и параметров $\boldW$, $\boldV$, $\boldb$:
$$
\Loss = \Loss(\boldy, \boldV, \boldW, \boldb),
$$
причем известны частные производные $\partial \Loss / \partial \boldy$, $\partial \Loss/ \partial \boldW$, $\partial \Loss / \partial \boldV$, $\partial \Loss / \partial \boldb$. Тогда полные производные по $\boldW$, $\boldV$, $\boldb$, которые учитывают зависимость $\boldy$ от $\boldW$, $\boldV$, $\boldb$ имеют вид:
\begin{gather*}
\boldy = \boldy(\boldx, \boldh, \boldW, \boldV, \boldb)
\end{gather*}


\subsubsection{Покомпонентные производные}
\begin{align}
\frac{d\Loss}{d\boldW_{i,j}} & = \frac{d\Loss(\boldy, \boldV, \boldW, \boldb)}{d\boldW_{i,j}} = \partder{\Loss}{\boldW_{i, j}} + \left(\partder{\boldy}{\boldW_{i,j}}\right)^T \partder{\Loss}{\boldy} =  \partder{\Loss}{\boldW_{i, j}} + \boldx_j f'(\boldz_i) \partder{\Loss}{\boldy_i} \\ 
\frac{d\Loss}{d\boldV_{i,j}} & = \frac{d\Loss(\boldy, \boldV, \boldW, \boldb)}{d\boldV_{i,j}} = \partder{\Loss}{\boldV_{i, j}} + \left(\partder{\boldy}{\boldV_{i,j}}\right)^T \partder{\Loss}{\boldy} = \partder{\Loss}{\boldV_{i, j}} + \boldh_j f'(\boldz_i) \partder{\Loss}{\boldy_i} \\\
\frac{d\Loss}{d\boldb_{i}} & = \frac{d\Loss(\boldy, \boldV, \boldW, \boldb)}{d\boldb_{i}} = \partder{\Loss}{\boldb_i} + \left(\partder{\boldy}{\boldb_i}\right)^T \partder{\Loss}{\boldy} = \partder{\Loss}{\boldb_i} + f'(\boldz_i)\partder{\Loss}{\boldy_i} \ \\
\frac{d\Loss}{d\boldx_{i}} & = \frac{d\Loss(\boldy, \boldV, \boldW, \boldb)}{d\boldx_{i}} = \left(\partder{\boldy}{\boldx_i}\right)^T\partder{\Loss}{\boldy} =
(\boldW^i)^T \diag (f'(\boldz)) \partder{\Loss}{\boldy} \\
\frac{d\Loss}{d\boldh_{i}} & = \frac{d\Loss(\boldy, \boldV, \boldW, \boldb)}{d\boldh_{i}} = \left(\partder{\boldy}{\boldh_i}\right)^T\partder{\Loss}{\boldy} =
(\boldV^i)^T \diag (f'(\boldz))  \partder{\Loss}{\boldy}
\end{align}
Таким образом, требуется найти частные производные $\partder{\boldy}{\boldW_{i,j}}$, $\partder{\boldy}{\boldV_{i,j}}$, $\partder{\boldy}{\boldb_{i}}$
\begin{align*}
\partder{\boldy}{\boldW_{i,j}} &= \left(\frac{d\boldy}{d\boldz}\right)^T \frac{\partial\boldz}{\partial\boldW_{i, j}} =  \boldx_j \diag(f'(\boldz))\bolde_i =  \boldx_j  f'(\boldz_i) \bolde_i. \\
\frac{\partial \boldy}{\partial \boldV_{i, j}} &= \left(\frac{d\boldy}{d\boldz}\right)^T \frac{\partial\boldz}{\partial\boldV_{i, j}}  = \boldh_j \diag(f'(\boldz))  \bolde_i= \boldh_j f'(\boldz_i) \bolde_i.\\ 
\frac{\partial \boldy}{\partial \boldx_{i}} &= \left(\frac{d\boldy}{d\boldz}\right)^T \frac{\partial\boldz}{\partial\boldx_{i}} = \diag(f'(\boldz)) \boldW^i\\
\frac{\partial \boldy}{\partial \boldh_{i}} &= \left(\frac{d\boldy}{d\boldz}\right)^T \frac{\partial\boldz}{\partial\boldh_{i}} = \diag(f'(\boldz)) \boldV^i\\
\frac{\partial \boldy}{\partial \boldb_{i}} &= f'(\boldz_i)\bolde_i 
\end{align*}

%\begin{gather*}
%\left(}\right)^T \partder{\Loss}{\boldy} = 
%\end{gather*}

\subsubsection{Производные по матрицам и векторам}
\begin{align*}
\frac{d\Loss}{d\boldW} & = \partder{\Loss}{\boldW} + \diag(f'(\boldz))  \partder{\Loss}{\boldy} \boldx^T \\ 
\frac{d\Loss}{d\boldV} & = \partder{\Loss}{\boldV} + \diag(f'(\boldz))  \partder{\Loss}{\boldy} \boldh^T \\
\frac{d\Loss}{d\boldb} & = \partder{\Loss}{\boldb} + \diag(f'(\boldz))  \partder{\Loss}{\boldy} \\ 
\frac{d\Loss}{d\boldx} & = \boldW^T \diag (f'(\boldz)) \partder{\Loss}{\boldy}  \\
\frac{d\Loss}{d\boldh} & = \boldV^T \diag (f'(\boldz)) \partder{\Loss}{\boldy} 
\end{align*}

\subsubsection{Minibatch-производные}
\begin{align*}
\frac{d\Loss}{d\boldW} & = \partder{\Loss}{\boldW} + \left(f'(\boldZ) \odot \partder{\Loss}{\boldY}\right) \boldX^T \\ 
\frac{d\Loss}{d\boldV} & = \partder{\Loss}{\boldV} + \left(f'(\boldZ) \odot \partder{\Loss}{\boldY}\right) \boldH^T \\
\frac{d\Loss}{d\boldb} & = \partder{\Loss}{\boldb} + \sum_{i=1}^{\NumOfSamples} f'(\boldZ^i) \odot \partder{\Loss}{\boldY^i} \\ 
\frac{d\Loss}{d\boldX} & = \boldW^T \left(f'(\boldZ) \odot \partder{\Loss}{\boldY}\right)  \\
\frac{d\Loss}{d\boldH} & = \boldV^T \left(f'(\boldZ) \odot \partder{\Loss}{\boldY}\right) 
\end{align*}

\chapter{TensorFlow}

\begin{itemize}
	\item Device placement - изучить достоинства размещения переменных вручную
\end{itemize}


\chapter{Классификация трафика}
The suite of classifiers includes:
\begin{itemize}
	\item the payload-based classifier, crl pay, which has been developed and widely
	used by the Internet research community
	\item the graph-based classifiers, BLINC and Traffic Dispersion Graphs
	\item the CoralReef’s [1] ports-applications matching database (i.e., ports-based classifier)
	\item the seven most commonly used machine learning algorithms such as C4.5 Decision Tree, Naive Bayes, Naive Bayes Kernel Estimation,
	Bayesian Networks, k-Nearest Neighbors, Neural Networks and Support Vector Machines
	\item a combined weight-based classifier which takes classification results from some or all of the aforementioned classifiers and performs a
	weighted voting process to derive a single best classification result.
\end{itemize}

BLINK

CoralReef

Метрики:
\begin{itemize}
	\item per-trace accuracy (Overall Accuracy)
	\item per-application accuracy (Precision, Recall, and FMeasure)
	\item computational performance (Learning Time
	and Classification Time)
\end{itemize}

Проблемы:
\begin{itemize}
	\item Классификатор привязан к точке своего функционирования
\end{itemize}

Вопросы
\begin{itemize}
	\item Is the whole dataset is unlabeled at the beginning? So, to train the classifier we first of all apply DPI to the training dataset to make it labeled?
	\item Only ``elephant'' flows are considered? What are these flows?
\end{itemize}

\url{http://mawi.wide.ad.jp/mawi/}

\bibliographystyle{ugost2008}
\bibliography{../bib/phy.bib}

\end{document}